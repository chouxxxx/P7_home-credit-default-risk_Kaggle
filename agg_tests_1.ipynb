{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (most) package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import imblearn.pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_path = \"home-credit-default-risk/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for dirname, _, filenamess in os.walk(le_path):\n",
    "    for filenamee in filenamess:\n",
    "#                        HomeCredit_columns_description.csv est illisible.\n",
    "        if filenamee != \"HomeCredit_columns_description.csv\":\n",
    "            filename = os.path.join(dirname, filenamee)\n",
    "#            print(filename)\n",
    "            filenames.append(filename)\n",
    "#            df = pd.read_csv(filename)\n",
    "#            display(df[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bar</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foo</td>\n",
       "      <td>2</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bar</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>foo</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xin</td>\n",
       "      <td>-2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A  B    C\n",
       "0  foo  1  1.0\n",
       "1  bar  1  2.2\n",
       "2  foo  2  3.7\n",
       "3  bar  2  4.0\n",
       "4  foo  8  5.0\n",
       "5  xin -2  6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A  B    C\n",
      "1  bar  1  2.2\n",
      "3  bar  2  4.0 \n",
      "\n",
      "\n",
      "     A  B    C\n",
      "0  foo  1  1.0\n",
      "2  foo  2  3.7\n",
      "4  foo  8  5.0 \n",
      "\n",
      "\n",
      "     A  B    C\n",
      "5  xin -2  6.0 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">C</th>\n",
       "      <th colspan=\"3\" halign=\"left\">B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>size</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>3.100000</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <td>3.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xin</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            C                  B         \n",
       "         mean  min  max size min max size\n",
       "A                                        \n",
       "bar  3.100000  2.2  4.0    2   1   2    2\n",
       "foo  3.233333  1.0  5.0    3   1   8    3\n",
       "xin  6.000000  6.0  6.0    1  -2  -2    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"xin\"],\n",
    "    \"B\": [1, 1, 2, 2, 8, -2],\n",
    "    \"C\": [1, 2.2, 3.7, 4, 5, 6],\n",
    "})\n",
    "display(df)\n",
    "\n",
    "df_gba = df.groupby(\"A\")\n",
    "\n",
    "for key, item in df_gba:\n",
    "    print(df_gba.get_group(key), \"\\n\\n\")\n",
    "\n",
    "result = df_gba.agg({\"C\": [\"mean\", \"min\", \"max\", \"size\"],\n",
    "                     \"B\": [\"min\", \"max\", \"size\"]})\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment peut-il join le bb_agg sur le bureau alors que ces deux dfs n'ont ni\n",
    "lignes ni colonnes en commun ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. application_train and application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filenames[1], nrows=num_rows)\n",
    "test_df = pd.read_csv(filenames[0], nrows=num_rows)\n",
    "bureau = pd.read_csv(filenames[2], nrows=num_rows)\n",
    "bb = pd.read_csv(filenames[3], nrows=num_rows)\n",
    "prev = pd.read_csv(filenames[7], nrows=num_rows)\n",
    "pos = pd.read_csv(filenames[6], nrows=num_rows)\n",
    "ins = pd.read_csv(filenames[5], nrows=num_rows)\n",
    "cc = pd.read_csv(filenames[4], nrows=num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[:,[\n",
    "    'SK_ID_CURR',\n",
    "    'CODE_GENDER',\n",
    "    'FLAG_OWN_CAR',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    df,\n",
    "    test_df,\n",
    "    bureau,\n",
    "    bb,\n",
    "    prev,\n",
    "    pos,\n",
    "    ins,\n",
    "    cc,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 --0 1 --0 1 --0 1 --0 1 --0 2 --0 2 --0 4 --0 4 --0 4 --0 4 --0 4 --0 4 --0 4 --0 4 --0 5 --0 6 --0 7 --1 1 --1 3 --1 4 --1 5 --1 6 --2 1 --2 2 --2 2 --2 3 --2 4 --2 5 --3 2 --3 4 --4 1 --4 1 --4 1 --4 2 --4 2 --4 3 --4 3 --4 3 --5 1 --5 1 --5 2 --5 2 --5 2 --5 2 --5 2 --5 2 --6 1 --6 1 --"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_in_common = []\n",
    "for i, d in enumerate(dfs):\n",
    "    for j, dd in enumerate(dfs[i+1:]):\n",
    "        for k in d.columns:\n",
    "            if k in dd.columns:\n",
    "                columns_in_common.append(k)\n",
    "                print(i, j+1, \"--\", end=\"\")\n",
    "columns_in_common = list(set(columns_in_common))\n",
    "len(columns_in_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMT_ANNUITY',\n",
       " 'CODE_GENDER',\n",
       " 'NAME_CONTRACT_STATUS',\n",
       " 'SK_DPD_DEF',\n",
       " 'MONTHS_BALANCE',\n",
       " 'SK_ID_CURR',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'NAME_TYPE_SUITE',\n",
       " 'NAME_CONTRACT_TYPE',\n",
       " 'AMT_CREDIT',\n",
       " 'WEEKDAY_APPR_PROCESS_START',\n",
       " 'HOUR_APPR_PROCESS_START',\n",
       " 'SK_ID_BUREAU',\n",
       " 'DAYS_BIRTH',\n",
       " 'SK_ID_PREV',\n",
       " 'SK_DPD',\n",
       " 'AMT_GOODS_PRICE',\n",
       " 'DAYS_EMPLOYED']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_in_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMT_ANNUITY',\n",
       " 'CODE_GENDER',\n",
       " 'SK_ID_CURR',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'NAME_TYPE_SUITE',\n",
       " 'NAME_CONTRACT_TYPE',\n",
       " 'AMT_CREDIT',\n",
       " 'WEEKDAY_APPR_PROCESS_START',\n",
       " 'HOUR_APPR_PROCESS_START',\n",
       " 'DAYS_BIRTH',\n",
       " 'AMT_GOODS_PRICE',\n",
       " 'DAYS_EMPLOYED']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df.columns) & set(columns_in_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AMT_ANNUITY', 'CODE_GENDER', 'SK_ID_CURR', 'FLAG_OWN_CAR',\n",
       "       'NAME_TYPE_SUITE', 'NAME_CONTRACT_TYPE', 'AMT_CREDIT',\n",
       "       'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START', 'DAYS_BIRTH',\n",
       "       'AMT_GOODS_PRICE', 'DAYS_EMPLOYED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:,list(set(df.columns) & set(columns_in_common))]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_df(d):\n",
    "    cols = list(set(d.columns) & set(columns_in_common))\n",
    "    if len(cols) >= 2:\n",
    "        d = d.loc[:,cols]\n",
    "        return d\n",
    "    else:\n",
    "        print(\"Une seule colonne en commun, voire aucune.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2 = []\n",
    "for d in dfs:\n",
    "    dfs_2.append(shrink_df(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renames(d):\n",
    "    for co in d.columns:\n",
    "        d = d.rename(columns={co: co[:7]})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_3 = []\n",
    "for d in dfs_2:\n",
    "    dfs_3.append(renames(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Index(['AMT_ANN', 'CODE_GE', 'SK_ID_C', 'FLAG_OW', 'NAME_TY', 'NAME_CO',\n",
      "       'AMT_CRE', 'WEEKDAY', 'HOUR_AP', 'DAYS_BI', 'AMT_GOO', 'DAYS_EM'],\n",
      "      dtype='object')\n",
      "1 Index(['CODE_GE', 'SK_ID_C', 'FLAG_OW', 'DAYS_BI', 'DAYS_EM'], dtype='object')\n",
      "2 Index(['AMT_ANN', 'SK_ID_C', 'SK_ID_B'], dtype='object')\n",
      "3 Index(['MONTHS_', 'SK_ID_B'], dtype='object')\n",
      "4 Index(['AMT_ANN', 'NAME_CO', 'SK_ID_C', 'NAME_TY', 'NAME_CO', 'AMT_CRE',\n",
      "       'WEEKDAY', 'HOUR_AP', 'SK_ID_P', 'AMT_GOO'],\n",
      "      dtype='object')\n",
      "5 Index(['SK_DPD_', 'MONTHS_', 'SK_ID_C', 'SK_ID_P', 'SK_DPD', 'NAME_CO'], dtype='object')\n",
      "6 Index(['SK_ID_P', 'SK_ID_C'], dtype='object')\n",
      "7 Index(['SK_DPD_', 'MONTHS_', 'SK_ID_C', 'SK_ID_P', 'SK_DPD', 'NAME_CO'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(dfs_3):\n",
    "    print(i, d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs_3[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Aberrant values\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()]\n",
    "    )\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg(\n",
    "        {**num_aggregations, **cat_aggregations}\n",
    "    )\n",
    "    bureau_agg.columns = pd.Index([\n",
    "        \"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\n",
    "        \"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\n",
    "        \"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Aberrant values\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\n",
    "        \"PREV_\" + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\n",
    "        \"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\n",
    "        \"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. pos_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\n",
    "        \"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. installment_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    # Percentage and difference paid in each installment (amount paid and\n",
    "    # installment value)\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\n",
    "        \"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # General aggregations\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace =True)\n",
    "\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\n",
    "        \"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. functions from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "        train_df.shape, test_df.shape\n",
    "    ))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds,\n",
    "                      shuffle=True,\n",
    "                      random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\n",
    "                                                      \"SK_ID_CURR\",\n",
    "                                                      \"SK_ID_BUREAU\",\n",
    "                                                      \"SK_ID_PREV\",\n",
    "                                                      \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "            folds.split(train_df[feats], train_df[\"TARGET\"])\n",
    "    ):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat(\n",
    "            [feature_importance_df, fold_importance_df],\n",
    "            axis=0\n",
    "        )\n",
    "        print(\"Fold %2d AUC : %.6f\" % (\n",
    "            n_fold + 1,\n",
    "            roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        ))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Write submission file and plot feature importance\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(submission_file_name,\n",
    "                                                 index=False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n",
    "        \"feature\"\n",
    "    ).mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[\n",
    "        feature_importance_df_.feature.isin(cols)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"feature\",\n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title(\"LightGBM Features (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"lgbm_importances01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000, test samples: 10000\n",
      "Application train test df shape: (20000, 246)\n",
      "Process application train test - done in 1s\n",
      "Bureau df shape: (2011, 108)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (9734, 242)\n",
      "Process previous_applications - done in 0s\n",
      "Pos-cash balance df shape: (9494, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (8893, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (9520, 131)\n",
      "Process credit card balance - done in 0s\n",
      "-----------------------------------------------------------------------\n",
      "subsampled df's TARGET has       9225 zeros,        775 ones and      10000 NaNs\n",
      "preproc_subsampled - done in 2s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_subsampled\"):\n",
    "        zeros, ones, nans, df = preproc(debug=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Suppression du caractère illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_df = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(\" \", \"_a_\")\\\n",
    "                                  .replace(\"-\", \"_b_\")\\\n",
    "                                  .replace(\":\", \"_c_\")\\\n",
    "                                  .replace(\"/\", \"_d_\")\\\n",
    "                                  .replace(\",\", \"_e_\")\\\n",
    "                                  .replace(\":\", \"_f_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.rename(columns=lambda x: x.replace(\":\", \"deuxpoints\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
