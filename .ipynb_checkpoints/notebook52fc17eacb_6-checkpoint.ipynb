{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (most) package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import imblearn.pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.utils.testing import ignore_warnings # For LogisticRegression\n",
    "#from sklearn.exceptions import ConvergenceWarning # For LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.perf_counter() - t0))\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offline j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"home-credit-default-risk/application_test.csv\",\n",
    "    \"home-credit-default-risk/application_train.csv\",\n",
    "    \"home-credit-default-risk/bureau.csv\",\n",
    "    \"home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"home-credit-default-risk/installments_payments.csv\",\n",
    "    \"home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"home-credit-default-risk/previous_application.csv\",\n",
    "    \"home-credit-default-risk/sample_submission.csv\"\n",
    "    ]\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "not offline (on Kaggle) j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"/kaggle/input/home-credit-default-risk/sample_submission.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_train.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_test.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/previous_application.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/installments_payments.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau.csv\"]\n",
    "    ]\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    le_path = \"home-credit-default-risk/\"\n",
    "else:\n",
    "    le_path = \"/kaggle/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for dirname, _, filenamess in os.walk(le_path):\n",
    "    for filenamee in filenamess:\n",
    "#                        HomeCredit_columns_description.csv est illisible.\n",
    "        if filenamee != \"HomeCredit_columns_description.csv\":\n",
    "            filename = os.path.join(dirname, filenamee)\n",
    "#            print(filename)\n",
    "            filenames.append(filename)\n",
    "#            df = pd.read_csv(filename)\n",
    "#            display(df[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not offline:\n",
    "    flnms = []\n",
    "    flnms.append(filenames[4])\n",
    "    flnms.append(filenames[3])\n",
    "    flnms.append(filenames[8])\n",
    "    flnms.append(filenames[1])\n",
    "    flnms.append(filenames[6])\n",
    "    flnms.append(filenames[7])\n",
    "    flnms.append(filenames[2])\n",
    "    flnms.append(filenames[5])\n",
    "    flnms.append(filenames[0])\n",
    "    filenames = flnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. application_train and application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(filenames[1], nrows=num_rows)\n",
    "    test_df = pd.read_csv(filenames[0], nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Aberrant values\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(filenames[2], nrows=num_rows)\n",
    "    bb = pd.read_csv(filenames[3], nrows=num_rows)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()]\n",
    "    )\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg(\n",
    "        {**num_aggregations, **cat_aggregations}\n",
    "    )\n",
    "    bureau_agg.columns = pd.Index([\n",
    "        \"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\n",
    "        \"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\n",
    "        \"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(filenames[7], nrows=num_rows)\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Aberrant values\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\n",
    "        \"PREV_\" + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\n",
    "        \"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\n",
    "        \"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. pos_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(filenames[6], nrows=num_rows)\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\n",
    "        \"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. installment_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(filenames[5], nrows=num_rows)\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    # Percentage and difference paid in each installment (amount paid and\n",
    "    # installment value)\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\n",
    "        \"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(filenames[4], nrows=num_rows)\n",
    "\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # General aggregations\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace =True)\n",
    "\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\n",
    "        \"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. functions from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "        train_df.shape, test_df.shape\n",
    "    ))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds,\n",
    "                      shuffle=True,\n",
    "                      random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\n",
    "                                                      \"SK_ID_CURR\",\n",
    "                                                      \"SK_ID_BUREAU\",\n",
    "                                                      \"SK_ID_PREV\",\n",
    "                                                      \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "            folds.split(train_df[feats], train_df[\"TARGET\"])\n",
    "    ):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat(\n",
    "            [feature_importance_df, fold_importance_df],\n",
    "            axis=0\n",
    "        )\n",
    "        print(\"Fold %2d AUC : %.6f\" % (\n",
    "            n_fold + 1,\n",
    "            roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        ))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Write submission file and plot feature importance\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(submission_file_name,\n",
    "                                                 index=False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n",
    "        \"feature\"\n",
    "    ).mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[\n",
    "        feature_importance_df_.feature.isin(cols)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"feature\",\n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title(\"LightGBM Features (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"lgbm_importances01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(debug=True):\n",
    "    num_rows = debug if debug else None\n",
    "    with timer(\"Process application train test\"):\n",
    "        df = application_train_test(num_rows)\n",
    "        print(\"Application train test df shape:\", df.shape)\n",
    "        # print(df.dtypes.value_counts())\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del cc\n",
    "        gc.collect()\n",
    "\n",
    "    zeros = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[0]\n",
    "    ones = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[1]\n",
    "    nans = df.TARGET.isna().sum()\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    if debug:\n",
    "        print(\"subsampled df's TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "    else:\n",
    "        print(\"TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "\n",
    "    return zeros, ones, nans, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. A first full run just to measure the target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# I ran this cell only once, just to get the exact values of zo and oz.\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_full\"):\n",
    "        zeros_full, ones_full, nans_full, df_full = preproc(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_full, ones_full, nans_full = 282682, 24825, 48744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo = 11.39 more zeros than ones in TARGET. (and oz = 0.09)\n"
     ]
    }
   ],
   "source": [
    "zo = zeros_full/ones_full\n",
    "oz = ones_full/zeros_full\n",
    "print(\"There is zo =\",\n",
    "      f\"{zo:.2f} more zeros than ones in TARGET. (and oz =\",\n",
    "      f\"{oz:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Subsampled run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000, test samples: 10000\n",
      "Application train test df shape: (20000, 246)\n",
      "Process application train test - done in 1s\n",
      "Bureau df shape: (2011, 108)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (9734, 242)\n",
      "Process previous_applications - done in 0s\n",
      "Pos-cash balance df shape: (9494, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (8893, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (9520, 131)\n",
      "Process credit card balance - done in 0s\n",
      "-----------------------------------------------------------------------\n",
      "subsampled df's TARGET has       9225 zeros,        775 ones and      10000 NaNs\n",
      "preproc_subsampled - done in 3s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_subsampled\"):\n",
    "        # I tried 1_000 here but the ROC AUC scores obtained were > 90% which\n",
    "        # proves overfitting.\n",
    "        zeros, ones, nans, df = preproc(debug=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Measure of the target imbalance after the subsampling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo_sub = 11.90 more zeros than ones in TARGET. (and oz_sub = 0.08)\n"
     ]
    }
   ],
   "source": [
    "zo_sub = zeros/ones\n",
    "oz_sub = ones/zeros\n",
    "print(\"There is zo_sub =\",\n",
    "      f\"{zo_sub:.2f} more zeros than ones in TARGET. (and oz_sub =\",\n",
    "      f\"{oz_sub:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'imbalance de 11.39 des targets du dataset est de 11.90 après subsampling.\n"
     ]
    }
   ],
   "source": [
    "if (zo/zo_sub >= 3/2 or zo/zo_sub <= 2/3):\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "    print(\n",
    "        \"L'imbalance des targets a été fortement modifiée par le subsampling.\",\n",
    "        f\"Elle est passée de {zo:.2f} à {zo_sub:.2f}.\",\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "else:\n",
    "    print(\n",
    "        f\"L'imbalance de {zo:.2f} des targets du dataset est de {zo_sub:.2f}\",\n",
    "        \"après subsampling.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Suppression du caractère illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_df = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "# Ce code prend un temps infini à run. Prende la cell en-dessous.\n",
    "for j in cols_of_df:\n",
    "    df = df.rename(columns={j: re.sub(r\"[ ]\", r\"_a_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[-]\", r\"_b_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_c_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[/]\", r\"_d_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[,]\", r\"_e_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_f_\", j)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "df = df.rename(columns=lambda x: x.replace(\" \", \"_a_\")\\\n",
    "                                  .replace(\"-\", \"_b_\")\\\n",
    "                                  .replace(\":\", \"_c_\")\\\n",
    "                                  .replace(\"/\", \"_d_\")\\\n",
    "                                  .replace(\",\", \"_e_\")\\\n",
    "                                  .replace(\":\", \"_f_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(\":\", \"deuxpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification run from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "with timer(\"Run LightGBM with kfold\"):\n",
    "    feat_importance = kfold_lightgbm(df, num_folds=10, stratified=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. Suppression des données sans TARGET\n",
    "En effet mon but ici c'est juste de créer un modèle qui fonctionne, faire les\n",
    " prédictions de solvabilité des futurs clients ça sera pour plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"TARGET\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"TARGET\", axis=\"columns\")\n",
    "y = df[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7405\n",
       "1.0     595\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_old = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "#X_train_i = imputer.fit_transform(X_train)\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "# X_test = imputer.fit_transform(X_test_old)\n",
    "X_test = imputer.transform(X_test_old) # recommandé par mentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Balancing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy=\"majority\")\n",
    "#X_train_u, y_train_u = undersampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler_1 = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "# La doc de imblearn dit exactement l'inverse mais ici j'ai cette erreur:\n",
    "#\n",
    "# ValueError: The 'sampling_strategy' parameter of RandomOverSampler must be a\n",
    "# float in the range (0, 1], a str among {'majority', 'not majority', 'all',\n",
    "# 'auto', 'not minority'}, an instance of 'collections.abc.Mapping' or a\n",
    "# callable. Got 'minority' instead.\n",
    "oversampler_2 = SMOTE()\n",
    "#X_train_o, y_train_o = oversampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Declaring the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = DummyClassifier()\n",
    "clf1 = LogisticRegression(random_state=42, n_jobs=1, solver=\"sag\")\n",
    "clf2 = SGDClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = MultinomialNB()\n",
    "clf5 = KNeighborsClassifier()\n",
    "clf6 = SVC(probability=True, random_state=42)\n",
    "clf7 = DecisionTreeClassifier(random_state=42)\n",
    "clf8 = RandomForestClassifier(random_state=42)\n",
    "clf9 = GradientBoostingClassifier(random_state=42)\n",
    "clf10 = AdaBoostClassifier(random_state=42)\n",
    "clf11 = XGBClassifier(random_state=42)\n",
    "# Even with logging_level=\"info\" catboost prints millions of lines and crashes\n",
    "# my computer so i need logging_level=\"Silent\" here.\n",
    "clf12 = CatBoostClassifier(random_state=42, logging_level=\"Silent\")\n",
    "clf13 = LGBMClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    clf0,\n",
    "    # LogisticReg has trouble converging and is quite slow.\n",
    "    clf1,\n",
    "    clf2,\n",
    "    clf3,\n",
    "    clf4,\n",
    "    clf5,\n",
    "    # No SVC because it never finishes computing.\n",
    "    # clf6,\n",
    "    clf7,\n",
    "    clf8,\n",
    "    clf9,\n",
    "    clf10,\n",
    "    clf11,\n",
    "    clf12,\n",
    "    clf13,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Declaring the classifiers' parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.0. DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param0 = {}\n",
    "param0[\"classifier__strategy\"] = [\"most_frequent\",\n",
    "                                  \"prior\"]\n",
    "param0[\"classifier\"] = [clf0]\n",
    "#param0[\"classifier\"] = [classifiers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = {}\n",
    "param1[\"classifier__C\"] = [10**-2,\n",
    "                           10**-1,\n",
    "                           10**0,\n",
    "                           10**1,\n",
    "                           10**2]\n",
    "param1[\"classifier__penalty\"] = [\n",
    "    # The default solver lbfgs supports only l2 penalty and None.\n",
    "    # \"l1\",\n",
    "    \"l2\",\n",
    "    # \"elasticnet\",\n",
    "]\n",
    "# param1[\"classifier__class_weight\"] = [{0: 1, 1: 1},\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# param1[\"classifier__class_weight\"] = [None]\n",
    "# param1[\"classifier__class_weight\"] = [None,\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# J'ai déjà fait un oversampling donc normalement je n'ai plus besoin de mettre\n",
    "# les poids. Je teste juste ici.\n",
    "param1[\"classifier\"] = [clf1]\n",
    "#param1[\"classifier\"] = [classifiers[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param2 = {}\n",
    "param2[\"classifier__loss\"] = [\"hinge\",\n",
    "                              \"log\",\n",
    "                              \"squared_hinge\",\n",
    "                              \"modified_huber\"]\n",
    "param2[\"classifier__penalty\"] = [\"l2\",\n",
    "                                 \"l1\",\n",
    "                                 \"elasticnet\"]\n",
    "param2[\"classifier\"] = [clf2]\n",
    "#param2[\"classifier\"] = [classifiers[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3 = {}\n",
    "param3[\"classifier\"] = [clf3]\n",
    "#param3[\"classifier\"] = [classifiers[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param4 = {}\n",
    "param4[\"classifier__alpha\"] = [10**0,\n",
    "                               10**1,\n",
    "                               10**2]\n",
    "param4[\"classifier\"] = [clf4]\n",
    "#param4[\"classifier\"] = [classifiers[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param5 = {}\n",
    "param5[\"classifier__n_neighbors\"] = [10**.5,\n",
    "                                     10**1,\n",
    "                                     10**1.5,\n",
    "                                     10**2]\n",
    "param5[\"classifier__weights\"] = [\"uniform\",\n",
    "                                 \"distance\"]\n",
    "param5[\"classifier\"] = [clf5]\n",
    "#param5[\"classifier\"] = [classifiers[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param6 = {}\n",
    "param6[\"classifier__kernel\"] = [\"linear\",\n",
    "                                \"rbf\",\n",
    "                                \"poly\",\n",
    "                                \"sigmoid\"]\n",
    "param6[\"classifier__C\"] = [10**-2,\n",
    "                           10**-1,\n",
    "                           10**0,\n",
    "                           10**1,\n",
    "                           10**2,\n",
    "                           10**3]\n",
    "param6[\"classifier__gamma\"] = [\"auto\",\n",
    "                               \"scale\"]\n",
    "#param6[\"classifier__class_weight\"] = [None,\n",
    "#                                         {0: oz, 1: zo}]\n",
    "# J'ai déjà fait un oversampling donc plus besoin de mettre les poids.\n",
    "#param6[\"classifier__class_weight\"] = [None]\n",
    "param6[\"classifier\"] = [clf6]\n",
    "#param6[\"classifier\"] = [classifiers[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param7 = {}\n",
    "param7[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30,\n",
    "                                   None]\n",
    "param7[\"classifier__min_samples_split\"] = [10**.5,\n",
    "                                           10**1]\n",
    "#param7[\"classifier__class_weight\"] = [{0: oz, 1: zo}]\n",
    "# J'ai déjà fait un oversampling donc plus besoin de mettre les poids.\n",
    "#param7[\"classifier__class_weight\"] = [None]\n",
    "param7[\"classifier\"] = [clf7]\n",
    "#param7[\"classifier\"] = [classifiers[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5.8. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param8 = {}\n",
    "param8[\"classifier__n_estimators\"] = [10**1,\n",
    "                                      10**2,\n",
    "                                      10**3]\n",
    "param8[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30]\n",
    "param8[\"classifier__criterion\"] = [\"gini\",\n",
    "                                   \"entropy\"]\n",
    "#param8[\"classifier__class_weight\"] = [None,\n",
    "#                                         {0: oz, 1: zo}]\n",
    "# J'ai déjà fait un oversampling donc plus besoin de mettre les poids.\n",
    "#param8[\"classifier__class_weight\"] = [None]\n",
    "param8[\"classifier\"] = [clf8]\n",
    "#param8[\"classifier\"] = [classifiers[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.9. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param9 = {}\n",
    "param9[\"classifier__n_estimators\"] = [10**1,\n",
    "                                      10**2,\n",
    "                                      10**3]\n",
    "param9[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30]\n",
    "param9[\"classifier\"] = [clf9]\n",
    "#param9[\"classifier\"] = [classifiers[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.10. AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param10 = {}\n",
    "param10[\"classifier__n_estimators\"] = [10**1,\n",
    "                                       10**2,\n",
    "                                       10**3]\n",
    "param10[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param10[\"classifier\"] = [clf10]\n",
    "#param10[\"classifier\"] = [classifiers[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.11. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param11 = {}\n",
    "param11[\"classifier__booster\"] = [\"gbtree\",\n",
    "                                  # gblinear doesn't support max_depth\n",
    "                                  # \"gblinear\",\n",
    "                                  \"dart\"]\n",
    "param11[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param11[\"classifier__max_depth\"] = [10**0,\n",
    "                                    3,\n",
    "                                    10**1]\n",
    "param11[\"classifier\"] = [clf11]\n",
    "#param11[\"classifier\"] = [classifiers[11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.12. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {}\n",
    "param12[\"classifier__iterations\"] = [10**1,\n",
    "                                     10**2,\n",
    "                                     10**3]\n",
    "param12[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param12[\"classifier__depth\"] = [10**0,\n",
    "                                3,\n",
    "                                10**1]\n",
    "param12[\"classifier\"] = [clf12]\n",
    "#param12[\"classifier\"] = [classifiers[12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.13. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param13 = {}\n",
    "param13[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "                                        \"dart\",\n",
    "                                        \"goss\"]\n",
    "param13[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param13[\"classifier__num_leaves\"] = [10**1,\n",
    "                                     10**1.5,\n",
    "                                     10**2]\n",
    "param13[\"classifier\"] = [clf13]\n",
    "#param13[\"classifier\"] = [classifiers[13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5bis. Déclaration des samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampler = {}\n",
    "param_sampler[\"sampler\"] = [None,\n",
    "                            undersampler,\n",
    "                            oversampler_1,\n",
    "                            oversampler_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5ter. Listing des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full = [\n",
    "    param0,\n",
    "    param1,\n",
    "    param2,\n",
    "    param3,\n",
    "    param4,\n",
    "    param5,\n",
    "    # param6,\n",
    "    param7,\n",
    "    param8,\n",
    "    param9,\n",
    "    param10,\n",
    "    param11,\n",
    "    param12,\n",
    "    param13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    # 6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Itération sur les classifiers avec une boucle.\n",
    "Je vais gridsearch les paramètres de chacun des estimateurs et enregistrer le\n",
    "best_estimator_ obtenu à chaque fois pour pouvoir comparer les estimateurs.\n",
    "\n",
    "Le problème c'est qu'ici apparemment on veut l'inverse. On veut plutôt random\n",
    "entre tous les estimateurs pour trouver lequel est le meilleur et ne sortir que\n",
    "ses stats à lui. Puis recommencer avec une autre technique de sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.25400e+03, 1.10762e+05, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [1.56100e+03, 1.01832e+05, 1.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# C'était juste pour constater qu'il y a pas mal de valeurs négatives\n",
    "# donc on ne peut pas utiliser MultinomialNB dans l'état\n",
    "# donc je vais rajouter un MinMaxScaler qui met tout entre 0 et 1.\n",
    "for co in df.columns:\n",
    "    neg = (df[co] < 0).sum()\n",
    "    if neg:\n",
    "        print(\"{:<40}\".format(co), \"{:>5.0f}\".format(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "sampler = SMOTE()\n",
    "print(str(sampler)[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    for (n, clf, param) in list(zip(nz, clfs, params_full))[0:]:\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            print(clf, n_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(d):\n",
    "    return d.loc[:, ~d.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7252049773319185\n",
      "Obtained with: LogisticRegression(C=0.1, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 331s\n",
      "Score: 0.7142789053501211\n",
      "Obtained with: SGDClassifier(loss='log', penalty='l1')\n",
      "2 - done in 426s\n",
      "Score: 0.49184743444980966\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 1s\n",
      "Score: 0.6152764144145166\n",
      "Obtained with: MultinomialNB(alpha=1)\n",
      "4 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 571, in _fit\n",
      "    raise TypeError(\n",
      "TypeError: n_neighbors does not take <class 'float'> value, enter integer value\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.55449532 0.55967465        nan        nan\n",
      " 0.62561692 0.62660989]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6266098877093038\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=100, weights='distance')\n",
      "5 - done in 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 259, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float 3.1622776601683795\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.67079421        nan 0.61372171        nan 0.54423312\n",
      "        nan 0.5485148         nan 0.62628022        nan 0.62662237\n",
      "        nan 0.62155993        nan 0.53992022        nan 0.63620538\n",
      "        nan 0.63167347        nan 0.59947004        nan 0.53750305\n",
      "        nan 0.63620652        nan 0.64206674        nan 0.59916477\n",
      "        nan 0.53857149]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6707942055958103\n",
      "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
      "7 - done in 56s\n",
      "Score: 0.7287354104369635\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000,\n",
      "                       random_state=42)\n",
      "8 - done in 1934s\n",
      "Score: 0.7414493954232604\n",
      "Obtained with: GradientBoostingClassifier(random_state=42)\n",
      "9 - done in 4283s\n",
      "Score: 0.7434472506085485\n",
      "Obtained with: AdaBoostClassifier(learning_rate=0.01, n_estimators=1000, random_state=42)\n",
      "10 - done in 1419s\n",
      "Score: 0.7469164032932553\n",
      "Obtained with: XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 831s\n",
      "Score: 0.7417983533724091\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000151C53E4520>\n",
      "12 - done in 2271s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Parameter num_leaves should be of type int, got \"31.622776601683793\"\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.68966063        nan 0.65821073 0.73534802        nan 0.70366491\n",
      " 0.73645334        nan 0.71777416 0.66501909        nan 0.66535557\n",
      " 0.71553459        nan 0.70902808 0.7041631         nan 0.699011\n",
      " 0.67924977        nan 0.66891664 0.72047958        nan 0.70882211\n",
      " 0.70403827        nan 0.70322119 0.69055601        nan 0.67861143\n",
      " 0.72449458        nan 0.70940087 0.71942419        nan 0.70728386\n",
      " 0.67500837        nan 0.63503538 0.71031781        nan 0.67390532\n",
      " 0.74309375        nan 0.71904856 0.63798819        nan 0.64700719\n",
      " 0.69334654        nan 0.67485914 0.7195615         nan 0.71623988\n",
      " 0.6559842         nan 0.65087239 0.69716578        nan 0.68213222\n",
      " 0.71961938        nan 0.72452295 0.66901707        nan 0.65054557\n",
      " 0.70255732        nan 0.68845999 0.72840631        nan 0.71690035\n",
      " 0.68966063        nan 0.65821073 0.73534802        nan 0.70366491\n",
      " 0.72665301        nan 0.7150381  0.66501909        nan 0.66535557\n",
      " 0.71553459        nan 0.70902808 0.72291264        nan 0.70507436\n",
      " 0.67924977        nan 0.66891664 0.72047958        nan 0.70882211\n",
      " 0.71199678        nan 0.7073894  0.69055601        nan 0.67861143\n",
      " 0.72449458        nan 0.70940087 0.73397035        nan 0.70310998]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7430937533690047\n",
      "Obtained with: LGBMClassifier(boosting_type='dart', num_leaves=10, random_state=42)\n",
      "13 - done in 285s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.743</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "12  <catboost.core.CatBoostClassifier object at 0x...  0.742         84.00\n",
       "13  LGBMClassifier(boosting_type='dart', num_leave...  0.743          2.63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7098065694880249\n",
      "Obtained with: LogisticRegression(C=0.1, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 60s\n",
      "Score: 0.6805326857279035\n",
      "Obtained with: SGDClassifier(penalty='elasticnet')\n",
      "2 - done in 61s\n",
      "Score: 0.5325092629894631\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 1s\n",
      "Score: 0.6550230085281918\n",
      "Obtained with: MultinomialNB(alpha=100)\n",
      "4 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 571, in _fit\n",
      "    raise TypeError(\n",
      "TypeError: n_neighbors does not take <class 'float'> value, enter integer value\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.58536646 0.58765937        nan        nan\n",
      " 0.6379496  0.63962687]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6396268703294957\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=100, weights='distance')\n",
      "5 - done in 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 259, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float 3.1622776601683795\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.67116416        nan 0.60438382        nan 0.58898031\n",
      "        nan 0.5980413         nan 0.5040882         nan 0.51171023\n",
      "        nan 0.53665307        nan 0.57427811        nan 0.50610648\n",
      "        nan 0.51024688        nan 0.57352799        nan 0.58912273\n",
      "        nan 0.50690937        nan 0.51766749        nan 0.57754924\n",
      "        nan 0.56092068]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6711641577630376\n",
      "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
      "7 - done in 18s\n",
      "Score: 0.7216592241217892\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=30, n_estimators=1000,\n",
      "                       random_state=42)\n",
      "8 - done in 405s\n",
      "Score: 0.7233801825929561\n",
      "Obtained with: GradientBoostingClassifier(random_state=42)\n",
      "9 - done in 247s\n",
      "Score: 0.731815318970262\n",
      "Obtained with: AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=42)\n",
      "10 - done in 219s\n",
      "Score: 0.7344798824323787\n",
      "Obtained with: XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=1, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 194s\n",
      "Score: 0.7418982177611084\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000151C53E4520>\n",
      "12 - done in 1356s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Parameter num_leaves should be of type int, got \"31.622776601683793\"\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.68706189        nan 0.68132763 0.71949171        nan 0.72548131\n",
      " 0.71641578        nan 0.72124785 0.6699238         nan 0.67389284\n",
      " 0.71268221        nan 0.70865132 0.71105601        nan 0.69762084\n",
      " 0.65931945        nan 0.68583061 0.71046079        nan 0.7109709\n",
      " 0.70518217        nan 0.71216587 0.67801792        nan 0.66825447\n",
      " 0.70404054        nan 0.70164039 0.70313381        nan 0.71236219\n",
      " 0.6721367         nan 0.65422523 0.6951753         nan 0.69683895\n",
      " 0.7172998         nan 0.71555218 0.65380137        nan 0.67888436\n",
      " 0.68269452        nan 0.6782341  0.71610143        nan 0.71738491\n",
      " 0.6660688         nan 0.65041847 0.66919751        nan 0.68245678\n",
      " 0.70773155        nan 0.71639875 0.63656852        nan 0.64972055\n",
      " 0.68682244        nan 0.6960599  0.69931343        nan 0.70684922\n",
      " 0.68619772        nan 0.67440408 0.72024353        nan 0.71473283\n",
      " 0.69676519        nan 0.69938663 0.67831751        nan 0.68315072\n",
      " 0.71438501        nan 0.71540011 0.69266734        nan 0.70545339\n",
      " 0.66041342        nan 0.68797826 0.71829561        nan 0.71892941\n",
      " 0.68591969        nan 0.70528317 0.66059669        nan 0.67084697\n",
      " 0.71348226        nan 0.70132037 0.70018327        nan 0.69970551]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7254813066347404\n",
      "Obtained with: LGBMClassifier(learning_rate=0.01, num_leaves=100, random_state=42)\n",
      "13 - done in 94s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier(learning_rate=0.01, num_leaves=...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.742        50.000\n",
       "13  LGBMClassifier(learning_rate=0.01, num_leaves=...  0.725         0.873"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7216019155805469\n",
      "Obtained with: LogisticRegression(C=0.01, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 591s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6889666872826106\n",
      "Obtained with: SGDClassifier(loss='log', penalty='elasticnet')\n",
      "2 - done in 1704s\n",
      "Score: 0.4919569448305993\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 1s\n",
      "Score: 0.6329836188357856\n",
      "Obtained with: MultinomialNB(alpha=100)\n",
      "4 - done in 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 571, in _fit\n",
      "    raise TypeError(\n",
      "TypeError: n_neighbors does not take <class 'float'> value, enter integer value\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5626127  0.56215537        nan        nan\n",
      " 0.59947458 0.60547949]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6054794909185822\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=100, weights='distance')\n",
      "5 - done in 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 259, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float 3.1622776601683795\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.68152395        nan 0.58520929        nan 0.55370888\n",
      "        nan 0.53655377        nan 0.50682653        nan 0.52390163\n",
      "        nan 0.51906729        nan 0.53658952        nan 0.50726173\n",
      "        nan 0.52390163        nan 0.53869632        nan 0.52156163\n",
      "        nan 0.50682653        nan 0.52390163        nan 0.59135265\n",
      "        nan 0.53393347]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6815239532680053\n",
      "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
      "7 - done in 230s\n",
      "Score: 0.7165984827421854\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=30, n_estimators=1000,\n",
      "                       random_state=42)\n",
      "8 - done in 3980s\n",
      "Score: 0.7332792401227877\n",
      "Obtained with: GradientBoostingClassifier(random_state=42)\n",
      "9 - done in 5042s\n",
      "Score: 0.7414471257780628\n",
      "Obtained with: AdaBoostClassifier(learning_rate=0.01, n_estimators=1000, random_state=42)\n",
      "10 - done in 2106s\n",
      "Score: 0.7417234550808846\n",
      "Obtained with: XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=1, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 1174s\n",
      "Score: 0.746853988050318\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000151C53E4520>\n",
      "12 - done in 2977s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Parameter num_leaves should be of type int, got \"31.622776601683793\"\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.70166705        nan 0.62731575 0.72605836        nan 0.69808442\n",
      " 0.73619687        nan 0.70624436 0.65563071        nan 0.67827552\n",
      " 0.70503918        nan 0.70033988 0.64475513        nan 0.66676275\n",
      " 0.6567451         nan 0.6714212  0.70573766        nan 0.69521672\n",
      " 0.66914588        nan 0.66701922 0.64783221        nan 0.67835553\n",
      " 0.70920001        nan 0.70060543 0.66660614        nan 0.68689677\n",
      " 0.68621644        nan 0.61522421 0.71329842        nan 0.67973207\n",
      " 0.73656852        nan 0.71894189 0.65944768        nan 0.66501853\n",
      " 0.6844552         nan 0.69783135 0.68378509        nan 0.69233938\n",
      " 0.65255363        nan 0.66042533 0.6816607         nan 0.67910962\n",
      " 0.6840756         nan 0.69498806 0.63453833        nan 0.67426222\n",
      " 0.67304172        nan 0.6858533  0.68566776        nan 0.69680491\n",
      " 0.69384302        nan 0.64988623 0.72573324        nan 0.70359739\n",
      " 0.74177339        nan 0.71820766 0.65628947        nan 0.68058262\n",
      " 0.69886234        nan 0.69847083 0.68075511        nan 0.66435806\n",
      " 0.65126731        nan 0.68772406 0.70441843        nan 0.70143498\n",
      " 0.67924466        nan 0.66559729 0.66464063        nan 0.67788855\n",
      " 0.70673347        nan 0.7065502  0.67415498        nan 0.67106259]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7417733872752342\n",
      "Obtained with: LGBMClassifier(boosting_type='goss', num_leaves=10, random_state=42)\n",
      "13 - done in 348s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.747</td>\n",
       "      <td>110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier(boosting_type='goss', num_leave...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.747        110.00\n",
       "13  LGBMClassifier(boosting_type='goss', num_leave...  0.742          3.22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7110548743467677\n",
      "Obtained with: LogisticRegression(C=0.01, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 570s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6933732034339732\n",
      "Obtained with: SGDClassifier(class_weight={0: 0.07025562292611486, 1: 14.2337361530715},\n",
      "              loss='log', penalty='elasticnet')\n",
      "2 - done in 1396s\n",
      "Score: 0.4926548607288965\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 2s\n",
      "Score: 0.6253553413262671\n",
      "Obtained with: MultinomialNB(alpha=100)\n",
      "4 - done in 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 571, in _fit\n",
      "    raise TypeError(\n",
      "TypeError: n_neighbors does not take <class 'float'> value, enter integer value\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.57204535 0.57446252        nan        nan\n",
      " 0.6119911  0.61405478]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6140547778868468\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=100, weights='distance')\n",
      "5 - done in 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 259, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float 3.1622776601683795\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.61586766        nan 0.5807188         nan 0.54196234\n",
      "        nan 0.53978007        nan 0.50682653        nan 0.52390163\n",
      "        nan 0.55786858        nan 0.52463586        nan 0.52145042\n",
      "        nan 0.54244407        nan 0.57724284        nan 0.54910264\n",
      "        nan 0.53378537        nan 0.58564279        nan 0.5864837\n",
      "        nan 0.53590749]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6158676569885213\n",
      "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
      "7 - done in 273s\n",
      "Score: 0.6953233960701093\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=30, n_estimators=1000,\n",
      "                       random_state=42)\n",
      "8 - done in 4437s\n",
      "Score: 0.6977876633435279\n",
      "Obtained with: GradientBoostingClassifier(random_state=42)\n",
      "9 - done in 8331s\n",
      "Score: 0.7020818320576037\n",
      "Obtained with: AdaBoostClassifier(learning_rate=0.1, n_estimators=1000, random_state=42)\n",
      "10 - done in 2823s\n",
      "Score: 0.7097452890676865\n",
      "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 1123s\n",
      "Score: 0.7145353752574628\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000151C53E4520>\n",
      "12 - done in 5674s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Parameter num_leaves should be of type int, got \"31.622776601683793\"\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.62677444        nan 0.61967782 0.66978762        nan 0.65512571\n",
      " 0.72092783        nan 0.70579611 0.61289669        nan 0.62102826\n",
      " 0.66220303        nan 0.65076629 0.61635506        nan 0.64853863\n",
      " 0.63258019        nan 0.63713707 0.67197612        nan 0.63906173\n",
      " 0.63965184        nan 0.62302442 0.62311974        nan 0.62121608\n",
      " 0.6764513         nan 0.65757636 0.65974614        nan 0.63619176\n",
      " 0.63069468        nan 0.61466588 0.65086559        nan 0.63682329\n",
      " 0.69283189        nan 0.69290566 0.59705797        nan 0.61589773\n",
      " 0.63205306        nan 0.62987477 0.64565448        nan 0.62765563\n",
      " 0.60436566        nan 0.62495759 0.63493551        nan 0.63545356\n",
      " 0.65346036        nan 0.63251153 0.58655065        nan 0.62224252\n",
      " 0.6347982         nan 0.63307894 0.66829362        nan 0.64522949\n",
      " 0.62090684        nan 0.6336577  0.67172646        nan 0.65885814\n",
      " 0.71759713        nan 0.70493024 0.62281901        nan 0.62600276\n",
      " 0.66215537        nan 0.64434433 0.6251942         nan 0.63086717\n",
      " 0.62413144        nan 0.62500752 0.67304172        nan 0.64475059\n",
      " 0.62806416        nan 0.61687254 0.63457804        nan 0.63248033\n",
      " 0.67839128        nan 0.645621   0.65003887        nan 0.64591152]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7209278309568256\n",
      "Obtained with: LGBMClassifier(num_leaves=10, random_state=42)\n",
      "13 - done in 436s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.715</td>\n",
       "      <td>210.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier(num_leaves=10, random_state=42)</td>\n",
       "      <td>0.721</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.715        210.00\n",
       "13     LGBMClassifier(num_leaves=10, random_state=42)  0.721          4.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%script echo\n",
    "result_dfs = dict()\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    # I create a different results table for each sampler.\n",
    "    # str(sampler) includes parentheses and the arguments inside, so i strip\n",
    "    # these out in order to keep only the name of the sampler.\n",
    "    nom_du_df = \"result_df_\" + str(sampler).split('(')[0] + \".csv\"\n",
    "    result_df = pd.DataFrame(columns=[\"classifier\", \"score\", \"run_time (s)\"])\n",
    "    # I gridsearch each classifier with its own set of parameters.\n",
    "    for (n, clf, param) in list(zip(nz, clfs, params_full))[0:]:\n",
    "        with timer(str(n)):\n",
    "            try:\n",
    "                result_df = pd.read_csv(nom_du_df)\n",
    "                result_df = clean_csv(result_df)\n",
    "            except ValueError:\n",
    "                print(\"An error occurred while reading the CSV file.\")\n",
    "            except FileNotFoundError:\n",
    "                print(\"The file could not be found.\")\n",
    "\n",
    "            t_clf = time.perf_counter()\n",
    "\n",
    "            # Without sampler i still try to optimize the gridsearch using class_weight.\n",
    "            if sampler is None:\n",
    "                # class_weight does not exist for dummy, NBayes, KN, GB, AdaB, XGB.\n",
    "                # From my testing, class_weight isn't supported by CatBoost either.\n",
    "                if n not in [0, 3, 4, 5, 9, 10, 11, 12]:\n",
    "                    param[\"classifier__class_weight\"] = [None,\n",
    "                                                         {0: .8*oz, 1: zo/.8},\n",
    "                                                         {0: oz, 1: zo},\n",
    "                                                         {0: oz/.8, 1: .8*zo},]\n",
    "\n",
    "            # Train the model\n",
    "            # I need a MinMaxScaler in the pipe because (at least) MultinomialNB\n",
    "            # errors on negative values.\n",
    "            pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                                   (\"sampler\", sampler),\n",
    "                                                   (\"classifier\", clf)])\n",
    "            gs = GridSearchCV(\n",
    "                pipeline,\n",
    "                param,\n",
    "                cv=5,\n",
    "                n_jobs=None,\n",
    "                scoring=\"roc_auc\",\n",
    "                # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "                # refit=\"roc_auc\",\n",
    "                # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "                # random_state=42, # seulement pour RandomSearchCV ?\n",
    "            ).fit(X_train, y_train)\n",
    "\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "            if t_mean > 10:\n",
    "                t_mean = round(t_mean, 0)\n",
    "            elif t_mean > 1:\n",
    "                t_mean = round(t_mean, 2)\n",
    "            elif t_mean > .001:\n",
    "                t_mean = round(t_mean, 3)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Save the results\n",
    "            print(\"Score:\", gs.best_score_)\n",
    "            print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "            result_df.loc[n, :] = [gs.best_params_[\"classifier\"],\n",
    "                                   round(gs.best_score_, 3),\n",
    "                                   t_mean]\n",
    "\n",
    "            # Je save à chaque classifier pour pouvoir relancer la loop à partir\n",
    "            # du classifier qui bug en cas d'erreur.\n",
    "            result_df.sort_index(inplace=True)\n",
    "            result_df.to_csv(nom_du_df)\n",
    "\n",
    "    result_dfs[str(sampler)] = result_df\n",
    "    #display(result_df[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LGBMClassifier(num_leaves=10, random_state=42), 0.7209278309568256, 4.04]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[gs.best_params_[\"classifier\"], gs.best_score_, t_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "print(param9)\n",
    "print([len(i) for i in param9.values()])\n",
    "print(math.prod([len(i) for i in param9.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. Itération sur les classifiers avec une méthode étrange\n",
    "En fait ici on ne donne au SearchCV qu'une seule pipe de travail ne contenant\n",
    "qu'un seul estimateur, mais il va quand même itérer sur tous les classifiers\n",
    "car ils sont tous renseignés dans ```params```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "params = [\n",
    "    # DummyClassifier\n",
    "    param0,\n",
    "    # LogisticRegression\n",
    "    # param1,\n",
    "    # SGDClassifier\n",
    "    # param2,\n",
    "    # GaussianNB\n",
    "    # param3,\n",
    "    # MultinomialNB\n",
    "    # param4,\n",
    "    # KNeighborsClassifier\n",
    "    param5,\n",
    "    # SVC\n",
    "    # param6,\n",
    "    # DecisionTreeClassifier\n",
    "    # param7,\n",
    "    # RandomForestClassifier\n",
    "    param8,\n",
    "    # GradientBoostingClassifier\n",
    "    # param9,\n",
    "    # AdaBoostClassifier\n",
    "    # param10,\n",
    "    # XGBClassifier\n",
    "    param11,\n",
    "    # CatBoostClassifier\n",
    "    # param12,\n",
    "    # LGBMClassifier\n",
    "    # param13,\n",
    "    # Samplers\n",
    "    # param_sampler,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "%%time\n",
    "rs = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    # n_jobs=-1,\n",
    "    scoring=[\"roc_auc\", \"accuracy\"],\n",
    "    refit=\"roc_auc\",\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "#).fit(X_train_u, y_train_u)\n",
    "#).fit(X_train_o, y_train_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_N = clean_csv(pd.read_csv(\"result_df_None.csv\")).sort_values(by=\"score\")\n",
    "res_U = clean_csv(pd.read_csv(\"result_df_RandomUnderSampler.csv\")).sort_values(by=\"score\")\n",
    "res_O = clean_csv(pd.read_csv(\"result_df_RandomOverSampler.csv\")).sort_values(by=\"score\")\n",
    "res_S = clean_csv(pd.read_csv(\"result_df_SMOTE.csv\")).sort_values(by=\"score\")\n",
    "res = [res_N, res_U, res_O, res_S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.741</td>\n",
       "      <td>476.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdaBoostClassifier(learning_rate=0.01, n_estim...</td>\n",
       "      <td>0.743</td>\n",
       "      <td>158.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.743</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='gbtree...</td>\n",
       "      <td>0.747</td>\n",
       "      <td>46.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "9         GradientBoostingClassifier(random_state=42)  0.741        476.00\n",
       "12  <catboost.core.CatBoostClassifier object at 0x...  0.742         84.00\n",
       "10  AdaBoostClassifier(learning_rate=0.01, n_estim...  0.743        158.00\n",
       "13  LGBMClassifier(boosting_type='dart', num_leave...  0.743          2.63\n",
       "11  XGBClassifier(base_score=None, booster='gbtree...  0.747         46.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.723</td>\n",
       "      <td>27.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMClassifier(learning_rate=0.01, num_leaves=...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoostClassifier(learning_rate=0.1, n_estima...</td>\n",
       "      <td>0.732</td>\n",
       "      <td>24.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='gbtree...</td>\n",
       "      <td>0.734</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "8         GradientBoostingClassifier(random_state=42)  0.723        27.000\n",
       "12  LGBMClassifier(learning_rate=0.01, num_leaves=...  0.725         0.873\n",
       "9   AdaBoostClassifier(learning_rate=0.1, n_estima...  0.732        24.000\n",
       "10  XGBClassifier(base_score=None, booster='gbtree...  0.734        11.000\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.742        50.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.733</td>\n",
       "      <td>560.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoostClassifier(learning_rate=0.01, n_estim...</td>\n",
       "      <td>0.741</td>\n",
       "      <td>234.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='gbtree...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>65.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMClassifier(boosting_type='goss', num_leave...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.747</td>\n",
       "      <td>110.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "8         GradientBoostingClassifier(random_state=42)  0.733        560.00\n",
       "9   AdaBoostClassifier(learning_rate=0.01, n_estim...  0.741        234.00\n",
       "10  XGBClassifier(base_score=None, booster='gbtree...  0.742         65.00\n",
       "12  LGBMClassifier(boosting_type='goss', num_leave...  0.742          3.22\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.747        110.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoostClassifier(learning_rate=0.1, n_estima...</td>\n",
       "      <td>0.702</td>\n",
       "      <td>314.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.710</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.01, n_jobs=1, random_st...</td>\n",
       "      <td>0.711</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.715</td>\n",
       "      <td>210.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMClassifier(num_leaves=10, random_state=42)</td>\n",
       "      <td>0.721</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  score  run_time (s)\n",
       "9   AdaBoostClassifier(learning_rate=0.1, n_estima...  0.702        314.00\n",
       "10  XGBClassifier(base_score=None, booster='dart',...  0.710         62.00\n",
       "1   LogisticRegression(C=0.01, n_jobs=1, random_st...  0.711         29.00\n",
       "11  <catboost.core.CatBoostClassifier object at 0x...  0.715        210.00\n",
       "12     LGBMClassifier(num_leaves=10, random_state=42)  0.721          4.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in res:\n",
    "    display(i[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\\n              colsample_bylevel=None, colsample_bynode=None,\\n              colsample_bytree=None, early_stopping_rounds=None,\\n              enable_categorical=False, eval_metric=None, feature_types=None,\\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\\n              max_cat_threshold=None, max_cat_to_onehot=None,\\n              max_delta_step=None, max_depth=3, max_leaves=None,\\n              min_child_weight=None, missing=nan, monotone_constraints=None,\\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\\n              predictor=None, random_state=42, ...)\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_N.iloc[-1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partons sur les résultats de SMOTE (parce que ça fait partie de l'exercice)\n",
    "même s'ils ne sont pas les meilleurs, ça me donnera une marge de progression\n",
    "pour le fine tuning, au moins.  \n",
    "Et ça tombe bien parce que LGBM compute vite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fine tuning with the best classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "                                     \"rf\",\n",
    "                                     \"dart\"]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,\n",
    "                                     10**-1.5,\n",
    "                                     10**-1]\n",
    "parm[\"classifier__num_leaves\"] = [10**1,\n",
    "                                  10**1.5,\n",
    "                                  10**2]\n",
    "parm[\"classifier__num_iterations\"] = [10**3,\n",
    "                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [6,\n",
    "                                 8]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est trop à mon avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "#                                     \"rf\",\n",
    "#                                     \"dart\"]\n",
    "                                    ]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,\n",
    "                                     10**-1.5,\n",
    "                                     10**-1]\n",
    "#parm[\"classifier__num_leaves\"] = [10**1,\n",
    "#                                  10**1.5,\n",
    "#                                  10**2]\n",
    "#parm[\"classifier__num_iterations\"] = [10**3,\n",
    "#                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [6,\n",
    "                                 8]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok testons ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m t_clf \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m      3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m imblearn\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mPipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, scaler),\n\u001b[0;32m      4\u001b[0m                                        (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m, oversampler_2),\n\u001b[0;32m      5\u001b[0m                                        (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, clf13)])\n\u001b[0;32m      6\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m      7\u001b[0m     pipeline,\n\u001b[0;32m      8\u001b[0m     parm,\n\u001b[0;32m      9\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     10\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# scoring=[\"roc_auc\", \"accuracy\"],\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# refit=\"roc_auc\",\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# n_iter=5, # seulement pour RandomSearchCV ?\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# random_state=42, # seulement pour RandomSearchCV ?\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m )\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     18\u001b[0m n_loops \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod([\u001b[38;5;28mlen\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m parm\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[0;32m     19\u001b[0m t_mean \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m t_clf)\u001b[38;5;241m/\u001b[39m(n_loops)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Applying the fine-tuned best classifier for the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = rs_cv.best_estimator_[best_algorithm].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Features' importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 2\u001b[0m       precision_score(\u001b[43mrs\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test), y_test))\n\u001b[0;32m      3\u001b[0m       \u001b[38;5;66;03m# precision_score(rs.best_estimator_.predict(X_test), y_test))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m       recall_score(rs\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X_test), y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rs' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\",\n",
    "      precision_score(rs.predict(X_test), y_test))\n",
    "      # precision_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"Recall:\",\n",
    "      recall_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"ROC AUC Score:\",\n",
    "      roc_auc_score(rs.best_estimator_.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot la courbe roc du meilleur pour chaque sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap (et lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "#rs_cv.fit(X_train, y_train)\n",
    "#rs_cv.fit(X_train_u, y_train_u)\n",
    "rs_cv.fit(X_train_o, y_train_o)\n",
    "\n",
    "for algorithm in classifiers.keys():\n",
    "    print(f\"Best parameters for {algorithm}: {rs_cv.best_params_[algorithm]}\")\n",
    "    print(f\"Best AUC score for {algorithm}: {rs_cv.best_score_[algorithm]['roc_auc']:.3f}\")\n",
    "    print(f\"Best accuracy score for {algorithm}: {rs_cv.best_score_[algorithm]['accuracy']:.3f}\")\n",
    "\n",
    "#best_algorithm = rs_cv.best_estimator_.keys()[0]\n",
    "best_algorithm = rs_cv.best_estimator_.named_steps.keys()\n",
    "print(f\"Overall best algorithm: {best_algorithm}\")\n",
    "print(f\"Best AUC score: {rs_cv.best_score_[best_algorithm]['roc_auc']:.3f}\")\n",
    "print(f\"Best accuracy score: {rs_cv.best_score_[best_algorithm]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
