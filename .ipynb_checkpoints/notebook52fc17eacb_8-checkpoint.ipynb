{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle/python Docker image: https://github.com/kaggle/docker-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (most) package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import imblearn.pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.utils.testing import ignore_warnings # For LogisticRegression\n",
    "#from sklearn.exceptions import ConvergenceWarning # For LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.perf_counter() - t0))\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offline j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"home-credit-default-risk/application_test.csv\",\n",
    "    \"home-credit-default-risk/application_train.csv\",\n",
    "    \"home-credit-default-risk/bureau.csv\",\n",
    "    \"home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"home-credit-default-risk/installments_payments.csv\",\n",
    "    \"home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"home-credit-default-risk/previous_application.csv\",\n",
    "    \"home-credit-default-risk/sample_submission.csv\"\n",
    "    ]\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "not offline (on Kaggle) j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"/kaggle/input/home-credit-default-risk/sample_submission.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_train.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_test.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/previous_application.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/installments_payments.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau.csv\"]\n",
    "    ]\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    le_path = \"home-credit-default-risk/\"\n",
    "else:\n",
    "    le_path = \"/kaggle/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for dirname, _, filenamess in os.walk(le_path):\n",
    "    for filenamee in filenamess:\n",
    "#                        HomeCredit_columns_description.csv est illisible.\n",
    "        if filenamee != \"HomeCredit_columns_description.csv\":\n",
    "            filename = os.path.join(dirname, filenamee)\n",
    "#            print(filename)\n",
    "            filenames.append(filename)\n",
    "#            df = pd.read_csv(filename)\n",
    "#            display(df[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not offline:\n",
    "    flnms = []\n",
    "    flnms.append(filenames[4])\n",
    "    flnms.append(filenames[3])\n",
    "    flnms.append(filenames[8])\n",
    "    flnms.append(filenames[1])\n",
    "    flnms.append(filenames[6])\n",
    "    flnms.append(filenames[7])\n",
    "    flnms.append(filenames[2])\n",
    "    flnms.append(filenames[5])\n",
    "    flnms.append(filenames[0])\n",
    "    filenames = flnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. application_train and application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(filenames[1], nrows=num_rows)\n",
    "    test_df = pd.read_csv(filenames[0], nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Aberrant values\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(filenames[2], nrows=num_rows)\n",
    "    bb = pd.read_csv(filenames[3], nrows=num_rows)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()]\n",
    "    )\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg(\n",
    "        {**num_aggregations, **cat_aggregations}\n",
    "    )\n",
    "    bureau_agg.columns = pd.Index([\n",
    "        \"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\n",
    "        \"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\n",
    "        \"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(filenames[7], nrows=num_rows)\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Aberrant values\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\n",
    "        \"PREV_\" + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\n",
    "        \"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\n",
    "        \"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. pos_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(filenames[6], nrows=num_rows)\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\n",
    "        \"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. installment_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(filenames[5], nrows=num_rows)\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    # Percentage and difference paid in each installment (amount paid and\n",
    "    # installment value)\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\n",
    "        \"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(filenames[4], nrows=num_rows)\n",
    "\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # General aggregations\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace =True)\n",
    "\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\n",
    "        \"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. functions from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "        train_df.shape, test_df.shape\n",
    "    ))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds,\n",
    "                      shuffle=True,\n",
    "                      random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\n",
    "                                                      \"SK_ID_CURR\",\n",
    "                                                      \"SK_ID_BUREAU\",\n",
    "                                                      \"SK_ID_PREV\",\n",
    "                                                      \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "            folds.split(train_df[feats], train_df[\"TARGET\"])\n",
    "    ):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat(\n",
    "            [feature_importance_df, fold_importance_df],\n",
    "            axis=0\n",
    "        )\n",
    "        print(\"Fold %2d AUC : %.6f\" % (\n",
    "            n_fold + 1,\n",
    "            roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        ))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Write submission file and plot feature importance\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(submission_file_name,\n",
    "                                                 index=False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n",
    "        \"feature\"\n",
    "    ).mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[\n",
    "        feature_importance_df_.feature.isin(cols)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"feature\",\n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title(\"LightGBM Features (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"lgbm_importances01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(debug=True):\n",
    "    num_rows = debug if debug else None\n",
    "    with timer(\"Process application train test\"):\n",
    "        df = application_train_test(num_rows)\n",
    "        print(\"Application train test df shape:\", df.shape)\n",
    "        # print(df.dtypes.value_counts())\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del cc\n",
    "        gc.collect()\n",
    "\n",
    "    zeros = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[0]\n",
    "    ones = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[1]\n",
    "    nans = df.TARGET.isna().sum()\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    if debug:\n",
    "        print(\"subsampled df's TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "    else:\n",
    "        print(\"TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "\n",
    "    return zeros, ones, nans, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. A first full run just to measure the target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# I ran this cell only once, just to get the exact values of zo and oz.\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_full\"):\n",
    "        zeros_full, ones_full, nans_full, df_full = preproc(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_full, ones_full, nans_full = 282682, 24825, 48744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo = 11.39 more zeros than ones in TARGET. (and oz = 0.09)\n"
     ]
    }
   ],
   "source": [
    "zo = zeros_full/ones_full\n",
    "oz = ones_full/zeros_full\n",
    "print(\"There is zo =\",\n",
    "      f\"{zo:.2f} more zeros than ones in TARGET. (and oz =\",\n",
    "      f\"{oz:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Subsampled run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000, test samples: 10000\n",
      "Application train test df shape: (20000, 246)\n",
      "Process application train test - done in 1s\n",
      "Bureau df shape: (2011, 108)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (9734, 242)\n",
      "Process previous_applications - done in 0s\n",
      "Pos-cash balance df shape: (9494, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (8893, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (9520, 131)\n",
      "Process credit card balance - done in 0s\n",
      "-----------------------------------------------------------------------\n",
      "subsampled df's TARGET has       9225 zeros,        775 ones and      10000 NaNs\n",
      "preproc_subsampled - done in 2s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_subsampled\"):\n",
    "        # I tried 1_000 here but the ROC AUC scores obtained were > 90% which\n",
    "        # proves overfitting.\n",
    "        zeros, ones, nans, df = preproc(debug=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Measure of the target imbalance after the subsampling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo_sub = 11.90 more zeros than ones in TARGET. (and oz_sub = 0.08)\n"
     ]
    }
   ],
   "source": [
    "zo_sub = zeros/ones\n",
    "oz_sub = ones/zeros\n",
    "print(\"There is zo_sub =\",\n",
    "      f\"{zo_sub:.2f} more zeros than ones in TARGET. (and oz_sub =\",\n",
    "      f\"{oz_sub:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'imbalance de 11.39 des targets du dataset est de 11.90 après subsampling.\n"
     ]
    }
   ],
   "source": [
    "if (zo/zo_sub >= 3/2 or zo/zo_sub <= 2/3):\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "    print(\n",
    "        \"L'imbalance des targets a été fortement modifiée par le subsampling.\",\n",
    "        f\"Elle est passée de {zo:.2f} à {zo_sub:.2f}.\",\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "else:\n",
    "    print(\n",
    "        f\"L'imbalance de {zo:.2f} des targets du dataset est de {zo_sub:.2f}\",\n",
    "        \"après subsampling.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Suppression du caractère illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_df = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "# Ce code prend un temps infini à run. Prende la cell en-dessous.\n",
    "for j in cols_of_df:\n",
    "    df = df.rename(columns={j: re.sub(r\"[ ]\", r\"_a_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[-]\", r\"_b_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_c_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[/]\", r\"_d_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[,]\", r\"_e_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_f_\", j)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "df = df.rename(columns=lambda x: x.replace(\" \", \"_a_\")\\\n",
    "                                  .replace(\"-\", \"_b_\")\\\n",
    "                                  .replace(\":\", \"_c_\")\\\n",
    "                                  .replace(\"/\", \"_d_\")\\\n",
    "                                  .replace(\",\", \"_e_\")\\\n",
    "                                  .replace(\":\", \"_f_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(\":\", \"deuxpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification run from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "with timer(\"Run LightGBM with kfold\"):\n",
    "    feat_importance = kfold_lightgbm(df, num_folds=10, stratified=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. Suppression des données sans TARGET et colonnes sans données\n",
    "En effet mon but ici c'est juste de créer un modèle qui fonctionne, faire les\n",
    " prédictions de solvabilité des futurs clients ça sera pour plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"TARGET\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_notnull = list(df.loc[:, df.notnull().sum() > 0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, cols_notnull]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"TARGET\", axis=\"columns\")\n",
    "y = df[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7405\n",
       "1.0     595\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_old = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names_old = list(X_train.columns)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "#X_train_i = imputer.fit_transform(X_train)\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "feature_names = imputer.get_feature_names_out()\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_old = X_test.copy()\n",
    "# X_test = imputer.fit_transform(X_test_old)\n",
    "X_test = imputer.transform(X_test_old) # pas de fit, recommandé par mentor\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 748)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Balancing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy=\"majority\")\n",
    "#X_train_u, y_train_u = undersampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler_1 = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "# La doc de imblearn dit exactement l'inverse mais ici j'ai cette erreur:\n",
    "#\n",
    "# ValueError: The 'sampling_strategy' parameter of RandomOverSampler must be a\n",
    "# float in the range (0, 1], a str among {'majority', 'not majority', 'all',\n",
    "# 'auto', 'not minority'}, an instance of 'collections.abc.Mapping' or a\n",
    "# callable. Got 'minority' instead.\n",
    "oversampler_2 = SMOTE()\n",
    "#X_train_o, y_train_o = oversampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Declaring the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = DummyClassifier()\n",
    "clf1 = LogisticRegression(random_state=42, n_jobs=1, solver=\"sag\")\n",
    "clf2 = SGDClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = MultinomialNB()\n",
    "clf5 = KNeighborsClassifier()\n",
    "clf6 = SVC(probability=True, random_state=42)\n",
    "clf7 = DecisionTreeClassifier(random_state=42)\n",
    "clf8 = RandomForestClassifier(random_state=42)\n",
    "clf9 = GradientBoostingClassifier(random_state=42)\n",
    "clf10 = AdaBoostClassifier(random_state=42)\n",
    "clf11 = XGBClassifier(random_state=42)\n",
    "# Even with logging_level=\"info\" catboost prints millions of lines and crashes\n",
    "# my computer so i need logging_level=\"Silent\" here.\n",
    "clf12 = CatBoostClassifier(random_state=42, logging_level=\"Silent\")\n",
    "clf13 = LGBMClassifier(random_state=42, verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Declaring the classifiers' parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.0. DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param0 = {}\n",
    "param0[\"classifier__strategy\"] = [\"most_frequent\",\n",
    "                                  \"prior\"]\n",
    "param0[\"classifier\"] = [clf0]\n",
    "#param0[\"classifier\"] = [classifiers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = {}\n",
    "param1[\"classifier__C\"] = [\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param1[\"classifier__penalty\"] = [\n",
    "    # The default solver lbfgs supports only l2 penalty and None.\n",
    "    # \"l1\",\n",
    "    \"l2\",\n",
    "    # \"elasticnet\",\n",
    "]\n",
    "# param1[\"classifier__class_weight\"] = [{0: 1, 1: 1},\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# param1[\"classifier__class_weight\"] = [None]\n",
    "# param1[\"classifier__class_weight\"] = [None,\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# Je rajoute les class_weights et leurs variantes plus tard dans un if.\n",
    "param1[\"classifier\"] = [clf1]\n",
    "#param1[\"classifier\"] = [classifiers[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param2 = {}\n",
    "param2[\"classifier__loss\"] = [\n",
    "    \"hinge\",\n",
    "    \"log\",\n",
    "    \"squared_hinge\",\n",
    "    \"modified_huber\",\n",
    "]\n",
    "param2[\"classifier__penalty\"] = [\n",
    "    # \"l1\",\n",
    "    # \"l2\",\n",
    "    \"elasticnet\",\n",
    "]\n",
    "param2[\"classifier\"] = [clf2]\n",
    "#param2[\"classifier\"] = [classifiers[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3 = {}\n",
    "param3[\"classifier\"] = [clf3]\n",
    "#param3[\"classifier\"] = [classifiers[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param4 = {}\n",
    "param4[\"classifier__alpha\"] = [\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param4[\"classifier\"] = [clf4]\n",
    "#param4[\"classifier\"] = [classifiers[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param5 = {}\n",
    "param5[\"classifier__n_neighbors\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    10**2,\n",
    "    10**3,\n",
    "]\n",
    "param5[\"classifier__weights\"] = [\n",
    "    \"uniform\",\n",
    "    \"distance\",\n",
    "]\n",
    "param5[\"classifier\"] = [clf5]\n",
    "#param5[\"classifier\"] = [classifiers[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param6 = {}\n",
    "param6[\"classifier__kernel\"] = [\"linear\",\n",
    "                                \"rbf\",\n",
    "                                \"poly\",\n",
    "                                \"sigmoid\"]\n",
    "param6[\"classifier__C\"] = [10**-2,\n",
    "                           10**-1,\n",
    "                           10**0,\n",
    "                           10**1,\n",
    "                           10**2,\n",
    "                           10**3]\n",
    "param6[\"classifier__gamma\"] = [\"auto\",\n",
    "                               \"scale\"]\n",
    "param6[\"classifier\"] = [clf6]\n",
    "#param6[\"classifier\"] = [classifiers[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param7 = {}\n",
    "param7[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param7[\"classifier__min_samples_split\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "]\n",
    "param7[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param7[\"classifier\"] = [clf7]\n",
    "#param7[\"classifier\"] = [classifiers[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5.8. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "param8 = {}\n",
    "param8[\"classifier__n_estimators\"] = [\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param8[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param8[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param8[\"classifier\"] = [clf8]\n",
    "#param8[\"classifier\"] = [classifiers[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.9. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param9 = {}\n",
    "param9[\"classifier__n_estimators\"] = [10**1,\n",
    "                                      10**2,\n",
    "                                      10**3]\n",
    "param9[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30]\n",
    "param9[\"classifier\"] = [clf9]\n",
    "#param9[\"classifier\"] = [classifiers[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.10. AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param10 = {}\n",
    "param10[\"classifier__n_estimators\"] = [10**1,\n",
    "                                       10**2,\n",
    "                                       10**3]\n",
    "param10[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param10[\"classifier\"] = [clf10]\n",
    "#param10[\"classifier\"] = [classifiers[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.11. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param11 = {}\n",
    "param11[\"classifier__booster\"] = [\n",
    "    # \"gbtree\",\n",
    "    # gblinear doesn't support max_depth\n",
    "    # \"gblinear\",\n",
    "    \"dart\",\n",
    "]\n",
    "param11[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    # 10**-2,\n",
    "    10**-.5,\n",
    "    10**-1,\n",
    "]\n",
    "param11[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param11[\"classifier\"] = [clf11]\n",
    "#param11[\"classifier\"] = [classifiers[11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.12. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {}\n",
    "param12[\"classifier__iterations\"] = [\n",
    "    # 10**1,\n",
    "    # 10**2,\n",
    "    10**3,\n",
    "]\n",
    "param12[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "]\n",
    "param12[\"classifier__depth\"] = [\n",
    "    # 10**0,\n",
    "    # 3,\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param12[\"classifier\"] = [clf12]\n",
    "#param12[\"classifier\"] = [classifiers[12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.13. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param13 = {}\n",
    "param13[\"classifier__boosting_type\"] = [\n",
    "    # \"gbdt\",\n",
    "    \"dart\",\n",
    "    # \"goss\",\n",
    "]\n",
    "param13[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "param13[\"classifier__num_leaves\"] = [\n",
    "    10**1,\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param13[\"classifier\"] = [clf13]\n",
    "#param13[\"classifier\"] = [classifiers[13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5bis. Listing des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = [\n",
    "    \"Dummy\",\n",
    "    \"Logistic\",\n",
    "    \"SGDC\",\n",
    "    \"GaussNB\",\n",
    "    \"MultinNB\",\n",
    "    \"KNeighbor\",\n",
    "    \"SVC\",\n",
    "    \"DecisionT\",\n",
    "    \"RandomF\",\n",
    "    \"GB\",\n",
    "    \"AdaB\",\n",
    "    \"XGB\",\n",
    "    \"CatB\",\n",
    "    \"LGBM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    clf0,\n",
    "    # LogisticReg has trouble converging and is quite slow.\n",
    "    clf1,\n",
    "    clf2,\n",
    "    clf3,\n",
    "    clf4,\n",
    "    clf5,\n",
    "    # No SVC because it never finishes computing.\n",
    "    clf6,\n",
    "    clf7,\n",
    "    clf8,\n",
    "    clf9,\n",
    "    # AdaBoost et GBoost sont de loin les plus lents et ne sont pas les\n",
    "    # meilleurs (ils sont proches des scores des meilleurs néanmoins).\n",
    "    # Je les abandonne car je ne peux pas attendre de nouveau 10h à chaque fois\n",
    "    # que je veux faire un nouveau RandomSearchCV.\n",
    "    clf10,\n",
    "    clf11,\n",
    "    clf12,\n",
    "    clf13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full = [\n",
    "    param0,\n",
    "    param1,\n",
    "    param2,\n",
    "    param3,\n",
    "    param4,\n",
    "    param5,\n",
    "    param6,\n",
    "    param7,\n",
    "    param8,\n",
    "    param9,\n",
    "    param10,\n",
    "    param11,\n",
    "    param12,\n",
    "    param13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = pd.DataFrame(columns=[\"nz\", \"clf\", \"clfs\", \"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA.nz = nz\n",
    "dA.clf = clf\n",
    "dA.clfs = clfs\n",
    "dA.params = params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les raisons citées précédemment:\n",
    "dA.drop(index=[6, 9, 10], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nz</th>\n",
       "      <th>clf</th>\n",
       "      <th>clfs</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dummy</td>\n",
       "      <td>DummyClassifier()</td>\n",
       "      <td>{'classifier__strategy': ['most_frequent', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>LogisticRegression(n_jobs=1, random_state=42, ...</td>\n",
       "      <td>{'classifier__C': [0.01, 0.1, 1, 10], 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SGDC</td>\n",
       "      <td>SGDClassifier()</td>\n",
       "      <td>{'classifier__loss': ['hinge', 'log', 'squared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GaussNB</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{'classifier': [GaussianNB()]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MultinNB</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>{'classifier__alpha': [1, 10], 'classifier': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>KNeighbor</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>{'classifier__n_neighbors': [3, 10, 100, 1000]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DecisionT</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__max_depth': [3, 10, None], 'clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>RandomF</td>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__n_estimators': [31, 100], 'class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>XGB</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'classifier__booster': ['dart'], 'classifier_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>CatB</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'classifier__iterations': [1000], 'classifier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>LGBMClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__boosting_type': ['dart'], 'class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nz        clf                                               clfs  \\\n",
       "0    0      Dummy                                  DummyClassifier()   \n",
       "1    1   Logistic  LogisticRegression(n_jobs=1, random_state=42, ...   \n",
       "2    2       SGDC                                    SGDClassifier()   \n",
       "3    3    GaussNB                                       GaussianNB()   \n",
       "4    4   MultinNB                                    MultinomialNB()   \n",
       "5    5  KNeighbor                             KNeighborsClassifier()   \n",
       "7    7  DecisionT            DecisionTreeClassifier(random_state=42)   \n",
       "8    8    RandomF            RandomForestClassifier(random_state=42)   \n",
       "11  11        XGB  XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  12       CatB  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "13  13       LGBM                    LGBMClassifier(random_state=42)   \n",
       "\n",
       "                                               params  \n",
       "0   {'classifier__strategy': ['most_frequent', 'pr...  \n",
       "1   {'classifier__C': [0.01, 0.1, 1, 10], 'classif...  \n",
       "2   {'classifier__loss': ['hinge', 'log', 'squared...  \n",
       "3                      {'classifier': [GaussianNB()]}  \n",
       "4   {'classifier__alpha': [1, 10], 'classifier': [...  \n",
       "5   {'classifier__n_neighbors': [3, 10, 100, 1000]...  \n",
       "7   {'classifier__max_depth': [3, 10, None], 'clas...  \n",
       "8   {'classifier__n_estimators': [31, 100], 'class...  \n",
       "11  {'classifier__booster': ['dart'], 'classifier_...  \n",
       "12  {'classifier__iterations': [1000], 'classifier...  \n",
       "13  {'classifier__boosting_type': ['dart'], 'class...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    DummyClassifier                  2         \n",
      "1    LogisticRegression               4         \n",
      "2    SGDClassifier                    4         \n",
      "3    GaussianNB                       1         \n",
      "4    MultinomialNB                    2         \n",
      "5    KNeighborsClassifier             8         \n",
      "6    DecisionTreeClassifier           12        \n",
      "7    RandomForestClassifier           12        \n",
      "8    XGBClassifier                    4         \n",
      "9    <catboost.core.CatBoostClassifier object at 0x00000175D93E9DC0>] 4         \n",
      "10   LGBMClassifier                   9         \n"
     ]
    }
   ],
   "source": [
    "nl = []\n",
    "for j, k in enumerate(dA.params):\n",
    "    nl.append(math.prod([len(i) for i in k.values()]))\n",
    "    print(\"{:<4}\".format(j),\n",
    "          \"{:<32}\".format(str(k[\"classifier\"]).split('(')[0][1:]),\n",
    "          \"{:<10.0f}\".format(nl[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5ter. Déclaration des samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampler = {}\n",
    "param_sampler[\"sampler\"] = [\n",
    "    None,\n",
    "    # undersampler,\n",
    "    # oversampler_1,\n",
    "    oversampler_2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Itération sur les classifiers avec une boucle.\n",
    "Je vais gridsearch les paramètres de chacun des estimateurs et enregistrer le\n",
    "best_estimator_ obtenu à chaque fois pour pouvoir comparer les estimateurs.\n",
    "\n",
    "Le problème c'est qu'ici apparemment on veut l'inverse. On veut plutôt random\n",
    "entre tous les estimateurs pour trouver lequel est le meilleur et ne sortir que\n",
    "ses stats à lui. Puis recommencer avec une autre technique de sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# C'était juste pour constater qu'il y a pas mal de valeurs négatives\n",
    "# donc on ne peut pas utiliser MultinomialNB dans l'état\n",
    "# donc je vais rajouter un MinMaxScaler qui met tout entre 0 et 1.\n",
    "for co in df.columns:\n",
    "    neg = (df[co] < 0).sum()\n",
    "    if neg:\n",
    "        print(\"{:<40}\".format(co), \"{:>5.0f}\".format(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    for (n, clf, param) in list(zip(nz, clfs, params_full))[0:]:\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            print(clf, n_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(d):\n",
    "    return d.loc[:, ~d.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(input_features=feature_names)\n",
    "cws = [\n",
    "    None,\n",
    "    # {0: .8*oz, 1: zo/.8},\n",
    "    {0: oz, 1: zo},\n",
    "    # {0: oz/.8, 1: .8*zo},\n",
    "]\n",
    "cws1 = [\n",
    "    1,\n",
    "    oz,\n",
    "    zo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08781952865764357, 11.386988922457201)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oz, zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7250222708935026\n",
      "Obtained with: LogisticRegression(C=0.1, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 97s\n",
      "Score: 0.7099938152168361\n",
      "Obtained with: SGDClassifier(loss='log', penalty='elasticnet')\n",
      "2 - done in 60s\n",
      "Score: 0.4930100602023389\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 1s\n",
      "Score: 0.6243430795680865\n",
      "Obtained with: MultinomialNB(alpha=1)\n",
      "4 - done in 1s\n",
      "Score: 0.6522858164197481\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
      "5 - done in 10s\n",
      "Score: 0.6871594822939304\n",
      "Obtained with: DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=3,\n",
      "                       random_state=42)\n",
      "7 - done in 47s\n",
      "Score: 0.7107564160032682\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=10, random_state=42)\n",
      "8 - done in 97s\n",
      "Score: 0.7420321268277735\n",
      "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 934s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 5128, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2339, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2266, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6080, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6099, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/json_helper.h:173: Can't parse parameter \"class_weights\" with value: null\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.71175619 0.67402221\n",
      " 0.71084153 0.69628743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7117561947128614\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000175D93E9DC0>\n",
      "12 - done in 1463s\n",
      "Score: 0.745211332338472\n",
      "Obtained with: LGBMClassifier(boosting_type='dart', num_leaves=10, random_state=42)\n",
      "13 - done in 71s\n",
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.713460698256345\n",
      "Obtained with: LogisticRegression(C=0.01, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 157s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6917674294565902\n",
      "Obtained with: SGDClassifier(class_weight={0: 0.08781952865764357, 1: 11.386988922457201},\n",
      "              loss='log', penalty='elasticnet')\n",
      "2 - done in 189s\n",
      "Score: 0.4993332917231714\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 2s\n",
      "Score: 0.6189208971907467\n",
      "Obtained with: MultinomialNB(alpha=10)\n",
      "4 - done in 2s\n",
      "Score: 0.6488529780582051\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
      "5 - done in 19s\n",
      "Score: 0.6381124495713207\n",
      "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
      "7 - done in 226s\n",
      "Score: 0.6855991012205017\n",
      "Obtained with: RandomForestClassifier(random_state=42)\n",
      "8 - done in 348s\n",
      "Score: 0.7173236343828551\n",
      "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 1717s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 5128, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2339, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2266, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6080, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6099, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/json_helper.h:173: Can't parse parameter \"class_weights\" with value: null\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.66966222 0.66499016\n",
      " 0.66001963 0.66197607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6696622200534501\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000175D93E9DC0>\n",
      "12 - done in 4823s\n",
      "Score: 0.7069649737004864\n",
      "Obtained with: LGBMClassifier(boosting_type='dart', random_state=42)\n",
      "13 - done in 194s\n"
     ]
    }
   ],
   "source": [
    "result_dfs = dict()\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    # I create a different results table for each sampler.\n",
    "    # str(sampler) includes parentheses and the arguments inside, so i strip\n",
    "    # these out in order to keep only the name of the sampler.\n",
    "    nom_du_df = \"result_df_\" + str(sampler).split('(')[0] + \".csv\"\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "        \"classifier\",\n",
    "        \"best_score\",\n",
    "        \"avg_score_folds\",\n",
    "        \"fold0_score\",\n",
    "        \"fold1_score\",\n",
    "        \"fold2_score\",\n",
    "        \"fold3_score\",\n",
    "        \"fold4_score\",\n",
    "        \"run_time (s)\",\n",
    "    ])\n",
    "    # I gridsearch each classifier with its own set of parameters.\n",
    "    for (n, clf, param) in list(zip(dA.nz, dA.clfs, dA.params))[0:]:\n",
    "        with timer(str(n)):\n",
    "            try:\n",
    "                result_df = pd.read_csv(nom_du_df)\n",
    "                result_df = clean_csv(result_df)\n",
    "            except ValueError:\n",
    "                print(\"An error occurred while reading the CSV file.\")\n",
    "            except FileNotFoundError:\n",
    "                print(\"The file could not be found.\")\n",
    "\n",
    "            t_clf = time.perf_counter()\n",
    "\n",
    "            # Without sampler i still try to optimize the gridsearch using class_weight.\n",
    "            if sampler is None:\n",
    "                # class_weight does not exist for dummy, NBayes, KN, GB, AdaB, XGB.\n",
    "                # CatBoost either.\n",
    "                if n in [1, 2, 6, 7, 8, 13]:\n",
    "                    param[\"classifier__class_weight\"] = cws\n",
    "                elif n in [12]:\n",
    "                    param[\"classifier__class_weights\"] = cws\n",
    "                elif n in [11]:\n",
    "                    param[\"classifier__scale_pos_weight\"] = cws1\n",
    "\n",
    "            # Train the model\n",
    "            # I need a MinMaxScaler in the pipe because (at least) MultinomialNB\n",
    "            # errors on negative values.\n",
    "            pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                                   (\"sampler\", sampler),\n",
    "                                                   (\"classifier\", clf)])\n",
    "            gs = GridSearchCV(\n",
    "                pipeline,\n",
    "                param,\n",
    "                cv=5,\n",
    "                n_jobs=None,\n",
    "                scoring=\"roc_auc\",\n",
    "                # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "                # refit=\"roc_auc\",\n",
    "                # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "                # random_state=42, # seulement pour RandomSearchCV ?\n",
    "            ).fit(X_train, y_train)\n",
    "\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "            if t_mean > 10:\n",
    "                t_mean = round(t_mean, 0)\n",
    "            elif t_mean > 1:\n",
    "                t_mean = round(t_mean, 2)\n",
    "            elif t_mean > .001:\n",
    "                t_mean = round(t_mean, 3)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Save the results\n",
    "            print(\"Score:\", gs.best_score_)\n",
    "            print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "            result_df.loc[n, :] = [\n",
    "                gs.best_params_[\"classifier\"],\n",
    "                round(gs.best_score_, 3),\n",
    "                round(np.mean([\n",
    "                    gs.cv_results_[\"split0_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split1_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split2_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split3_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split4_test_score\"].max(),\n",
    "                ]), 3),\n",
    "                round(gs.cv_results_[\"split0_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split1_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split2_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split3_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split4_test_score\"].max(), 3),\n",
    "                t_mean\n",
    "            ]\n",
    "\n",
    "            # Je save à chaque classifier pour pouvoir relancer la loop à partir\n",
    "            # du classifier qui bug en cas d'erreur.\n",
    "            #result_df.sort_index(by=\"best_score\", inplace=True)\n",
    "            result_df.sort_values(by=\"best_score\", inplace=True)\n",
    "            result_df.to_csv(nom_du_df)\n",
    "\n",
    "    result_dfs[str(sampler)] = result_df\n",
    "    #display(result_df[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.12846975, 1.93213859, 3.49252276, 1.20664921, 1.90970011,\n",
       "        3.44738073, 1.19674945, 1.9298975 , 3.48896518, 1.01345282,\n",
       "        1.63482342, 3.1525496 , 1.09063888, 1.71527624, 3.28194938,\n",
       "        1.06873031, 1.62983508, 3.28338637]),\n",
       " 'std_fit_time': array([0.16328593, 0.02055966, 0.06306082, 0.01070929, 0.07148658,\n",
       "        0.11888143, 0.05199477, 0.06873583, 0.08543938, 0.02194296,\n",
       "        0.03327168, 0.13876321, 0.05604037, 0.03026382, 0.19121445,\n",
       "        0.05841236, 0.01412593, 0.19140034]),\n",
       " 'mean_score_time': array([0.0305635 , 0.0313375 , 0.0289206 , 0.02932396, 0.03036981,\n",
       "        0.03311548, 0.03042021, 0.03111563, 0.03199601, 0.02480702,\n",
       "        0.03254519, 0.03337388, 0.02807384, 0.02978086, 0.03095374,\n",
       "        0.02895303, 0.03263464, 0.03402157]),\n",
       " 'std_score_time': array([0.00258001, 0.00388554, 0.00182127, 0.0054116 , 0.00370879,\n",
       "        0.00366714, 0.00742412, 0.00523109, 0.0009727 , 0.00519239,\n",
       "        0.00427954, 0.00293639, 0.00158762, 0.00664471, 0.00373306,\n",
       "        0.00939595, 0.00483121, 0.00494056]),\n",
       " 'param_classifier': masked_array(data=[LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "                    LGBMClassifier(boosting_type='dart', random_state=42)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__boosting_type': masked_array(data=['dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__class_weight': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "                    {0: 0.08781952865764357, 1: 11.386988922457201}],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.03162277660168379,\n",
       "                    0.03162277660168379, 0.03162277660168379, 0.1, 0.1,\n",
       "                    0.1, 0.01, 0.01, 0.01, 0.03162277660168379,\n",
       "                    0.03162277660168379, 0.03162277660168379, 0.1, 0.1,\n",
       "                    0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__num_leaves': masked_array(data=[10, 31, 100, 10, 31, 100, 10, 31, 100, 10, 31, 100, 10,\n",
       "                    31, 100, 10, 31, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 100},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 100},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': None,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 100},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.01,\n",
       "   'classifier__num_leaves': 100},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.03162277660168379,\n",
       "   'classifier__num_leaves': 100},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 10},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 31},\n",
       "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
       "   'classifier__boosting_type': 'dart',\n",
       "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__num_leaves': 100}],\n",
       " 'split0_test_score': array([0.64817095, 0.67550031, 0.64628714, 0.6726434 , 0.69122044,\n",
       "        0.68749539, 0.70733493, 0.71905764, 0.69418801, 0.60924937,\n",
       "        0.59878631, 0.59178729, 0.62355097, 0.63450485, 0.6273583 ,\n",
       "        0.65516429, 0.6262859 , 0.64346711]),\n",
       " 'split1_test_score': array([0.65688071, 0.65286344, 0.60819399, 0.65186196, 0.66716221,\n",
       "        0.63343244, 0.67547478, 0.69234392, 0.67710893, 0.64053643,\n",
       "        0.63311469, 0.64089106, 0.6497058 , 0.65401245, 0.62346586,\n",
       "        0.65688639, 0.65229603, 0.63346932]),\n",
       " 'split2_test_score': array([0.65185061, 0.6361702 , 0.62084726, 0.68161133, 0.64918945,\n",
       "        0.65259108, 0.68548392, 0.67246183, 0.65605797, 0.61199848,\n",
       "        0.60011972, 0.59785292, 0.62040184, 0.64187552, 0.60677262,\n",
       "        0.62763633, 0.60400933, 0.60970046]),\n",
       " 'split3_test_score': array([0.70101964, 0.70594193, 0.66230517, 0.7343664 , 0.7193839 ,\n",
       "        0.72008749, 0.73396921, 0.73971141, 0.72984981, 0.66129517,\n",
       "        0.63344379, 0.66828284, 0.66985457, 0.66448119, 0.65616577,\n",
       "        0.65703959, 0.62347154, 0.66181719]),\n",
       " 'split4_test_score': array([0.67793167, 0.65333156, 0.6414528 , 0.67950056, 0.68006514,\n",
       "        0.64267557, 0.67786926, 0.71125006, 0.6892799 , 0.62200761,\n",
       "        0.60928342, 0.60181912, 0.6276108 , 0.61687254, 0.63681138,\n",
       "        0.61947412, 0.65088318, 0.59407396]),\n",
       " 'mean_test_score': array([0.66717072, 0.66476149, 0.63581727, 0.68399673, 0.68140423,\n",
       "        0.6672564 , 0.69602642, 0.70696497, 0.68929692, 0.62901741,\n",
       "        0.61494959, 0.62012665, 0.6382248 , 0.64234931, 0.63011479,\n",
       "        0.64324015, 0.63138919, 0.62850561]),\n",
       " 'std_test_score': array([0.01981479, 0.02408659, 0.01913182, 0.02729304, 0.02358489,\n",
       "        0.03213749, 0.02204874, 0.02298333, 0.02417368, 0.0195146 ,\n",
       "        0.01539668, 0.02961278, 0.01885679, 0.01635038, 0.01624907,\n",
       "        0.01629192, 0.01819616, 0.02407205]),\n",
       " 'rank_test_score': array([ 7,  8, 12,  4,  5,  6,  2,  1,  3, 15, 18, 17, 11, 10, 14,  9, 13,\n",
       "        16])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# Ce calcul date de la version 7 mais si je le refais ici en mettant à jour les\n",
    "# valeurs la conclusion sera identique.\n",
    "t_check = pd.DataFrame(columns=[\"clf\", \"t_tot\", \"rt_x_nl\", \"run_time\", \"n_loops\"])\n",
    "t_check.clf = dA.clf\n",
    "t_check.t_tot = [2, 153, 160, 2, 2, 18, 222, 346, 1397, 4513, 127]\n",
    "#t_check.run_time = result_df[\"run_time (s)\"] # non car il est sorted par score.\n",
    "t_check.run_time = [1, 19, 20, 1.6, 1.1, 2.3, 9.2, 14, 116, 564, 7]\n",
    "t_check.n_loops = nl\n",
    "t_check.rt_x_nl = t_check.run_time*t_check.n_loops\n",
    "t_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ```t_tot``` environ identique à ```rt_x_nl``` donc les temps sont\n",
    "correctment évalués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_N = clean_csv(pd.read_csv(\"result_df_None.csv\"))\n",
    "#res_U = clean_csv(pd.read_csv(\"result_df_RandomUnderSampler.csv\")).sort_values(by=\"score\")\n",
    "#res_O = clean_csv(pd.read_csv(\"result_df_RandomOverSampler.csv\")).sort_values(by=\"score\")\n",
    "res_S = clean_csv(pd.read_csv(\"result_df_SMOTE.csv\"))\n",
    "#res = [res_N, res_U, res_O, res_S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>avg_score_folds</th>\n",
       "      <th>fold0_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB(alpha=1)</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.642</td>\n",
       "      <td>1.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGDClassifier(loss='log', penalty='elasticnet')</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.691</td>\n",
       "      <td>7.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.701</td>\n",
       "      <td>4.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, n_jobs=1, random_sta...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.718</td>\n",
       "      <td>12.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.739</td>\n",
       "      <td>78.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.735</td>\n",
       "      <td>3.920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "0                                        GaussianNB()       0.493   \n",
       "1           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "2                              MultinomialNB(alpha=1)       0.624   \n",
       "3   KNeighborsClassifier(n_neighbors=1000, weights...       0.652   \n",
       "4   DecisionTreeClassifier(criterion='entropy', ma...       0.687   \n",
       "5     SGDClassifier(loss='log', penalty='elasticnet')       0.710   \n",
       "6   RandomForestClassifier(criterion='entropy', ma...       0.711   \n",
       "7   <catboost.core.CatBoostClassifier object at 0x...       0.712   \n",
       "8   LogisticRegression(C=0.1, n_jobs=1, random_sta...       0.725   \n",
       "9   XGBClassifier(base_score=None, booster='dart',...       0.742   \n",
       "10  LGBMClassifier(boosting_type='dart', num_leave...       0.745   \n",
       "\n",
       "    avg_score_folds  fold0_score  fold1_score  fold2_score  fold3_score  \\\n",
       "0             0.493        0.493        0.496        0.492        0.503   \n",
       "1             0.500        0.500        0.500        0.500        0.500   \n",
       "2             0.624        0.624        0.619        0.620        0.662   \n",
       "3             0.653        0.674        0.631        0.638        0.681   \n",
       "4             0.688        0.691        0.683        0.690        0.705   \n",
       "5             0.710        0.715        0.691        0.704        0.749   \n",
       "6             0.714        0.731        0.704        0.699        0.735   \n",
       "7               NaN          NaN          NaN          NaN          NaN   \n",
       "8             0.725        0.739        0.702        0.706        0.760   \n",
       "9             0.745        0.747        0.754        0.728        0.758   \n",
       "10            0.746        0.739        0.755        0.744        0.758   \n",
       "\n",
       "    fold4_score  run_time (s)  \n",
       "0         0.481         0.723  \n",
       "1         0.500         0.302  \n",
       "2         0.596         0.401  \n",
       "3         0.642         1.300  \n",
       "4         0.669         1.950  \n",
       "5         0.691         7.530  \n",
       "6         0.701         4.030  \n",
       "7           NaN       183.000  \n",
       "8         0.718        12.000  \n",
       "9         0.739        78.000  \n",
       "10        0.735         3.920  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>avg_score_folds</th>\n",
       "      <th>fold0_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.492</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB(alpha=10)</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.601</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=3, min_sample...</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.666</td>\n",
       "      <td>9.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.640</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.683</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGDClassifier(class_weight={0: 0.0878195286576...</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.681</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', random_st...</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.711</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.01, n_jobs=1, random_st...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.708</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.732</td>\n",
       "      <td>143.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "0                                        GaussianNB()       0.499   \n",
       "1           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "2                             MultinomialNB(alpha=10)       0.619   \n",
       "3   DecisionTreeClassifier(max_depth=3, min_sample...       0.638   \n",
       "4   KNeighborsClassifier(n_neighbors=1000, weights...       0.649   \n",
       "5   <catboost.core.CatBoostClassifier object at 0x...       0.670   \n",
       "6             RandomForestClassifier(random_state=42)       0.686   \n",
       "7   SGDClassifier(class_weight={0: 0.0878195286576...       0.692   \n",
       "8   LGBMClassifier(boosting_type='dart', random_st...       0.707   \n",
       "9   LogisticRegression(C=0.01, n_jobs=1, random_st...       0.713   \n",
       "10  XGBClassifier(base_score=None, booster='dart',...       0.717   \n",
       "\n",
       "    avg_score_folds  fold0_score  fold1_score  fold2_score  fold3_score  \\\n",
       "0             0.499        0.498        0.503        0.498        0.505   \n",
       "1             0.500        0.500        0.500        0.500        0.500   \n",
       "2             0.619        0.630        0.599        0.610        0.656   \n",
       "3             0.645        0.656        0.626        0.621        0.658   \n",
       "4             0.649        0.671        0.614        0.627        0.695   \n",
       "5               NaN          NaN          NaN          NaN          NaN   \n",
       "6             0.692        0.686        0.687        0.677        0.724   \n",
       "7             0.695        0.698        0.676        0.683        0.739   \n",
       "8             0.710        0.719        0.692        0.685        0.740   \n",
       "9             0.715        0.728        0.691        0.690        0.757   \n",
       "10            0.729        0.726        0.739        0.705        0.742   \n",
       "\n",
       "    fold4_score  run_time (s)  \n",
       "0         0.492          1.67  \n",
       "1         0.500          1.07  \n",
       "2         0.601          1.18  \n",
       "3         0.666          9.41  \n",
       "4         0.640          2.42  \n",
       "5           NaN        603.00  \n",
       "6         0.683         15.00  \n",
       "7         0.681         24.00  \n",
       "8         0.711         11.00  \n",
       "9         0.708         20.00  \n",
       "10        0.732        143.00  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partons sur les résultats de SMOTE (parce que ça fait partie de l'exercice)\n",
    "même s'ils ne sont pas les meilleurs, ça me donnera une marge de progression\n",
    "pour le fine tuning, au moins.  \n",
    "Je vais utiliser LGBM car il compute un peu plus vite que la LogisticReg et\n",
    "beaucoup plus vite que XGB ou CatB, et que c'est l'algo choisi par les\n",
    "meilleures équipes de la compétition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fine tuning with the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "    \"rf\",\n",
    "    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    10**1,\n",
    "    10**1.5,\n",
    "    10**2,\n",
    "]\n",
    "parm[\"classifier__num_iterations\"] = [\n",
    "    10**3,\n",
    "    10**4,\n",
    "]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est trop à mon avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "#    \"rf\",\n",
    "#    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "#parm[\"classifier__num_leaves\"] = [\n",
    "#    10**1,\n",
    "#    10**1.5,\n",
    "#    10**2,\n",
    "#]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok testons ça."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier fine tuning a donné:\n",
    "```\n",
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "#    \"rf\",\n",
    "#    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "    ? pas vu dans l'output, peut-être parce que c'est la valeur par\n",
    "    défaut et donc best_estimator_ pense que ce n'est pas nécessaire\n",
    "    de préciser que le meilleur learning_rate trouvé est .1.\n",
    "#parm[\"classifier__num_leaves\"] = [\n",
    "#    10**1,\n",
    "#    10**1.5,\n",
    "#    10**2,\n",
    "#]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,     <------------------------------------------------\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,     <------------------------------------------------\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,     <------------------------------------------------\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,     <------------------------------------------------\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,     <------------------------------------------------\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]\n",
    "```\n",
    "\n",
    "Je teste maintenant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,\n",
    "    10**-1,\n",
    "    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,\n",
    "    30,\n",
    "    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je trouve:\n",
    "```\n",
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,     <-------------------------------------- sans SMOTE\n",
    "    10**-1,     <-------------------------------------- avec SMOTE (car\n",
    "                                                        rien d'indiqué)\n",
    "    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,     <-------------------------------------- avec SMOTE\n",
    "    30,     <-------------------------------------- sans SMOTE\n",
    "    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,     <-------------------------------------- sans SMOTE\n",
    "    8,     <-------------------------------------- avec SMOTE\n",
    "    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Sans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "Score: 0.735\n",
      "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9, lambda_l1=0.01,\n",
      "               lambda_l2=0.03162277660168379, learning_rate=0.07943282347242814,\n",
      "               max_depth=7, min_gain_to_split=0.02, min_sum_hessian_in_leaf=1,\n",
      "               num_leaves=30, random_state=42, verbose=-1)\n",
      "run_time per search (s) 2.98\n",
      "Fine tuning with class_weight and LGBM - done in 161s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Fine tuning with class_weight and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    parm[\"classifier__class_weight\"] = cws\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", None),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "# On peut aussi faire un predict_proba avec un seuil < .5 au lieu de faire un\n",
    "# oversampling ou de jouer sur class_weights.\n",
    "y_pred_proba = gs.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7677716727716728"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_positive)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Avec SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "Score: 0.721\n",
      "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9, lambda_l1=0.01,\n",
      "               lambda_l2=0.03162277660168379, max_depth=8,\n",
      "               min_gain_to_split=0.02, min_sum_hessian_in_leaf=1, num_leaves=25,\n",
      "               random_state=42, verbose=-1)\n",
      "run_time per search (s) 7.38\n",
      "Fine tuning with SMOTE and LGBM - done in 399s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = rs_cv.best_estimator_[best_algorithm].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "# On peut aussi faire un predict_proba avec un seuil < .5 au lieu de faire un\n",
    "# oversampling ou de jouer sur class_weights.\n",
    "y_pred_proba = gs.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7604853479853481"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_positive)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Fine tuning using an Fbeta scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion                 Predicted\n",
    "    Matrix               Positive    Negative\n",
    "              Positive      TP          FN\n",
    "    Actual    Negative      FP          TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>1812</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>169</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive  Negative\n",
       "Positive      1812         8\n",
       "Negative       169        11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5341346153846154"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = .1\n",
    "fb_score = fbeta_score(y_test, y_pred, beta=beta)\n",
    "fb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5784572510253445"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = .01\n",
    "fb_score = fbeta_score(y_test, y_pred, beta=beta)\n",
    "fb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Applying the fine-tuned best classifier for the final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Features' importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# InvalidModelError: Model type not yet supported by TreeExplainer:\n",
    "# <class 'imblearn.pipeline.Pipeline'>\n",
    "explainer = shap.TreeExplainer(gs.best_estimator_)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = imputer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'SK_ID_CURR', 'CODE_GENDER', 'FLAG_OWN_CAR',\n",
       "       'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
       "       'AMT_CREDIT', 'AMT_ANNUITY'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.get_feature_names_out(input_features=feature_names)[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 748), (10, 748))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'SK_ID_CURR', 'CODE_GENDER'], dtype='object')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'SK_ID_CURR', 'CODE_GENDER'], dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample.columns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2db0af1d94f49fb98d874b4a95a7cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MemoryError: Unable to allocate 148. GiB for an array with shape\n",
    "# (3312, 5984000) and data type float64\n",
    "K = 10\n",
    "X_train_sample = shap.sample(X_train, K)\n",
    "print(X_train_sample.shape)\n",
    "explainer = shap.KernelExplainer(gs.best_estimator_.predict_proba, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Precision:\",\n",
    "      precision_score(rs.predict(X_test), y_test))\n",
    "      # precision_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"Recall:\",\n",
    "      recall_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"ROC AUC Score:\",\n",
    "      roc_auc_score(rs.best_estimator_.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot la courbe roc du meilleur pour chaque sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap (et lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "#rs_cv.fit(X_train, y_train)\n",
    "#rs_cv.fit(X_train_u, y_train_u)\n",
    "rs_cv.fit(X_train_o, y_train_o)\n",
    "\n",
    "for algorithm in classifiers.keys():\n",
    "    print(f\"Best parameters for {algorithm}: {rs_cv.best_params_[algorithm]}\")\n",
    "    print(f\"Best AUC score for {algorithm}: {rs_cv.best_score_[algorithm]['roc_auc']:.3f}\")\n",
    "    print(f\"Best accuracy score for {algorithm}: {rs_cv.best_score_[algorithm]['accuracy']:.3f}\")\n",
    "\n",
    "#best_algorithm = rs_cv.best_estimator_.keys()[0]\n",
    "best_algorithm = rs_cv.best_estimator_.named_steps.keys()\n",
    "print(f\"Overall best algorithm: {best_algorithm}\")\n",
    "print(f\"Best AUC score: {rs_cv.best_score_[best_algorithm]['roc_auc']:.3f}\")\n",
    "print(f\"Best accuracy score: {rs_cv.best_score_[best_algorithm]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
