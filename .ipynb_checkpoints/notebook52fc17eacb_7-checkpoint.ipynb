{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle/python Docker image: https://github.com/kaggle/docker-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (most) package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import imblearn.pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.utils.testing import ignore_warnings # For LogisticRegression\n",
    "#from sklearn.exceptions import ConvergenceWarning # For LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.perf_counter() - t0))\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offline j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"home-credit-default-risk/application_test.csv\",\n",
    "    \"home-credit-default-risk/application_train.csv\",\n",
    "    \"home-credit-default-risk/bureau.csv\",\n",
    "    \"home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"home-credit-default-risk/installments_payments.csv\",\n",
    "    \"home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"home-credit-default-risk/previous_application.csv\",\n",
    "    \"home-credit-default-risk/sample_submission.csv\"\n",
    "    ]\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "not offline (on Kaggle) j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"/kaggle/input/home-credit-default-risk/sample_submission.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_train.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_test.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/previous_application.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/installments_payments.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau.csv\"]\n",
    "    ]\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    le_path = \"home-credit-default-risk/\"\n",
    "else:\n",
    "    le_path = \"/kaggle/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for dirname, _, filenamess in os.walk(le_path):\n",
    "    for filenamee in filenamess:\n",
    "#                        HomeCredit_columns_description.csv est illisible.\n",
    "        if filenamee != \"HomeCredit_columns_description.csv\":\n",
    "            filename = os.path.join(dirname, filenamee)\n",
    "#            print(filename)\n",
    "            filenames.append(filename)\n",
    "#            df = pd.read_csv(filename)\n",
    "#            display(df[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not offline:\n",
    "    flnms = []\n",
    "    flnms.append(filenames[4])\n",
    "    flnms.append(filenames[3])\n",
    "    flnms.append(filenames[8])\n",
    "    flnms.append(filenames[1])\n",
    "    flnms.append(filenames[6])\n",
    "    flnms.append(filenames[7])\n",
    "    flnms.append(filenames[2])\n",
    "    flnms.append(filenames[5])\n",
    "    flnms.append(filenames[0])\n",
    "    filenames = flnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. application_train and application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(filenames[1], nrows=num_rows)\n",
    "    test_df = pd.read_csv(filenames[0], nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Aberrant values\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(filenames[2], nrows=num_rows)\n",
    "    bb = pd.read_csv(filenames[3], nrows=num_rows)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()]\n",
    "    )\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg(\n",
    "        {**num_aggregations, **cat_aggregations}\n",
    "    )\n",
    "    bureau_agg.columns = pd.Index([\n",
    "        \"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\n",
    "        \"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\n",
    "        \"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(filenames[7], nrows=num_rows)\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Aberrant values\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\n",
    "        \"PREV_\" + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\n",
    "        \"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\n",
    "        \"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. pos_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(filenames[6], nrows=num_rows)\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\n",
    "        \"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. installment_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(filenames[5], nrows=num_rows)\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    # Percentage and difference paid in each installment (amount paid and\n",
    "    # installment value)\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\n",
    "        \"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(filenames[4], nrows=num_rows)\n",
    "\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # General aggregations\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace =True)\n",
    "\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\n",
    "        \"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. functions from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "        train_df.shape, test_df.shape\n",
    "    ))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds,\n",
    "                      shuffle=True,\n",
    "                      random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\n",
    "                                                      \"SK_ID_CURR\",\n",
    "                                                      \"SK_ID_BUREAU\",\n",
    "                                                      \"SK_ID_PREV\",\n",
    "                                                      \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "            folds.split(train_df[feats], train_df[\"TARGET\"])\n",
    "    ):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat(\n",
    "            [feature_importance_df, fold_importance_df],\n",
    "            axis=0\n",
    "        )\n",
    "        print(\"Fold %2d AUC : %.6f\" % (\n",
    "            n_fold + 1,\n",
    "            roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        ))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Write submission file and plot feature importance\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(submission_file_name,\n",
    "                                                 index=False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n",
    "        \"feature\"\n",
    "    ).mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[\n",
    "        feature_importance_df_.feature.isin(cols)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"feature\",\n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title(\"LightGBM Features (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"lgbm_importances01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(debug=True):\n",
    "    num_rows = debug if debug else None\n",
    "    with timer(\"Process application train test\"):\n",
    "        df = application_train_test(num_rows)\n",
    "        print(\"Application train test df shape:\", df.shape)\n",
    "        # print(df.dtypes.value_counts())\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del cc\n",
    "        gc.collect()\n",
    "\n",
    "    zeros = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[0]\n",
    "    ones = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[1]\n",
    "    nans = df.TARGET.isna().sum()\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    if debug:\n",
    "        print(\"subsampled df's TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "    else:\n",
    "        print(\"TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "\n",
    "    return zeros, ones, nans, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. A first full run just to measure the target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# I ran this cell only once, just to get the exact values of zo and oz.\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_full\"):\n",
    "        zeros_full, ones_full, nans_full, df_full = preproc(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_full, ones_full, nans_full = 282682, 24825, 48744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo = 11.39 more zeros than ones in TARGET. (and oz = 0.09)\n"
     ]
    }
   ],
   "source": [
    "zo = zeros_full/ones_full\n",
    "oz = ones_full/zeros_full\n",
    "print(\"There is zo =\",\n",
    "      f\"{zo:.2f} more zeros than ones in TARGET. (and oz =\",\n",
    "      f\"{oz:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Subsampled run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000, test samples: 10000\n",
      "Application train test df shape: (20000, 246)\n",
      "Process application train test - done in 1s\n",
      "Bureau df shape: (2011, 108)\n",
      "Process bureau and bureau_balance - done in 0s\n",
      "Previous applications df shape: (9734, 242)\n",
      "Process previous_applications - done in 0s\n",
      "Pos-cash balance df shape: (9494, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (8893, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (9520, 131)\n",
      "Process credit card balance - done in 0s\n",
      "-----------------------------------------------------------------------\n",
      "subsampled df's TARGET has       9225 zeros,        775 ones and      10000 NaNs\n",
      "preproc_subsampled - done in 2s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_subsampled\"):\n",
    "        # I tried 1_000 here but the ROC AUC scores obtained were > 90% which\n",
    "        # proves overfitting.\n",
    "        zeros, ones, nans, df = preproc(debug=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Measure of the target imbalance after the subsampling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo_sub = 11.90 more zeros than ones in TARGET. (and oz_sub = 0.08)\n"
     ]
    }
   ],
   "source": [
    "zo_sub = zeros/ones\n",
    "oz_sub = ones/zeros\n",
    "print(\"There is zo_sub =\",\n",
    "      f\"{zo_sub:.2f} more zeros than ones in TARGET. (and oz_sub =\",\n",
    "      f\"{oz_sub:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'imbalance de 11.39 des targets du dataset est de 11.90 après subsampling.\n"
     ]
    }
   ],
   "source": [
    "if (zo/zo_sub >= 3/2 or zo/zo_sub <= 2/3):\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "    print(\n",
    "        \"L'imbalance des targets a été fortement modifiée par le subsampling.\",\n",
    "        f\"Elle est passée de {zo:.2f} à {zo_sub:.2f}.\",\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "else:\n",
    "    print(\n",
    "        f\"L'imbalance de {zo:.2f} des targets du dataset est de {zo_sub:.2f}\",\n",
    "        \"après subsampling.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Suppression du caractère illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_df = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "# Ce code prend un temps infini à run. Prende la cell en-dessous.\n",
    "for j in cols_of_df:\n",
    "    df = df.rename(columns={j: re.sub(r\"[ ]\", r\"_a_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[-]\", r\"_b_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_c_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[/]\", r\"_d_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[,]\", r\"_e_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_f_\", j)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "df = df.rename(columns=lambda x: x.replace(\" \", \"_a_\")\\\n",
    "                                  .replace(\"-\", \"_b_\")\\\n",
    "                                  .replace(\":\", \"_c_\")\\\n",
    "                                  .replace(\"/\", \"_d_\")\\\n",
    "                                  .replace(\",\", \"_e_\")\\\n",
    "                                  .replace(\":\", \"_f_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(\":\", \"deuxpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification run from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "with timer(\"Run LightGBM with kfold\"):\n",
    "    feat_importance = kfold_lightgbm(df, num_folds=10, stratified=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. Suppression des données sans TARGET et colonnes sans données\n",
    "En effet mon but ici c'est juste de créer un modèle qui fonctionne, faire les\n",
    " prédictions de solvabilité des futurs clients ça sera pour plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"TARGET\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_notnull = list(df.loc[:, df.notnull().sum() > 0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, cols_notnull]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"TARGET\", axis=\"columns\")\n",
    "y = df[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7405\n",
       "1.0     595\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_old = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names_old = list(X_train.columns)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "#X_train_i = imputer.fit_transform(X_train)\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "feature_names = imputer.get_feature_names_out()\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_old = X_test.copy()\n",
    "# X_test = imputer.fit_transform(X_test_old)\n",
    "X_test = imputer.transform(X_test_old) # pas de fit, recommandé par mentor\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 748)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Balancing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy=\"majority\")\n",
    "#X_train_u, y_train_u = undersampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler_1 = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "# La doc de imblearn dit exactement l'inverse mais ici j'ai cette erreur:\n",
    "#\n",
    "# ValueError: The 'sampling_strategy' parameter of RandomOverSampler must be a\n",
    "# float in the range (0, 1], a str among {'majority', 'not majority', 'all',\n",
    "# 'auto', 'not minority'}, an instance of 'collections.abc.Mapping' or a\n",
    "# callable. Got 'minority' instead.\n",
    "oversampler_2 = SMOTE()\n",
    "#X_train_o, y_train_o = oversampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Declaring the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = DummyClassifier()\n",
    "clf1 = LogisticRegression(random_state=42, n_jobs=1, solver=\"sag\")\n",
    "clf2 = SGDClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = MultinomialNB()\n",
    "clf5 = KNeighborsClassifier()\n",
    "clf6 = SVC(probability=True, random_state=42)\n",
    "clf7 = DecisionTreeClassifier(random_state=42)\n",
    "clf8 = RandomForestClassifier(random_state=42)\n",
    "clf9 = GradientBoostingClassifier(random_state=42)\n",
    "clf10 = AdaBoostClassifier(random_state=42)\n",
    "clf11 = XGBClassifier(random_state=42)\n",
    "# Even with logging_level=\"info\" catboost prints millions of lines and crashes\n",
    "# my computer so i need logging_level=\"Silent\" here.\n",
    "clf12 = CatBoostClassifier(random_state=42, logging_level=\"Silent\")\n",
    "clf13 = LGBMClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Declaring the classifiers' parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.0. DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param0 = {}\n",
    "param0[\"classifier__strategy\"] = [\"most_frequent\",\n",
    "                                  \"prior\"]\n",
    "param0[\"classifier\"] = [clf0]\n",
    "#param0[\"classifier\"] = [classifiers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = {}\n",
    "param1[\"classifier__C\"] = [\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param1[\"classifier__penalty\"] = [\n",
    "    # The default solver lbfgs supports only l2 penalty and None.\n",
    "    # \"l1\",\n",
    "    \"l2\",\n",
    "    # \"elasticnet\",\n",
    "]\n",
    "# param1[\"classifier__class_weight\"] = [{0: 1, 1: 1},\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# param1[\"classifier__class_weight\"] = [None]\n",
    "# param1[\"classifier__class_weight\"] = [None,\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# Je rajoute les class_weights et leurs variantes plus tard dans un if.\n",
    "param1[\"classifier\"] = [clf1]\n",
    "#param1[\"classifier\"] = [classifiers[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param2 = {}\n",
    "param2[\"classifier__loss\"] = [\n",
    "    \"hinge\",\n",
    "    \"log\",\n",
    "    \"squared_hinge\",\n",
    "    \"modified_huber\",\n",
    "]\n",
    "param2[\"classifier__penalty\"] = [\n",
    "    # \"l1\",\n",
    "    # \"l2\",\n",
    "    \"elasticnet\",\n",
    "]\n",
    "param2[\"classifier\"] = [clf2]\n",
    "#param2[\"classifier\"] = [classifiers[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3 = {}\n",
    "param3[\"classifier\"] = [clf3]\n",
    "#param3[\"classifier\"] = [classifiers[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param4 = {}\n",
    "param4[\"classifier__alpha\"] = [\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param4[\"classifier\"] = [clf4]\n",
    "#param4[\"classifier\"] = [classifiers[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param5 = {}\n",
    "param5[\"classifier__n_neighbors\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    10**2,\n",
    "    10**3,\n",
    "]\n",
    "param5[\"classifier__weights\"] = [\n",
    "    \"uniform\",\n",
    "    \"distance\",\n",
    "]\n",
    "param5[\"classifier\"] = [clf5]\n",
    "#param5[\"classifier\"] = [classifiers[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param6 = {}\n",
    "param6[\"classifier__kernel\"] = [\"linear\",\n",
    "                                \"rbf\",\n",
    "                                \"poly\",\n",
    "                                \"sigmoid\"]\n",
    "param6[\"classifier__C\"] = [10**-2,\n",
    "                           10**-1,\n",
    "                           10**0,\n",
    "                           10**1,\n",
    "                           10**2,\n",
    "                           10**3]\n",
    "param6[\"classifier__gamma\"] = [\"auto\",\n",
    "                               \"scale\"]\n",
    "param6[\"classifier\"] = [clf6]\n",
    "#param6[\"classifier\"] = [classifiers[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param7 = {}\n",
    "param7[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param7[\"classifier__min_samples_split\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "]\n",
    "param7[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param7[\"classifier\"] = [clf7]\n",
    "#param7[\"classifier\"] = [classifiers[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5.8. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "param8 = {}\n",
    "param8[\"classifier__n_estimators\"] = [\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param8[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param8[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param8[\"classifier\"] = [clf8]\n",
    "#param8[\"classifier\"] = [classifiers[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.9. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param9 = {}\n",
    "param9[\"classifier__n_estimators\"] = [10**1,\n",
    "                                      10**2,\n",
    "                                      10**3]\n",
    "param9[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30]\n",
    "param9[\"classifier\"] = [clf9]\n",
    "#param9[\"classifier\"] = [classifiers[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.10. AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param10 = {}\n",
    "param10[\"classifier__n_estimators\"] = [10**1,\n",
    "                                       10**2,\n",
    "                                       10**3]\n",
    "param10[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param10[\"classifier\"] = [clf10]\n",
    "#param10[\"classifier\"] = [classifiers[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.11. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param11 = {}\n",
    "param11[\"classifier__booster\"] = [\n",
    "    # \"gbtree\",\n",
    "    # gblinear doesn't support max_depth\n",
    "    # \"gblinear\",\n",
    "    \"dart\",\n",
    "]\n",
    "param11[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    # 10**-2,\n",
    "    10**-.5,\n",
    "    10**-1,\n",
    "]\n",
    "param11[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param11[\"classifier\"] = [clf11]\n",
    "#param11[\"classifier\"] = [classifiers[11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.12. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {}\n",
    "param12[\"classifier__iterations\"] = [\n",
    "    # 10**1,\n",
    "    # 10**2,\n",
    "    10**3,\n",
    "]\n",
    "param12[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "]\n",
    "param12[\"classifier__depth\"] = [\n",
    "    # 10**0,\n",
    "    # 3,\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param12[\"classifier\"] = [clf12]\n",
    "#param12[\"classifier\"] = [classifiers[12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.13. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param13 = {}\n",
    "param13[\"classifier__boosting_type\"] = [\n",
    "    # \"gbdt\",\n",
    "    \"dart\",\n",
    "    # \"goss\",\n",
    "]\n",
    "param13[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "param13[\"classifier__num_leaves\"] = [\n",
    "    10**1,\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param13[\"classifier\"] = [clf13]\n",
    "#param13[\"classifier\"] = [classifiers[13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5bis. Listing des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = [\n",
    "    \"Dummy\",\n",
    "    \"Logistic\",\n",
    "    \"SGDC\",\n",
    "    \"GaussNB\",\n",
    "    \"MultinNB\",\n",
    "    \"KNeighbor\",\n",
    "    \"SVC\",\n",
    "    \"DecisionT\",\n",
    "    \"RandomF\",\n",
    "    \"GB\",\n",
    "    \"AdaB\",\n",
    "    \"XGB\",\n",
    "    \"CatB\",\n",
    "    \"LGBM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    clf0,\n",
    "    # LogisticReg has trouble converging and is quite slow.\n",
    "    clf1,\n",
    "    clf2,\n",
    "    clf3,\n",
    "    clf4,\n",
    "    clf5,\n",
    "    # No SVC because it never finishes computing.\n",
    "    clf6,\n",
    "    clf7,\n",
    "    clf8,\n",
    "    clf9,\n",
    "    # AdaBoost et GBoost sont de loin les plus lents et ne sont pas les\n",
    "    # meilleurs (ils sont proches des scores des meilleurs néanmoins).\n",
    "    # Je les abandonne car je ne peux pas attendre de nouveau 10h à chaque fois\n",
    "    # que je veux faire un nouveau RandomSearchCV.\n",
    "    clf10,\n",
    "    clf11,\n",
    "    clf12,\n",
    "    clf13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full = [\n",
    "    param0,\n",
    "    param1,\n",
    "    param2,\n",
    "    param3,\n",
    "    param4,\n",
    "    param5,\n",
    "    param6,\n",
    "    param7,\n",
    "    param8,\n",
    "    param9,\n",
    "    param10,\n",
    "    param11,\n",
    "    param12,\n",
    "    param13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = pd.DataFrame(columns=[\"nz\", \"clf\", \"clfs\", \"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA.nz = nz\n",
    "dA.clf = clf\n",
    "dA.clfs = clfs\n",
    "dA.params = params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les raisons citées précédemment:\n",
    "dA.drop(index=[6, 9, 10], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nz</th>\n",
       "      <th>clf</th>\n",
       "      <th>clfs</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dummy</td>\n",
       "      <td>DummyClassifier()</td>\n",
       "      <td>{'classifier__strategy': ['most_frequent', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>LogisticRegression(n_jobs=1, random_state=42, ...</td>\n",
       "      <td>{'classifier__C': [0.01, 0.1, 1, 10], 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SGDC</td>\n",
       "      <td>SGDClassifier()</td>\n",
       "      <td>{'classifier__loss': ['hinge', 'log', 'squared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GaussNB</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{'classifier': [GaussianNB()]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MultinNB</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>{'classifier__alpha': [1, 10], 'classifier': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>KNeighbor</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>{'classifier__n_neighbors': [3, 10, 100, 1000]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DecisionT</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__max_depth': [3, 10, None], 'clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>RandomF</td>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__n_estimators': [31, 100], 'class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>XGB</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'classifier__booster': ['dart'], 'classifier_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>CatB</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'classifier__iterations': [1000], 'classifier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>LGBMClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__boosting_type': ['dart'], 'class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nz        clf                                               clfs  \\\n",
       "0    0      Dummy                                  DummyClassifier()   \n",
       "1    1   Logistic  LogisticRegression(n_jobs=1, random_state=42, ...   \n",
       "2    2       SGDC                                    SGDClassifier()   \n",
       "3    3    GaussNB                                       GaussianNB()   \n",
       "4    4   MultinNB                                    MultinomialNB()   \n",
       "5    5  KNeighbor                             KNeighborsClassifier()   \n",
       "7    7  DecisionT            DecisionTreeClassifier(random_state=42)   \n",
       "8    8    RandomF            RandomForestClassifier(random_state=42)   \n",
       "11  11        XGB  XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  12       CatB  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "13  13       LGBM                    LGBMClassifier(random_state=42)   \n",
       "\n",
       "                                               params  \n",
       "0   {'classifier__strategy': ['most_frequent', 'pr...  \n",
       "1   {'classifier__C': [0.01, 0.1, 1, 10], 'classif...  \n",
       "2   {'classifier__loss': ['hinge', 'log', 'squared...  \n",
       "3                      {'classifier': [GaussianNB()]}  \n",
       "4   {'classifier__alpha': [1, 10], 'classifier': [...  \n",
       "5   {'classifier__n_neighbors': [3, 10, 100, 1000]...  \n",
       "7   {'classifier__max_depth': [3, 10, None], 'clas...  \n",
       "8   {'classifier__n_estimators': [31, 100], 'class...  \n",
       "11  {'classifier__booster': ['dart'], 'classifier_...  \n",
       "12  {'classifier__iterations': [1000], 'classifier...  \n",
       "13  {'classifier__boosting_type': ['dart'], 'class...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    DummyClassifier                  2         \n",
      "1    LogisticRegression               8         \n",
      "2    SGDClassifier                    8         \n",
      "3    GaussianNB                       1         \n",
      "4    MultinomialNB                    2         \n",
      "5    KNeighborsClassifier             8         \n",
      "6    DecisionTreeClassifier           24        \n",
      "7    RandomForestClassifier           24        \n",
      "8    XGBClassifier                    12        \n",
      "9    <catboost.core.CatBoostClassifier object at 0x00000200000EFD90>] 8         \n",
      "10   LGBMClassifier                   18        \n"
     ]
    }
   ],
   "source": [
    "nl = []\n",
    "for j, k in enumerate(dA.params):\n",
    "    nl.append(math.prod([len(i) for i in k.values()]))\n",
    "    print(\"{:<4}\".format(j),\n",
    "          \"{:<32}\".format(str(k[\"classifier\"]).split('(')[0][1:]),\n",
    "          \"{:<10.0f}\".format(nl[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5ter. Déclaration des samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampler = {}\n",
    "param_sampler[\"sampler\"] = [\n",
    "    None,\n",
    "    # undersampler,\n",
    "    # oversampler_1,\n",
    "    oversampler_2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Itération sur les classifiers avec une boucle.\n",
    "Je vais gridsearch les paramètres de chacun des estimateurs et enregistrer le\n",
    "best_estimator_ obtenu à chaque fois pour pouvoir comparer les estimateurs.\n",
    "\n",
    "Le problème c'est qu'ici apparemment on veut l'inverse. On veut plutôt random\n",
    "entre tous les estimateurs pour trouver lequel est le meilleur et ne sortir que\n",
    "ses stats à lui. Puis recommencer avec une autre technique de sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# C'était juste pour constater qu'il y a pas mal de valeurs négatives\n",
    "# donc on ne peut pas utiliser MultinomialNB dans l'état\n",
    "# donc je vais rajouter un MinMaxScaler qui met tout entre 0 et 1.\n",
    "for co in df.columns:\n",
    "    neg = (df[co] < 0).sum()\n",
    "    if neg:\n",
    "        print(\"{:<40}\".format(co), \"{:>5.0f}\".format(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    for (n, clf, param) in list(zip(nz, clfs, params_full))[0:]:\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            print(clf, n_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(d):\n",
    "    return d.loc[:, ~d.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cws = [\n",
    "    None,\n",
    "    # {0: .8*oz, 1: zo/.8},\n",
    "    {0: oz, 1: zo},\n",
    "    # {0: oz/.8, 1: .8*zo},\n",
    "]\n",
    "cws1 = [\n",
    "    1,\n",
    "    oz,\n",
    "    zo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08781952865764357, 11.386988922457201)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oz, zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7250222708935026\n",
      "Obtained with: LogisticRegression(C=0.1, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 94s\n",
      "Score: 0.7059300154903284\n",
      "Obtained with: SGDClassifier(loss='log', penalty='elasticnet')\n",
      "2 - done in 63s\n",
      "Score: 0.4930100602023389\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 1s\n",
      "Score: 0.6243430795680865\n",
      "Obtained with: MultinomialNB(alpha=1)\n",
      "4 - done in 1s\n",
      "Score: 0.6522858164197481\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
      "5 - done in 10s\n",
      "Score: 0.6871594822939304\n",
      "Obtained with: DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=3,\n",
      "                       random_state=42)\n",
      "7 - done in 47s\n",
      "Score: 0.7107564160032682\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=10, random_state=42)\n",
      "8 - done in 97s\n",
      "Score: 0.7420321268277735\n",
      "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 899s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 5128, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2339, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2266, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6080, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6099, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/json_helper.h:173: Can't parse parameter \"class_weights\" with value: null\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.71175619 0.67402221\n",
      " 0.71084153 0.69628743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7117561947128614\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000200000EFD90>\n",
      "12 - done in 1210s\n",
      "Score: 0.745211332338472\n",
      "Obtained with: LGBMClassifier(boosting_type='dart', num_leaves=10, random_state=42)\n",
      "13 - done in 59s\n",
      "The file could not be found.\n",
      "Score: 0.5\n",
      "Obtained with: DummyClassifier(strategy='most_frequent')\n",
      "0 - done in 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7123973694812159\n",
      "Obtained with: LogisticRegression(C=0.01, n_jobs=1, random_state=42, solver='sag')\n",
      "1 - done in 153s\n",
      "Score: 0.6908402793933238\n",
      "Obtained with: SGDClassifier(loss='log', penalty='elasticnet')\n",
      "2 - done in 160s\n",
      "Score: 0.5013617871186287\n",
      "Obtained with: GaussianNB()\n",
      "3 - done in 2s\n",
      "Score: 0.6176215253150551\n",
      "Obtained with: MultinomialNB(alpha=10)\n",
      "4 - done in 2s\n",
      "Score: 0.6502249785802234\n",
      "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
      "5 - done in 18s\n",
      "Score: 0.6338256572041375\n",
      "Obtained with: DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=3,\n",
      "                       random_state=42)\n",
      "7 - done in 222s\n",
      "Score: 0.6793547398702897\n",
      "Obtained with: RandomForestClassifier(criterion='entropy', random_state=42)\n",
      "8 - done in 346s\n",
      "Score: 0.7134448107399611\n",
      "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n",
      "11 - done in 1397s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 5128, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2339, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\achou\\anaconda3\\lib\\site-packages\\catboost\\core.py\", line 2266, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6080, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6099, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/json_helper.h:173: Can't parse parameter \"class_weights\" with value: null\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.6617718  0.66265583\n",
      " 0.66919467 0.66158455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.669194673142721\n",
      "Obtained with: <catboost.core.CatBoostClassifier object at 0x00000200000EFD90>\n",
      "12 - done in 4513s\n",
      "Score: 0.6970585398237621\n",
      "Obtained with: LGBMClassifier(boosting_type='dart', num_leaves=10, random_state=42)\n",
      "13 - done in 127s\n"
     ]
    }
   ],
   "source": [
    "result_dfs = dict()\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    # I create a different results table for each sampler.\n",
    "    # str(sampler) includes parentheses and the arguments inside, so i strip\n",
    "    # these out in order to keep only the name of the sampler.\n",
    "    nom_du_df = \"result_df_\" + str(sampler).split('(')[0] + \".csv\"\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "        \"classifier\",\n",
    "        \"best_score\",\n",
    "        \"avg_score_folds\",\n",
    "        \"fold0_score\",\n",
    "        \"fold1_score\",\n",
    "        \"fold2_score\",\n",
    "        \"fold3_score\",\n",
    "        \"fold4_score\",\n",
    "        \"run_time (s)\",\n",
    "    ])\n",
    "    # I gridsearch each classifier with its own set of parameters.\n",
    "    for (n, clf, param) in list(zip(dA.nz, dA.clfs, dA.params))[0:]:\n",
    "        with timer(str(n)):\n",
    "            try:\n",
    "                result_df = pd.read_csv(nom_du_df)\n",
    "                result_df = clean_csv(result_df)\n",
    "            except ValueError:\n",
    "                print(\"An error occurred while reading the CSV file.\")\n",
    "            except FileNotFoundError:\n",
    "                print(\"The file could not be found.\")\n",
    "\n",
    "            t_clf = time.perf_counter()\n",
    "\n",
    "            # Without sampler i still try to optimize the gridsearch using class_weight.\n",
    "            if sampler is None:\n",
    "                # class_weight does not exist for dummy, NBayes, KN, GB, AdaB, XGB.\n",
    "                # CatBoost either.\n",
    "                if n in [1, 2, 6, 7, 8, 13]:\n",
    "                    param[\"classifier__class_weight\"] = cws\n",
    "                elif n in [12]:\n",
    "                    param[\"classifier__class_weights\"] = cws\n",
    "                elif n in [11]:\n",
    "                    param[\"classifier__scale_pos_weight\"] = cws1\n",
    "\n",
    "            # Train the model\n",
    "            # I need a MinMaxScaler in the pipe because (at least) MultinomialNB\n",
    "            # errors on negative values.\n",
    "            pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                                   (\"sampler\", sampler),\n",
    "                                                   (\"classifier\", clf)])\n",
    "            gs = GridSearchCV(\n",
    "                pipeline,\n",
    "                param,\n",
    "                cv=5,\n",
    "                n_jobs=None,\n",
    "                scoring=\"roc_auc\",\n",
    "                # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "                # refit=\"roc_auc\",\n",
    "                # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "                # random_state=42, # seulement pour RandomSearchCV ?\n",
    "            ).fit(X_train, y_train)\n",
    "\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "            if t_mean > 10:\n",
    "                t_mean = round(t_mean, 0)\n",
    "            elif t_mean > 1:\n",
    "                t_mean = round(t_mean, 2)\n",
    "            elif t_mean > .001:\n",
    "                t_mean = round(t_mean, 3)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Save the results\n",
    "            print(\"Score:\", gs.best_score_)\n",
    "            print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "            result_df.loc[n, :] = [\n",
    "                gs.best_params_[\"classifier\"],\n",
    "                round(gs.best_score_, 3),\n",
    "                round(np.mean([\n",
    "                    gs.cv_results_[\"split0_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split1_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split2_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split3_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split4_test_score\"].max(),\n",
    "                ], axis=1), 3)\n",
    "                round(gs.cv_results_[\"split0_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split1_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split2_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split3_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split4_test_score\"].max(), 3),\n",
    "                t_mean\n",
    "            ]\n",
    "\n",
    "            # Je save à chaque classifier pour pouvoir relancer la loop à partir\n",
    "            # du classifier qui bug en cas d'erreur.\n",
    "            #result_df.sort_index(by=\"best_score\", inplace=True)\n",
    "            result_df.sort_values(by=\"best_score\", inplace=True)\n",
    "            result_df.to_csv(nom_du_df)\n",
    "\n",
    "    result_dfs[str(sampler)] = result_df\n",
    "    #display(result_df[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_check = pd.DataFrame(columns=[\"clf\", \"t_tot\", \"rt_x_nl\", \"run_time\", \"n_loops\"])\n",
    "t_check.clf = dA.clf\n",
    "t_check.t_tot = [2, 153, 160, 2, 2, 18, 222, 346, 1397, 4513, 127]\n",
    "#t_check.run_time = result_df[\"run_time (s)\"] # non car il est sorted par score.\n",
    "t_check.run_time = [1, 19, 20, 1.6, 1.1, 2.3, 9.2, 14, 116, 564, 7]\n",
    "t_check.n_loops = nl\n",
    "t_check.rt_x_nl = t_check.run_time*t_check.n_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>t_tot</th>\n",
       "      <th>rt_x_nl</th>\n",
       "      <th>run_time</th>\n",
       "      <th>n_loops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>153</td>\n",
       "      <td>152.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDC</td>\n",
       "      <td>160</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussNB</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinNB</td>\n",
       "      <td>2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighbor</td>\n",
       "      <td>18</td>\n",
       "      <td>18.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionT</td>\n",
       "      <td>222</td>\n",
       "      <td>220.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomF</td>\n",
       "      <td>346</td>\n",
       "      <td>336.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGB</td>\n",
       "      <td>1397</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CatB</td>\n",
       "      <td>4513</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>127</td>\n",
       "      <td>126.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          clf  t_tot  rt_x_nl  run_time  n_loops\n",
       "0       Dummy      2      2.0       1.0        2\n",
       "1    Logistic    153    152.0      19.0        8\n",
       "2        SGDC    160    160.0      20.0        8\n",
       "3     GaussNB      2      1.6       1.6        1\n",
       "4    MultinNB      2      2.2       1.1        2\n",
       "5   KNeighbor     18     18.4       2.3        8\n",
       "7   DecisionT    222    220.8       9.2       24\n",
       "8     RandomF    346    336.0      14.0       24\n",
       "11        XGB   1397   1392.0     116.0       12\n",
       "12       CatB   4513   4512.0     564.0        8\n",
       "13       LGBM    127    126.0       7.0       18"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ```t_tot``` environ identique à ```rt_x_nl``` donc les temps sont\n",
    "correctment évalués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. Itération sur les classifiers avec une méthode étrange\n",
    "En fait ici on ne donne au SearchCV qu'une seule pipe de travail ne contenant\n",
    "qu'un seul estimateur, mais il va quand même itérer sur tous les classifiers\n",
    "car ils sont tous renseignés dans ```params```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "params = [\n",
    "    # DummyClassifier\n",
    "    param0,\n",
    "    # LogisticRegression\n",
    "    # param1,\n",
    "    # SGDClassifier\n",
    "    # param2,\n",
    "    # GaussianNB\n",
    "    # param3,\n",
    "    # MultinomialNB\n",
    "    # param4,\n",
    "    # KNeighborsClassifier\n",
    "    param5,\n",
    "    # SVC\n",
    "    # param6,\n",
    "    # DecisionTreeClassifier\n",
    "    # param7,\n",
    "    # RandomForestClassifier\n",
    "    param8,\n",
    "    # GradientBoostingClassifier\n",
    "    # param9,\n",
    "    # AdaBoostClassifier\n",
    "    # param10,\n",
    "    # XGBClassifier\n",
    "    param11,\n",
    "    # CatBoostClassifier\n",
    "    # param12,\n",
    "    # LGBMClassifier\n",
    "    # param13,\n",
    "    # Samplers\n",
    "    # param_sampler,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "%%time\n",
    "rs = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    # n_jobs=-1,\n",
    "    scoring=[\"roc_auc\", \"accuracy\"],\n",
    "    refit=\"roc_auc\",\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "#).fit(X_train_u, y_train_u)\n",
    "#).fit(X_train_o, y_train_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_N = clean_csv(pd.read_csv(\"result_df_None.csv\"))\n",
    "#res_U = clean_csv(pd.read_csv(\"result_df_RandomUnderSampler.csv\")).sort_values(by=\"score\")\n",
    "#res_O = clean_csv(pd.read_csv(\"result_df_RandomOverSampler.csv\")).sort_values(by=\"score\")\n",
    "res_S = clean_csv(pd.read_csv(\"result_df_SMOTE.csv\"))\n",
    "#res = [res_N, res_U, res_O, res_S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "res_N[\"avg_score_folds\"] = round(res_N[[\n",
    "    \"fold1_score\", \"fold2_score\", \"fold3_score\", \"fold4_score\", \"fold5_score\",\n",
    "]].mean(axis=1), 3)\n",
    "#res_N[\"avg_score_folds\"] = round(res_N[[\n",
    "#    \"fold0_score\", \"fold1_score\", \"fold2_score\", \"fold3_score\", \"fold4_score\",\n",
    "#]].mean(axis=1), 3)\n",
    "res_S[\"avg_score_folds\"] = round(res_S[[\n",
    "    \"fold1_score\", \"fold2_score\", \"fold3_score\", \"fold4_score\", \"fold5_score\",\n",
    "]].mean(axis=1), 3)\n",
    "#res_S[\"avg_score_folds\"] = round(res_S[[\n",
    "#    \"fold0_score\", \"fold1_score\", \"fold2_score\", \"fold3_score\", \"fold4_score\",\n",
    "#]].mean(axis=1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>fold5_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "      <th>avg_score_folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinomialNB(alpha=1)</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.642</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDClassifier(loss='log', penalty='elasticnet')</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.689</td>\n",
       "      <td>7.820</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.701</td>\n",
       "      <td>4.040</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, n_jobs=1, random_sta...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.718</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.739</td>\n",
       "      <td>75.000</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.735</td>\n",
       "      <td>3.300</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "3                                        GaussianNB()       0.493   \n",
       "0           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "4                              MultinomialNB(alpha=1)       0.624   \n",
       "5   KNeighborsClassifier(n_neighbors=1000, weights...       0.652   \n",
       "6   DecisionTreeClassifier(criterion='entropy', ma...       0.687   \n",
       "2     SGDClassifier(loss='log', penalty='elasticnet')       0.706   \n",
       "7   RandomForestClassifier(criterion='entropy', ma...       0.711   \n",
       "9   <catboost.core.CatBoostClassifier object at 0x...       0.712   \n",
       "1   LogisticRegression(C=0.1, n_jobs=1, random_sta...       0.725   \n",
       "8   XGBClassifier(base_score=None, booster='dart',...       0.742   \n",
       "10  LGBMClassifier(boosting_type='dart', num_leave...       0.745   \n",
       "\n",
       "    fold1_score  fold2_score  fold3_score  fold4_score  fold5_score  \\\n",
       "3         0.493        0.496        0.492        0.503        0.481   \n",
       "0         0.500        0.500        0.500        0.500        0.500   \n",
       "4         0.624        0.619        0.620        0.662        0.596   \n",
       "5         0.674        0.631        0.638        0.681        0.642   \n",
       "6         0.691        0.683        0.690        0.705        0.669   \n",
       "2         0.710        0.689        0.702        0.743        0.689   \n",
       "7         0.731        0.704        0.699        0.735        0.701   \n",
       "9           NaN          NaN          NaN          NaN          NaN   \n",
       "1         0.739        0.702        0.706        0.760        0.718   \n",
       "8         0.747        0.754        0.728        0.758        0.739   \n",
       "10        0.739        0.755        0.744        0.758        0.735   \n",
       "\n",
       "    run_time (s)  avg_score_folds  \n",
       "3          0.696            0.493  \n",
       "0          0.297            0.500  \n",
       "4          0.390            0.624  \n",
       "5          1.280            0.653  \n",
       "6          1.940            0.688  \n",
       "2          7.820            0.707  \n",
       "7          4.040            0.714  \n",
       "9        151.000              NaN  \n",
       "1         12.000            0.725  \n",
       "8         75.000            0.745  \n",
       "10         3.300            0.746  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_N.sort_values(by=\"best_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>fold5_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "      <th>avg_score_folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.495</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinomialNB(alpha=10)</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.601</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.668</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.650</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>564.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ra...</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.675</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDClassifier(loss='log', penalty='elasticnet')</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.679</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.694</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.01, n_jobs=1, random_st...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.706</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.723</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "0           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "3                                        GaussianNB()       0.501   \n",
       "4                             MultinomialNB(alpha=10)       0.618   \n",
       "6   DecisionTreeClassifier(criterion='entropy', ma...       0.634   \n",
       "5   KNeighborsClassifier(n_neighbors=1000, weights...       0.650   \n",
       "9   <catboost.core.CatBoostClassifier object at 0x...       0.669   \n",
       "7   RandomForestClassifier(criterion='entropy', ra...       0.679   \n",
       "2     SGDClassifier(loss='log', penalty='elasticnet')       0.691   \n",
       "10  LGBMClassifier(boosting_type='dart', num_leave...       0.697   \n",
       "1   LogisticRegression(C=0.01, n_jobs=1, random_st...       0.712   \n",
       "8   XGBClassifier(base_score=None, booster='dart',...       0.713   \n",
       "\n",
       "    fold1_score  fold2_score  fold3_score  fold4_score  fold5_score  \\\n",
       "0         0.500        0.500        0.500        0.500        0.500   \n",
       "3         0.500        0.503        0.499        0.510        0.495   \n",
       "4         0.632        0.593        0.612        0.652        0.601   \n",
       "6         0.650        0.627        0.629        0.673        0.668   \n",
       "5         0.673        0.619        0.630        0.687        0.650   \n",
       "9           NaN          NaN          NaN          NaN          NaN   \n",
       "7         0.676        0.679        0.679        0.717        0.675   \n",
       "2         0.700        0.676        0.681        0.732        0.679   \n",
       "10        0.721        0.681        0.700        0.729        0.694   \n",
       "1         0.723        0.690        0.699        0.757        0.706   \n",
       "8         0.723        0.721        0.695        0.759        0.723   \n",
       "\n",
       "    run_time (s)  avg_score_folds  \n",
       "0           1.03            0.500  \n",
       "3           1.61            0.501  \n",
       "4           1.13            0.618  \n",
       "6           9.23            0.649  \n",
       "5           2.26            0.652  \n",
       "9         564.00              NaN  \n",
       "7          14.00            0.685  \n",
       "2          20.00            0.694  \n",
       "10          7.05            0.705  \n",
       "1          19.00            0.715  \n",
       "8         116.00            0.724  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_S.sort_values(by=\"best_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partons sur les résultats de SMOTE (parce que ça fait partie de l'exercice)\n",
    "même s'ils ne sont pas les meilleurs, ça me donnera une marge de progression\n",
    "pour le fine tuning, au moins.  \n",
    "Et ça tombe bien parce que LGBM compute vite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fine tuning with the best classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "                                     \"rf\",\n",
    "                                     \"dart\"]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,\n",
    "                                     10**-1.5,\n",
    "                                     10**-1]\n",
    "parm[\"classifier__num_leaves\"] = [10**1,\n",
    "                                  10**1.5,\n",
    "                                  10**2]\n",
    "parm[\"classifier__num_iterations\"] = [10**3,\n",
    "                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [6,\n",
    "                                 8]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est trop à mon avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "#                                     \"rf\",\n",
    "#                                     \"dart\"]\n",
    "                                    ]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,\n",
    "                                     10**-1.5,\n",
    "                                     10**-1]\n",
    "#parm[\"classifier__num_leaves\"] = [10**1,\n",
    "#                                  10**1.5,\n",
    "#                                  10**2]\n",
    "#parm[\"classifier__num_iterations\"] = [10**3,\n",
    "#                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-2,\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [6,\n",
    "                                 8]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.8,\n",
    "                                        .9,\n",
    "                                        .95]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok testons ça."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier fine tuning a donné:\n",
    "```\n",
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "#                                     \"rf\",\n",
    "#                                     \"dart\"]\n",
    "                                    ]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,     ? pas vu dans l'output\n",
    "                                     10**-1.5,     ? pas vu dans l'output\n",
    "                                     10**-1]     ? pas vu dans l'output\n",
    "#parm[\"classifier__num_leaves\"] = [10**1,\n",
    "#                                  10**1.5,\n",
    "#                                  10**2]\n",
    "#parm[\"classifier__num_iterations\"] = [10**3,\n",
    "#                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.8,\n",
    "                                        .9,     <------------\n",
    "                                        .95]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2,     <------------\n",
    "                                 10**-1.5]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-2,\n",
    "                                 10**-1.5]     <------------\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [6,\n",
    "                                 8]     <------------\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.8,\n",
    "                                        .9,     <------------\n",
    "                                        .95]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]\n",
    "```\n",
    "\n",
    "Je teste maintenant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\"gbdt\",\n",
    "#                                     \"rf\",\n",
    "#                                     \"dart\"]\n",
    "                                    ]\n",
    "parm[\"classifier__learning_rate\"] = [10**-2,\n",
    "                                     10**-1.5,\n",
    "                                     10**-1]\n",
    "parm[\"classifier__num_leaves\"] = [30]\n",
    "#parm[\"classifier__num_iterations\"] = [10**3,\n",
    "#                                      10**4]\n",
    "parm[\"classifier__feature_fraction\"] = [.9]\n",
    "parm[\"classifier__lambda_l1\"] = [10**-2]\n",
    "parm[\"classifier__lambda_l2\"] = [10**-1.5]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [8]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [.9]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier__out_len\"] = [10]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", round(gs.best_score_, 3))\n",
    "print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Applying the fine-tuned best classifier for the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = rs_cv.best_estimator_[best_algorithm].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "# On peut aussi faire un predict_proba avec un seuil < .5 au lieu de faire un\n",
    "# oversampling ou de jouer sur class_weights.\n",
    "y_pred_proba = gs.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_positive)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion                 Predicted\n",
    "    Matrix               Positive    Negative\n",
    "              Positive      TP          FN\n",
    "    Actual    Negative      FP          TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .1\n",
    "fb_score = fbeta_score(y_test, y_pred, beta=beta)\n",
    "fb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .01\n",
    "fb_score = fbeta_score(y_test, y_pred, beta=beta)\n",
    "fb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Features' importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "# InvalidModelError: Model type not yet supported by TreeExplainer:\n",
    "# <class 'imblearn.pipeline.Pipeline'>\n",
    "explainer = shap.TreeExplainer(gs.best_estimator_)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MemoryError: Unable to allocate 148. GiB for an array with shape\n",
    "# (3312, 5984000) and data type float64\n",
    "K = 10\n",
    "feature_names = list(X_train.columns)\n",
    "X_train_sample = shap.sample(X_train, K)\n",
    "print(X_train_sample.shape)\n",
    "explainer = shap.KernelExplainer(gs.best_estimator_.predict_proba, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Precision:\",\n",
    "      precision_score(rs.predict(X_test), y_test))\n",
    "      # precision_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"Recall:\",\n",
    "      recall_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"ROC AUC Score:\",\n",
    "      roc_auc_score(rs.best_estimator_.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot la courbe roc du meilleur pour chaque sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap (et lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "#rs_cv.fit(X_train, y_train)\n",
    "#rs_cv.fit(X_train_u, y_train_u)\n",
    "rs_cv.fit(X_train_o, y_train_o)\n",
    "\n",
    "for algorithm in classifiers.keys():\n",
    "    print(f\"Best parameters for {algorithm}: {rs_cv.best_params_[algorithm]}\")\n",
    "    print(f\"Best AUC score for {algorithm}: {rs_cv.best_score_[algorithm]['roc_auc']:.3f}\")\n",
    "    print(f\"Best accuracy score for {algorithm}: {rs_cv.best_score_[algorithm]['accuracy']:.3f}\")\n",
    "\n",
    "#best_algorithm = rs_cv.best_estimator_.keys()[0]\n",
    "best_algorithm = rs_cv.best_estimator_.named_steps.keys()\n",
    "print(f\"Overall best algorithm: {best_algorithm}\")\n",
    "print(f\"Best AUC score: {rs_cv.best_score_[best_algorithm]['roc_auc']:.3f}\")\n",
    "print(f\"Best accuracy score: {rs_cv.best_score_[best_algorithm]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
