{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle/python Docker image: https://github.com/kaggle/docker-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (most) package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import imblearn.pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.utils.testing import ignore_warnings # For LogisticRegression\n",
    "#from sklearn.exceptions import ConvergenceWarning # For LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.perf_counter() - t0))\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offline j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"home-credit-default-risk/application_test.csv\",\n",
    "    \"home-credit-default-risk/application_train.csv\",\n",
    "    \"home-credit-default-risk/bureau.csv\",\n",
    "    \"home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"home-credit-default-risk/installments_payments.csv\",\n",
    "    \"home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"home-credit-default-risk/previous_application.csv\",\n",
    "    \"home-credit-default-risk/sample_submission.csv\"\n",
    "    ]\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "not offline (on Kaggle) j'ai:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">filenames = [\n",
    "    \"/kaggle/input/home-credit-default-risk/sample_submission.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_train.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/application_test.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/previous_application.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/credit_card_balance.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/installments_payments.csv\",\n",
    "    \"/kaggle/input/home-credit-default-risk/bureau.csv\"]\n",
    "    ]\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    le_path = \"home-credit-default-risk/\"\n",
    "else:\n",
    "    le_path = \"/kaggle/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for dirname, _, filenamess in os.walk(le_path):\n",
    "    for filenamee in filenamess:\n",
    "#                        HomeCredit_columns_description.csv est illisible.\n",
    "        if filenamee != \"HomeCredit_columns_description.csv\":\n",
    "            filename = os.path.join(dirname, filenamee)\n",
    "#            print(filename)\n",
    "            filenames.append(filename)\n",
    "#            df = pd.read_csv(filename)\n",
    "#            display(df[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not offline:\n",
    "    flnms = []\n",
    "    flnms.append(filenames[4])\n",
    "    flnms.append(filenames[3])\n",
    "    flnms.append(filenames[8])\n",
    "    flnms.append(filenames[1])\n",
    "    flnms.append(filenames[6])\n",
    "    flnms.append(filenames[7])\n",
    "    flnms.append(filenames[2])\n",
    "    flnms.append(filenames[5])\n",
    "    flnms.append(filenames[0])\n",
    "    filenames = flnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. application_train and application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(filenames[1], nrows=num_rows)\n",
    "    test_df = pd.read_csv(filenames[0], nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Aberrant values\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(filenames[2], nrows=num_rows)\n",
    "    bb = pd.read_csv(filenames[3], nrows=num_rows)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()]\n",
    "    )\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg(\n",
    "        {**num_aggregations, **cat_aggregations}\n",
    "    )\n",
    "    bureau_agg.columns = pd.Index([\n",
    "        \"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\n",
    "        \"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\n",
    "        \"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()\n",
    "    ])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(filenames[7], nrows=num_rows)\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Aberrant values\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\n",
    "        \"PREV_\" + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\n",
    "        \"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\n",
    "        \"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()\n",
    "    ])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. pos_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(filenames[6], nrows=num_rows)\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\n",
    "        \"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. installment_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(filenames[5], nrows=num_rows)\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Features\n",
    "    # Percentage and difference paid in each installment (amount paid and\n",
    "    # installment value)\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\n",
    "        \"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(filenames[4], nrows=num_rows)\n",
    "\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # General aggregations\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace =True)\n",
    "\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\n",
    "        \"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()\n",
    "    ])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. functions from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "        train_df.shape, test_df.shape\n",
    "    ))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds,\n",
    "                      shuffle=True,\n",
    "                      random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\n",
    "                                                      \"SK_ID_CURR\",\n",
    "                                                      \"SK_ID_BUREAU\",\n",
    "                                                      \"SK_ID_PREV\",\n",
    "                                                      \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "            folds.split(train_df[feats], train_df[\"TARGET\"])\n",
    "    ):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx],\\\n",
    "                           train_df[\"TARGET\"].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat(\n",
    "            [feature_importance_df, fold_importance_df],\n",
    "            axis=0\n",
    "        )\n",
    "        print(\"Fold %2d AUC : %.6f\" % (\n",
    "            n_fold + 1,\n",
    "            roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        ))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Write submission file and plot feature importance\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(submission_file_name,\n",
    "                                                 index=False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n",
    "        \"feature\"\n",
    "    ).mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[\n",
    "        feature_importance_df_.feature.isin(cols)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"feature\",\n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title(\"LightGBM Features (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"lgbm_importances01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(debug=True):\n",
    "    num_rows = debug if debug else None\n",
    "    with timer(\"Process application train test\"):\n",
    "        df = application_train_test(num_rows)\n",
    "        print(\"Application train test df shape:\", df.shape)\n",
    "        # print(df.dtypes.value_counts())\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        # print(df.dtypes.value_counts())\n",
    "        del cc\n",
    "        gc.collect()\n",
    "\n",
    "    zeros = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[0]\n",
    "    ones = df.TARGET.value_counts(\n",
    "        sort=True,\n",
    "        ascending=False,\n",
    "        dropna=True,\n",
    "    )[1]\n",
    "    nans = df.TARGET.isna().sum()\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    if debug:\n",
    "        print(\"subsampled df's TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "    else:\n",
    "        print(\"TARGET has\",\n",
    "              f\"{zeros:10.0f} zeros,\",\n",
    "              f\"{ones:10.0f} ones and\",\n",
    "              f\"{nans:10.0f} NaNs\")\n",
    "\n",
    "    return zeros, ones, nans, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. A first full run just to measure the target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# I ran this cell only once, just to get the exact values of zo and oz.\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_full\"):\n",
    "        zeros_full, ones_full, nans_full, df_full = preproc(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_full, ones_full, nans_full = 282682, 24825, 48744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo = 11.39 more zeros than ones in TARGET. (and oz = 0.09)\n"
     ]
    }
   ],
   "source": [
    "zo = zeros_full/ones_full\n",
    "oz = ones_full/zeros_full\n",
    "print(\"There is zo =\",\n",
    "      f\"{zo:.2f} more zeros than ones in TARGET. (and oz =\",\n",
    "      f\"{oz:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Subsampled run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Run the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 100000, test samples: 48744\n",
      "Application train test df shape: (148741, 248)\n",
      "Process application train test - done in 1s\n",
      "Bureau df shape: (23121, 113)\n",
      "Process bureau and bureau_balance - done in 1s\n",
      "Previous applications df shape: (79977, 247)\n",
      "Process previous_applications - done in 1s\n",
      "Pos-cash balance df shape: (77469, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (48591, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (53383, 131)\n",
      "Process credit card balance - done in 1s\n",
      "-----------------------------------------------------------------------\n",
      "subsampled df's TARGET has      91904 zeros,       8093 ones and      48744 NaNs\n",
      "preproc_subsampled - done in 5s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"preproc_subsampled\"):\n",
    "        # I tried 1_000 here but the ROC AUC scores obtained were > 90% which\n",
    "        # proves overfitting.\n",
    "        zeros, ones, nans, df = preproc(debug=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Measure of the target imbalance after the subsampling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is zo_sub = 11.36 more zeros than ones in TARGET. (and oz_sub = 0.09)\n"
     ]
    }
   ],
   "source": [
    "zo_sub = zeros/ones\n",
    "oz_sub = ones/zeros\n",
    "print(\"There is zo_sub =\",\n",
    "      f\"{zo_sub:.2f} more zeros than ones in TARGET. (and oz_sub =\",\n",
    "      f\"{oz_sub:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'imbalance de 11.39 des targets du dataset est de 11.36 après subsampling.\n"
     ]
    }
   ],
   "source": [
    "if (zo/zo_sub >= 3/2 or zo/zo_sub <= 2/3):\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "    print(\n",
    "        \"L'imbalance des targets a été fortement modifiée par le subsampling.\",\n",
    "        f\"Elle est passée de {zo:.2f} à {zo_sub:.2f}.\",\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        print(\"!\")\n",
    "else:\n",
    "    print(\n",
    "        f\"L'imbalance de {zo:.2f} des targets du dataset est de {zo_sub:.2f}\",\n",
    "        \"après subsampling.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Suppression du caractère illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_df = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "# Ce code prend un temps infini à run. Prende la cell en-dessous.\n",
    "for j in cols_of_df:\n",
    "    df = df.rename(columns={j: re.sub(r\"[ ]\", r\"_a_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[-]\", r\"_b_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_c_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[/]\", r\"_d_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[,]\", r\"_e_\", j)})\n",
    "    df = df.rename(columns={j: re.sub(r\"[:]\", r\"_f_\", j)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipped\n",
    "df = df.rename(columns=lambda x: x.replace(\" \", \"_a_\")\\\n",
    "                                  .replace(\"-\", \"_b_\")\\\n",
    "                                  .replace(\":\", \"_c_\")\\\n",
    "                                  .replace(\"/\", \"_d_\")\\\n",
    "                                  .replace(\",\", \"_e_\")\\\n",
    "                                  .replace(\":\", \"_f_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.rename(columns=lambda x: x.replace(\":\", \"deuxpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification run from the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipped\n",
    "with timer(\"Run LightGBM with kfold\"):\n",
    "    feat_importance = kfold_lightgbm(df, num_folds=10, stratified=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. Suppression des données sans TARGET et colonnes sans données\n",
    "En effet mon but ici c'est juste de créer un modèle qui fonctionne, faire les\n",
    " prédictions de solvabilité des futurs clients ça sera pour plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"TARGET\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_notnull = list(df.loc[:, df.notnull().sum() > 0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(df).sum()[np.isinf(df).sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, cols_notnull]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"TARGET\", axis=\"columns\")\n",
    "y = df[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    73518\n",
       "1.0     6479\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_old = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names_old = list(X_train.columns)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "#X_train_i = imputer.fit_transform(X_train)\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "feature_names = imputer.get_feature_names_out()\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_old = X_test.copy()\n",
    "# X_test = imputer.fit_transform(X_test_old)\n",
    "X_test = imputer.transform(X_test_old) # pas de fit, recommandé par mentor\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 779)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Balancing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy=\"majority\")\n",
    "#X_train_u, y_train_u = undersampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler_1 = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "# La doc de imblearn dit exactement l'inverse mais ici j'ai cette erreur:\n",
    "#\n",
    "# ValueError: The 'sampling_strategy' parameter of RandomOverSampler must be a\n",
    "# float in the range (0, 1], a str among {'majority', 'not majority', 'all',\n",
    "# 'auto', 'not minority'}, an instance of 'collections.abc.Mapping' or a\n",
    "# callable. Got 'minority' instead.\n",
    "oversampler_2 = SMOTE()\n",
    "#X_train_o, y_train_o = oversampler.fit_resample(X_train_i, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Declaring the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = DummyClassifier()\n",
    "clf1 = LogisticRegression(random_state=42, n_jobs=1, solver=\"sag\")\n",
    "clf2 = SGDClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = MultinomialNB()\n",
    "clf5 = KNeighborsClassifier()\n",
    "clf6 = SVC(probability=True, random_state=42)\n",
    "clf7 = DecisionTreeClassifier(random_state=42)\n",
    "clf8 = RandomForestClassifier(random_state=42)\n",
    "clf9 = GradientBoostingClassifier(random_state=42)\n",
    "clf10 = AdaBoostClassifier(random_state=42)\n",
    "clf11 = XGBClassifier(random_state=42)\n",
    "# Even with logging_level=\"info\" catboost prints millions of lines and crashes\n",
    "# my computer so i need logging_level=\"Silent\" here.\n",
    "clf12 = CatBoostClassifier(random_state=42, logging_level=\"Silent\")\n",
    "clf13 = LGBMClassifier(random_state=42, verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Declaring the classifiers' parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.0. DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param0 = {}\n",
    "param0[\"classifier__strategy\"] = [\"most_frequent\",\n",
    "                                  \"prior\"]\n",
    "param0[\"classifier\"] = [clf0]\n",
    "#param0[\"classifier\"] = [classifiers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = {}\n",
    "param1[\"classifier__C\"] = [\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param1[\"classifier__penalty\"] = [\n",
    "    # The default solver lbfgs supports only l2 penalty and None.\n",
    "    # \"l1\",\n",
    "    \"l2\",\n",
    "    # \"elasticnet\",\n",
    "]\n",
    "# param1[\"classifier__class_weight\"] = [{0: 1, 1: 1},\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# param1[\"classifier__class_weight\"] = [None]\n",
    "# param1[\"classifier__class_weight\"] = [None,\n",
    "#                                       {0: oz, 1: zo}]\n",
    "# Je rajoute les class_weights et leurs variantes plus tard dans un if.\n",
    "param1[\"classifier\"] = [clf1]\n",
    "#param1[\"classifier\"] = [classifiers[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param2 = {}\n",
    "param2[\"classifier__loss\"] = [\n",
    "    \"hinge\",\n",
    "    \"log\",\n",
    "    \"squared_hinge\",\n",
    "    \"modified_huber\",\n",
    "]\n",
    "param2[\"classifier__penalty\"] = [\n",
    "    # \"l1\",\n",
    "    # \"l2\",\n",
    "    \"elasticnet\",\n",
    "]\n",
    "param2[\"classifier\"] = [clf2]\n",
    "#param2[\"classifier\"] = [classifiers[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3 = {}\n",
    "param3[\"classifier\"] = [clf3]\n",
    "#param3[\"classifier\"] = [classifiers[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param4 = {}\n",
    "param4[\"classifier__alpha\"] = [\n",
    "    10**0,\n",
    "    10**1,\n",
    "    # 10**2,\n",
    "]\n",
    "param4[\"classifier\"] = [clf4]\n",
    "#param4[\"classifier\"] = [classifiers[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param5 = {}\n",
    "param5[\"classifier__n_neighbors\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    10**2,\n",
    "    10**3,\n",
    "]\n",
    "param5[\"classifier__weights\"] = [\n",
    "    \"uniform\",\n",
    "    \"distance\",\n",
    "]\n",
    "param5[\"classifier\"] = [clf5]\n",
    "#param5[\"classifier\"] = [classifiers[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "param6 = {}\n",
    "param6[\"classifier__kernel\"] = [\"linear\",\n",
    "                                \"rbf\",\n",
    "                                \"poly\",\n",
    "                                \"sigmoid\"]\n",
    "param6[\"classifier__C\"] = [10**-2,\n",
    "                           10**-1,\n",
    "                           10**0,\n",
    "                           10**1,\n",
    "                           10**2,\n",
    "                           10**3]\n",
    "param6[\"classifier__gamma\"] = [\"auto\",\n",
    "                               \"scale\"]\n",
    "param6[\"classifier\"] = [clf6]\n",
    "#param6[\"classifier\"] = [classifiers[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param7 = {}\n",
    "param7[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param7[\"classifier__min_samples_split\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "]\n",
    "param7[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param7[\"classifier\"] = [clf7]\n",
    "#param7[\"classifier\"] = [classifiers[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5.8. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param8 = {}\n",
    "param8[\"classifier__n_estimators\"] = [\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param8[\"classifier__max_depth\"] = [\n",
    "    int(10**.5),\n",
    "    10**1,\n",
    "    # int(10**1.5),\n",
    "    None,\n",
    "]\n",
    "param8[\"classifier__criterion\"] = [\n",
    "    \"gini\",\n",
    "    \"entropy\",\n",
    "]\n",
    "param8[\"classifier\"] = [clf8]\n",
    "#param8[\"classifier\"] = [classifiers[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.9. GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param9 = {}\n",
    "param9[\"classifier__n_estimators\"] = [10**1,\n",
    "                                      10**2,\n",
    "                                      10**3]\n",
    "param9[\"classifier__max_depth\"] = [3,\n",
    "                                   10**1,\n",
    "                                   30]\n",
    "param9[\"classifier\"] = [clf9]\n",
    "#param9[\"classifier\"] = [classifiers[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.10. AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "param10 = {}\n",
    "param10[\"classifier__n_estimators\"] = [10**1,\n",
    "                                       10**2,\n",
    "                                       10**3]\n",
    "param10[\"classifier__learning_rate\"] = [10**-3,\n",
    "                                        10**-2,\n",
    "                                        10**-1]\n",
    "param10[\"classifier\"] = [clf10]\n",
    "#param10[\"classifier\"] = [classifiers[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.11. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param11 = {}\n",
    "param11[\"classifier__booster\"] = [\n",
    "    # \"gbtree\",\n",
    "    # gblinear doesn't support max_depth\n",
    "    # \"gblinear\",\n",
    "    \"dart\",\n",
    "]\n",
    "param11[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    # 10**-2,\n",
    "    10**-.5,\n",
    "    10**-1,\n",
    "]\n",
    "param11[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param11[\"classifier\"] = [clf11]\n",
    "#param11[\"classifier\"] = [classifiers[11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.12. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {}\n",
    "param12[\"classifier__iterations\"] = [\n",
    "    # 10**1,\n",
    "    # 10**2,\n",
    "    10**3,\n",
    "]\n",
    "param12[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1,\n",
    "]\n",
    "param12[\"classifier__depth\"] = [\n",
    "    # 10**0,\n",
    "    # 3,\n",
    "    6,\n",
    "    10**1,\n",
    "]\n",
    "param12[\"classifier\"] = [clf12]\n",
    "#param12[\"classifier\"] = [classifiers[12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.13. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "param13 = {}\n",
    "param13[\"classifier__boosting_type\"] = [\n",
    "    # \"gbdt\",\n",
    "    \"dart\",\n",
    "    # \"goss\",\n",
    "]\n",
    "param13[\"classifier__learning_rate\"] = [\n",
    "    # 10**-3,\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "param13[\"classifier__num_leaves\"] = [\n",
    "    10**1,\n",
    "    int(10**1.5),\n",
    "    10**2,\n",
    "]\n",
    "param13[\"classifier\"] = [clf13]\n",
    "#param13[\"classifier\"] = [classifiers[13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5bis. Listing des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = [\n",
    "    \"Dummy\",\n",
    "    \"Logistic\",\n",
    "    \"SGDC\",\n",
    "    \"GaussNB\",\n",
    "    \"MultinNB\",\n",
    "    \"KNeighbor\",\n",
    "    \"SVC\",\n",
    "    \"DecisionT\",\n",
    "    \"RandomF\",\n",
    "    \"GB\",\n",
    "    \"AdaB\",\n",
    "    \"XGB\",\n",
    "    \"CatB\",\n",
    "    \"LGBM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    clf0,\n",
    "    # LogisticReg has trouble converging and is quite slow.\n",
    "    clf1,\n",
    "    clf2,\n",
    "    clf3,\n",
    "    clf4,\n",
    "    clf5,\n",
    "    # No SVC because it never finishes computing.\n",
    "    clf6,\n",
    "    clf7,\n",
    "    clf8,\n",
    "    clf9,\n",
    "    # AdaBoost et GBoost sont de loin les plus lents et ne sont pas les\n",
    "    # meilleurs (ils sont proches des scores des meilleurs néanmoins).\n",
    "    # Je les abandonne car je ne peux pas attendre de nouveau 10h à chaque fois\n",
    "    # que je veux faire un nouveau RandomSearchCV.\n",
    "    clf10,\n",
    "    clf11,\n",
    "    clf12,\n",
    "    clf13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full = [\n",
    "    param0,\n",
    "    param1,\n",
    "    param2,\n",
    "    param3,\n",
    "    param4,\n",
    "    param5,\n",
    "    param6,\n",
    "    param7,\n",
    "    param8,\n",
    "    param9,\n",
    "    param10,\n",
    "    param11,\n",
    "    param12,\n",
    "    param13,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = pd.DataFrame(columns=[\"nz\", \"clf\", \"clfs\", \"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA.nz = nz\n",
    "dA.clf = clf\n",
    "dA.clfs = clfs\n",
    "dA.params = params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les raisons citées précédemment:\n",
    "dA.drop(index=[6, 9, 10], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nz</th>\n",
       "      <th>clf</th>\n",
       "      <th>clfs</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dummy</td>\n",
       "      <td>DummyClassifier()</td>\n",
       "      <td>{'classifier__strategy': ['most_frequent', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>LogisticRegression(n_jobs=1, random_state=42, ...</td>\n",
       "      <td>{'classifier__C': [0.01, 0.1, 1, 10], 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SGDC</td>\n",
       "      <td>SGDClassifier()</td>\n",
       "      <td>{'classifier__loss': ['hinge', 'log', 'squared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GaussNB</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{'classifier': [GaussianNB()]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MultinNB</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>{'classifier__alpha': [1, 10], 'classifier': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>KNeighbor</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>{'classifier__n_neighbors': [3, 10, 100, 1000]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DecisionT</td>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__max_depth': [3, 10, None], 'clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>RandomF</td>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>{'classifier__n_estimators': [31, 100], 'class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>XGB</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'classifier__booster': ['dart'], 'classifier_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>CatB</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'classifier__iterations': [1000], 'classifier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>LGBMClassifier(random_state=42, verbose=-1)</td>\n",
       "      <td>{'classifier__boosting_type': ['dart'], 'class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nz        clf                                               clfs  \\\n",
       "0    0      Dummy                                  DummyClassifier()   \n",
       "1    1   Logistic  LogisticRegression(n_jobs=1, random_state=42, ...   \n",
       "2    2       SGDC                                    SGDClassifier()   \n",
       "3    3    GaussNB                                       GaussianNB()   \n",
       "4    4   MultinNB                                    MultinomialNB()   \n",
       "5    5  KNeighbor                             KNeighborsClassifier()   \n",
       "7    7  DecisionT            DecisionTreeClassifier(random_state=42)   \n",
       "8    8    RandomF            RandomForestClassifier(random_state=42)   \n",
       "11  11        XGB  XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  12       CatB  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "13  13       LGBM        LGBMClassifier(random_state=42, verbose=-1)   \n",
       "\n",
       "                                               params  \n",
       "0   {'classifier__strategy': ['most_frequent', 'pr...  \n",
       "1   {'classifier__C': [0.01, 0.1, 1, 10], 'classif...  \n",
       "2   {'classifier__loss': ['hinge', 'log', 'squared...  \n",
       "3                      {'classifier': [GaussianNB()]}  \n",
       "4   {'classifier__alpha': [1, 10], 'classifier': [...  \n",
       "5   {'classifier__n_neighbors': [3, 10, 100, 1000]...  \n",
       "7   {'classifier__max_depth': [3, 10, None], 'clas...  \n",
       "8   {'classifier__n_estimators': [31, 100], 'class...  \n",
       "11  {'classifier__booster': ['dart'], 'classifier_...  \n",
       "12  {'classifier__iterations': [1000], 'classifier...  \n",
       "13  {'classifier__boosting_type': ['dart'], 'class...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    DummyClassifier                  2         \n",
      "1    LogisticRegression               4         \n",
      "2    SGDClassifier                    4         \n",
      "3    GaussianNB                       1         \n",
      "4    MultinomialNB                    2         \n",
      "5    KNeighborsClassifier             8         \n",
      "6    DecisionTreeClassifier           12        \n",
      "7    RandomForestClassifier           12        \n",
      "8    XGBClassifier                    4         \n",
      "9    <catboost.core.CatBoostClassifier object at 0x0000018EBD209D60>] 4         \n",
      "10   LGBMClassifier                   9         \n"
     ]
    }
   ],
   "source": [
    "nl = []\n",
    "for j, k in enumerate(dA.params):\n",
    "    nl.append(math.prod([len(i) for i in k.values()]))\n",
    "    print(\"{:<4}\".format(j),\n",
    "          \"{:<32}\".format(str(k[\"classifier\"]).split('(')[0][1:]),\n",
    "          \"{:<10.0f}\".format(nl[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.5ter. Déclaration des samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampler = {}\n",
    "param_sampler[\"sampler\"] = [\n",
    "    None,\n",
    "    # undersampler,\n",
    "    # oversampler_1,\n",
    "    oversampler_2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Itération sur les classifiers avec une boucle.\n",
    "Je vais gridsearch les paramètres de chacun des estimateurs et enregistrer le\n",
    "best_estimator_ obtenu à chaque fois pour pouvoir comparer les estimateurs.\n",
    "\n",
    "Le problème c'est qu'ici apparemment on veut l'inverse. On veut plutôt random\n",
    "entre tous les estimateurs pour trouver lequel est le meilleur et ne sortir que\n",
    "ses stats à lui. Puis recommencer avec une autre technique de sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# C'était juste pour constater qu'il y a pas mal de valeurs négatives\n",
    "# donc on ne peut pas utiliser MultinomialNB dans l'état\n",
    "# donc je vais rajouter un MinMaxScaler qui met tout entre 0 et 1.\n",
    "for co in df.columns:\n",
    "    neg = (df[co] < 0).sum()\n",
    "    if neg:\n",
    "        print(\"{:<40}\".format(co), \"{:>5.0f}\".format(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    for (n, clf, param) in list(zip(nz, clfs, params_full))[0:]:\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            print(clf, n_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "DummyClassifier() 2\n",
    "LogisticRegression(n_jobs=1, random_state=42, solver='sag') 5\n",
    "SGDClassifier() 12\n",
    "GaussianNB() 1\n",
    "MultinomialNB() 3\n",
    "KNeighborsClassifier() 8\n",
    "DecisionTreeClassifier(random_state=42) 8\n",
    "RandomForestClassifier(random_state=42) 18\n",
    "GradientBoostingClassifier(random_state=42) 9\n",
    "AdaBoostClassifier(random_state=42) 9\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...) 18\n",
    "catboost.core.CatBoostClassifier object at 0x000001E1F6F96910> 27\n",
    "LGBMClassifier(random_state=42) 27\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(d):\n",
    "    return d.loc[:, ~d.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(input_features=feature_names)\n",
    "scaler = MinMaxScaler()\n",
    "cws = [\n",
    "    None,\n",
    "    # {0: .8*oz, 1: zo/.8},\n",
    "    {0: oz, 1: zo},\n",
    "    # {0: oz/.8, 1: .8*zo},\n",
    "]\n",
    "cws1 = [\n",
    "    1,\n",
    "    oz,\n",
    "    zo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08781952865764357, 11.386988922457201)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oz, zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "result_dfs = dict()\n",
    "for sampler in param_sampler[\"sampler\"][0:]:\n",
    "    # I create a different results table for each sampler.\n",
    "    # str(sampler) includes parentheses and the arguments inside, so i strip\n",
    "    # these out in order to keep only the name of the sampler.\n",
    "    nom_du_df = \"result_df_\" + str(sampler).split('(')[0] + \".csv\"\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "        \"classifier\",\n",
    "        \"best_score\",\n",
    "        \"avg_score_folds\",\n",
    "        \"fold0_score\",\n",
    "        \"fold1_score\",\n",
    "        \"fold2_score\",\n",
    "        \"fold3_score\",\n",
    "        \"fold4_score\",\n",
    "        \"run_time (s)\",\n",
    "    ])\n",
    "    # I gridsearch each classifier with its own set of parameters.\n",
    "    for (n, clf, param) in list(zip(dA.nz, dA.clfs, dA.params))[0:]:\n",
    "        with timer(str(n)):\n",
    "            try:\n",
    "                result_df = pd.read_csv(nom_du_df)\n",
    "                result_df = clean_csv(result_df)\n",
    "            except ValueError:\n",
    "                print(\"An error occurred while reading the CSV file.\")\n",
    "            except FileNotFoundError:\n",
    "                print(\"The file could not be found.\")\n",
    "\n",
    "            t_clf = time.perf_counter()\n",
    "\n",
    "            # Without sampler i still try to optimize the gridsearch using class_weight.\n",
    "            if sampler is None:\n",
    "                # class_weight does not exist for dummy, NBayes, KN, GB, AdaB, XGB.\n",
    "                # CatBoost either.\n",
    "                if n in [1, 2, 6, 7, 8, 13]:\n",
    "                    param[\"classifier__class_weight\"] = cws\n",
    "                elif n in [12]:\n",
    "                    param[\"classifier__class_weights\"] = cws\n",
    "                elif n in [11]:\n",
    "                    param[\"classifier__scale_pos_weight\"] = cws1\n",
    "\n",
    "            # Train the model\n",
    "            # I need a MinMaxScaler in the pipe because (at least) MultinomialNB\n",
    "            # errors on negative values.\n",
    "            pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                                   (\"sampler\", sampler),\n",
    "                                                   (\"classifier\", clf)])\n",
    "            gs = GridSearchCV(\n",
    "                pipeline,\n",
    "                param,\n",
    "                cv=5,\n",
    "                n_jobs=None,\n",
    "                scoring=\"roc_auc\",\n",
    "                # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "                # refit=\"roc_auc\",\n",
    "                # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "                # random_state=42, # seulement pour RandomSearchCV ?\n",
    "            ).fit(X_train, y_train)\n",
    "\n",
    "            n_loops = math.prod([len(i) for i in param.values()])\n",
    "            t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "            if t_mean > 10:\n",
    "                t_mean = round(t_mean, 0)\n",
    "            elif t_mean > 1:\n",
    "                t_mean = round(t_mean, 2)\n",
    "            elif t_mean > .001:\n",
    "                t_mean = round(t_mean, 3)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Save the results\n",
    "            print(\"Score:\", gs.best_score_)\n",
    "            print(\"Obtained with:\", gs.best_params_[\"classifier\"])\n",
    "            result_df.loc[n, :] = [\n",
    "                gs.best_params_[\"classifier\"],\n",
    "                round(gs.best_score_, 3),\n",
    "                round(np.mean([\n",
    "                    gs.cv_results_[\"split0_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split1_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split2_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split3_test_score\"].max(),\n",
    "                    gs.cv_results_[\"split4_test_score\"].max(),\n",
    "                ]), 3),\n",
    "                round(gs.cv_results_[\"split0_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split1_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split2_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split3_test_score\"].max(), 3),\n",
    "                round(gs.cv_results_[\"split4_test_score\"].max(), 3),\n",
    "                t_mean\n",
    "            ]\n",
    "\n",
    "            # Je save à chaque classifier pour pouvoir relancer la loop à partir\n",
    "            # du classifier qui bug en cas d'erreur.\n",
    "            #result_df.sort_index(by=\"best_score\", inplace=True)\n",
    "            result_df.sort_values(by=\"best_score\", inplace=True)\n",
    "            result_df.to_csv(nom_du_df)\n",
    "\n",
    "    result_dfs[str(sampler)] = result_df\n",
    "    #display(result_df[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">The file could not be found.\n",
    "Score: 0.5\n",
    "Obtained with: DummyClassifier(strategy='most_frequent')\n",
    "0 - done in 1s\n",
    "Score: 0.7250222708935026\n",
    "Obtained with: LogisticRegression(C=0.1, n_jobs=1, random_state=42, solver='sag')\n",
    "1 - done in 97s\n",
    "Score: 0.7099938152168361\n",
    "Obtained with: SGDClassifier(loss='log', penalty='elasticnet')\n",
    "2 - done in 60s\n",
    "Score: 0.4930100602023389\n",
    "Obtained with: GaussianNB()\n",
    "3 - done in 1s\n",
    "Score: 0.6243430795680865\n",
    "Obtained with: MultinomialNB(alpha=1)\n",
    "4 - done in 1s\n",
    "Score: 0.6522858164197481\n",
    "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
    "5 - done in 10s\n",
    "Score: 0.6871594822939304\n",
    "Obtained with: DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=3,\n",
    "                       random_state=42)\n",
    "7 - done in 47s\n",
    "Score: 0.7107564160032682\n",
    "Obtained with: RandomForestClassifier(criterion='entropy', max_depth=10, random_state=42)\n",
    "8 - done in 97s\n",
    "Score: 0.7420321268277735\n",
    "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...)\n",
    "11 - done in 934s\n",
    "20 fits failed out of a total of 40.\n",
    "The score on these train-test partitions for these parameters will be set to nan.\n",
    "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
    "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.71175619 0.67402221\n",
    " 0.71084153 0.69628743]\n",
    "Score: 0.7117561947128614\n",
    "Obtained with: catboost.core.CatBoostClassifier object at 0x00000175D93E9DC0\n",
    "12 - done in 1463s\n",
    "Score: 0.745211332338472\n",
    "Obtained with: LGBMClassifier(boosting_type='dart', num_leaves=10, random_state=42)\n",
    "13 - done in 71s\n",
    "The file could not be found.\n",
    "Score: 0.5\n",
    "Obtained with: DummyClassifier(strategy='most_frequent')\n",
    "0 - done in 2s\n",
    "Score: 0.713460698256345\n",
    "Obtained with: LogisticRegression(C=0.01, n_jobs=1, random_state=42, solver='sag')\n",
    "1 - done in 157s\n",
    "Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
    "Score: 0.6917674294565902\n",
    "Obtained with: SGDClassifier(class_weight={0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "              loss='log', penalty='elasticnet')\n",
    "2 - done in 189s\n",
    "Score: 0.4993332917231714\n",
    "Obtained with: GaussianNB()\n",
    "3 - done in 2s\n",
    "Score: 0.6189208971907467\n",
    "Obtained with: MultinomialNB(alpha=10)\n",
    "4 - done in 2s\n",
    "Score: 0.6488529780582051\n",
    "Obtained with: KNeighborsClassifier(n_neighbors=1000, weights='distance')\n",
    "5 - done in 19s\n",
    "Score: 0.6381124495713207\n",
    "Obtained with: DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\n",
    "7 - done in 226s\n",
    "Score: 0.6855991012205017\n",
    "Obtained with: RandomForestClassifier(random_state=42)\n",
    "8 - done in 348s\n",
    "Score: 0.7173236343828551\n",
    "Obtained with: XGBClassifier(base_score=None, booster='dart', callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              predictor=None, random_state=42, ...)\n",
    "11 - done in 1717s\n",
    "20 fits failed out of a total of 40.\n",
    "The score on these train-test partitions for these parameters will be set to nan.\n",
    "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
    "One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.66966222 0.66499016\n",
    " 0.66001963 0.66197607]\n",
    "Score: 0.6696622200534501\n",
    "Obtained with: catboost.core.CatBoostClassifier object at 0x00000175D93E9DC0\n",
    "12 - done in 4823s\n",
    "Score: 0.7069649737004864\n",
    "Obtained with: LGBMClassifier(boosting_type='dart', random_state=42)\n",
    "13 - done in 194s\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">{'mean_fit_time': array([1.12846975, 1.93213859, 3.49252276, 1.20664921, 1.90970011,\n",
    "        3.44738073, 1.19674945, 1.9298975 , 3.48896518, 1.01345282,\n",
    "        1.63482342, 3.1525496 , 1.09063888, 1.71527624, 3.28194938,\n",
    "        1.06873031, 1.62983508, 3.28338637]),\n",
    " 'std_fit_time': array([0.16328593, 0.02055966, 0.06306082, 0.01070929, 0.07148658,\n",
    "        0.11888143, 0.05199477, 0.06873583, 0.08543938, 0.02194296,\n",
    "        0.03327168, 0.13876321, 0.05604037, 0.03026382, 0.19121445,\n",
    "        0.05841236, 0.01412593, 0.19140034]),\n",
    " 'mean_score_time': array([0.0305635 , 0.0313375 , 0.0289206 , 0.02932396, 0.03036981,\n",
    "        0.03311548, 0.03042021, 0.03111563, 0.03199601, 0.02480702,\n",
    "        0.03254519, 0.03337388, 0.02807384, 0.02978086, 0.03095374,\n",
    "        0.02895303, 0.03263464, 0.03402157]),\n",
    " 'std_score_time': array([0.00258001, 0.00388554, 0.00182127, 0.0054116 , 0.00370879,\n",
    "        0.00366714, 0.00742412, 0.00523109, 0.0009727 , 0.00519239,\n",
    "        0.00427954, 0.00293639, 0.00158762, 0.00664471, 0.00373306,\n",
    "        0.00939595, 0.00483121, 0.00494056]),\n",
    " 'param_classifier': masked_array(data=[LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "                    LGBMClassifier(boosting_type='dart', random_state=42)],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_classifier__boosting_type': masked_array(data=['dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
    "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
    "                    'dart', 'dart', 'dart', 'dart'],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_classifier__class_weight': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "                    {0: 0.08781952865764357, 1: 11.386988922457201}],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_classifier__learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.03162277660168379,\n",
    "                    0.03162277660168379, 0.03162277660168379, 0.1, 0.1,\n",
    "                    0.1, 0.01, 0.01, 0.01, 0.03162277660168379,\n",
    "                    0.03162277660168379, 0.03162277660168379, 0.1, 0.1,\n",
    "                    0.1],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_classifier__num_leaves': masked_array(data=[10, 31, 100, 10, 31, 100, 10, 31, 100, 10, 31, 100, 10,\n",
    "                    31, 100, 10, 31, 100],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'params': [{'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 100},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 100},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': None,\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 100},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.01,\n",
    "   'classifier__num_leaves': 100},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.03162277660168379,\n",
    "   'classifier__num_leaves': 100},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 10},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 31},\n",
    "  {'classifier': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "   'classifier__boosting_type': 'dart',\n",
    "   'classifier__class_weight': {0: 0.08781952865764357, 1: 11.386988922457201},\n",
    "   'classifier__learning_rate': 0.1,\n",
    "   'classifier__num_leaves': 100}],\n",
    " 'split0_test_score': array([0.64817095, 0.67550031, 0.64628714, 0.6726434 , 0.69122044,\n",
    "        0.68749539, 0.70733493, 0.71905764, 0.69418801, 0.60924937,\n",
    "        0.59878631, 0.59178729, 0.62355097, 0.63450485, 0.6273583 ,\n",
    "        0.65516429, 0.6262859 , 0.64346711]),\n",
    " 'split1_test_score': array([0.65688071, 0.65286344, 0.60819399, 0.65186196, 0.66716221,\n",
    "        0.63343244, 0.67547478, 0.69234392, 0.67710893, 0.64053643,\n",
    "        0.63311469, 0.64089106, 0.6497058 , 0.65401245, 0.62346586,\n",
    "        0.65688639, 0.65229603, 0.63346932]),\n",
    " 'split2_test_score': array([0.65185061, 0.6361702 , 0.62084726, 0.68161133, 0.64918945,\n",
    "        0.65259108, 0.68548392, 0.67246183, 0.65605797, 0.61199848,\n",
    "        0.60011972, 0.59785292, 0.62040184, 0.64187552, 0.60677262,\n",
    "        0.62763633, 0.60400933, 0.60970046]),\n",
    " 'split3_test_score': array([0.70101964, 0.70594193, 0.66230517, 0.7343664 , 0.7193839 ,\n",
    "        0.72008749, 0.73396921, 0.73971141, 0.72984981, 0.66129517,\n",
    "        0.63344379, 0.66828284, 0.66985457, 0.66448119, 0.65616577,\n",
    "        0.65703959, 0.62347154, 0.66181719]),\n",
    " 'split4_test_score': array([0.67793167, 0.65333156, 0.6414528 , 0.67950056, 0.68006514,\n",
    "        0.64267557, 0.67786926, 0.71125006, 0.6892799 , 0.62200761,\n",
    "        0.60928342, 0.60181912, 0.6276108 , 0.61687254, 0.63681138,\n",
    "        0.61947412, 0.65088318, 0.59407396]),\n",
    " 'mean_test_score': array([0.66717072, 0.66476149, 0.63581727, 0.68399673, 0.68140423,\n",
    "        0.6672564 , 0.69602642, 0.70696497, 0.68929692, 0.62901741,\n",
    "        0.61494959, 0.62012665, 0.6382248 , 0.64234931, 0.63011479,\n",
    "        0.64324015, 0.63138919, 0.62850561]),\n",
    " 'std_test_score': array([0.01981479, 0.02408659, 0.01913182, 0.02729304, 0.02358489,\n",
    "        0.03213749, 0.02204874, 0.02298333, 0.02417368, 0.0195146 ,\n",
    "        0.01539668, 0.02961278, 0.01885679, 0.01635038, 0.01624907,\n",
    "        0.01629192, 0.01819616, 0.02407205]),\n",
    " 'rank_test_score': array([ 7,  8, 12,  4,  5,  6,  2,  1,  3, 15, 18, 17, 11, 10, 14,  9, 13,\n",
    "        16])}\n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# Ce calcul date de la version 7 mais si je le refais ici en mettant à jour les\n",
    "# valeurs la conclusion sera identique.\n",
    "t_check = pd.DataFrame(columns=[\"clf\", \"t_tot\", \"rt_x_nl\", \"run_time\", \"n_loops\"])\n",
    "t_check.clf = dA.clf\n",
    "t_check.t_tot = [2, 153, 160, 2, 2, 18, 222, 346, 1397, 4513, 127]\n",
    "#t_check.run_time = result_df[\"run_time (s)\"] # non car il est sorted par score.\n",
    "t_check.run_time = [1, 19, 20, 1.6, 1.1, 2.3, 9.2, 14, 116, 564, 7]\n",
    "t_check.n_loops = nl\n",
    "t_check.rt_x_nl = t_check.run_time*t_check.n_loops\n",
    "t_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ```t_tot``` environ identique à ```rt_x_nl``` donc les temps sont\n",
    "correctment évalués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_N = clean_csv(pd.read_csv(\"result_df_None.csv\"))\n",
    "#res_U = clean_csv(pd.read_csv(\"result_df_RandomUnderSampler.csv\")).sort_values(by=\"score\")\n",
    "#res_O = clean_csv(pd.read_csv(\"result_df_RandomOverSampler.csv\")).sort_values(by=\"score\")\n",
    "res_S = clean_csv(pd.read_csv(\"result_df_SMOTE.csv\"))\n",
    "#res = [res_N, res_U, res_O, res_S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>avg_score_folds</th>\n",
       "      <th>fold0_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB(alpha=1)</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.642</td>\n",
       "      <td>1.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGDClassifier(loss='log', penalty='elasticnet')</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.691</td>\n",
       "      <td>7.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier(criterion='entropy', ma...</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.701</td>\n",
       "      <td>4.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, n_jobs=1, random_sta...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.718</td>\n",
       "      <td>12.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.739</td>\n",
       "      <td>78.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', num_leave...</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.735</td>\n",
       "      <td>3.920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "0                                        GaussianNB()       0.493   \n",
       "1           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "2                              MultinomialNB(alpha=1)       0.624   \n",
       "3   KNeighborsClassifier(n_neighbors=1000, weights...       0.652   \n",
       "4   DecisionTreeClassifier(criterion='entropy', ma...       0.687   \n",
       "5     SGDClassifier(loss='log', penalty='elasticnet')       0.710   \n",
       "6   RandomForestClassifier(criterion='entropy', ma...       0.711   \n",
       "7   <catboost.core.CatBoostClassifier object at 0x...       0.712   \n",
       "8   LogisticRegression(C=0.1, n_jobs=1, random_sta...       0.725   \n",
       "9   XGBClassifier(base_score=None, booster='dart',...       0.742   \n",
       "10  LGBMClassifier(boosting_type='dart', num_leave...       0.745   \n",
       "\n",
       "    avg_score_folds  fold0_score  fold1_score  fold2_score  fold3_score  \\\n",
       "0             0.493        0.493        0.496        0.492        0.503   \n",
       "1             0.500        0.500        0.500        0.500        0.500   \n",
       "2             0.624        0.624        0.619        0.620        0.662   \n",
       "3             0.653        0.674        0.631        0.638        0.681   \n",
       "4             0.688        0.691        0.683        0.690        0.705   \n",
       "5             0.710        0.715        0.691        0.704        0.749   \n",
       "6             0.714        0.731        0.704        0.699        0.735   \n",
       "7               NaN          NaN          NaN          NaN          NaN   \n",
       "8             0.725        0.739        0.702        0.706        0.760   \n",
       "9             0.745        0.747        0.754        0.728        0.758   \n",
       "10            0.746        0.739        0.755        0.744        0.758   \n",
       "\n",
       "    fold4_score  run_time (s)  \n",
       "0         0.481         0.723  \n",
       "1         0.500         0.302  \n",
       "2         0.596         0.401  \n",
       "3         0.642         1.300  \n",
       "4         0.669         1.950  \n",
       "5         0.691         7.530  \n",
       "6         0.701         4.030  \n",
       "7           NaN       183.000  \n",
       "8         0.718        12.000  \n",
       "9         0.739        78.000  \n",
       "10        0.735         3.920  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_score</th>\n",
       "      <th>avg_score_folds</th>\n",
       "      <th>fold0_score</th>\n",
       "      <th>fold1_score</th>\n",
       "      <th>fold2_score</th>\n",
       "      <th>fold3_score</th>\n",
       "      <th>fold4_score</th>\n",
       "      <th>run_time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.492</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DummyClassifier(strategy='most_frequent')</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB(alpha=10)</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.601</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=3, min_sample...</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.666</td>\n",
       "      <td>9.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1000, weights...</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.640</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0.670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.683</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGDClassifier(class_weight={0: 0.0878195286576...</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.681</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LGBMClassifier(boosting_type='dart', random_st...</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.711</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.01, n_jobs=1, random_st...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.708</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='dart',...</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.732</td>\n",
       "      <td>143.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  best_score  \\\n",
       "0                                        GaussianNB()       0.499   \n",
       "1           DummyClassifier(strategy='most_frequent')       0.500   \n",
       "2                             MultinomialNB(alpha=10)       0.619   \n",
       "3   DecisionTreeClassifier(max_depth=3, min_sample...       0.638   \n",
       "4   KNeighborsClassifier(n_neighbors=1000, weights...       0.649   \n",
       "5   <catboost.core.CatBoostClassifier object at 0x...       0.670   \n",
       "6             RandomForestClassifier(random_state=42)       0.686   \n",
       "7   SGDClassifier(class_weight={0: 0.0878195286576...       0.692   \n",
       "8   LGBMClassifier(boosting_type='dart', random_st...       0.707   \n",
       "9   LogisticRegression(C=0.01, n_jobs=1, random_st...       0.713   \n",
       "10  XGBClassifier(base_score=None, booster='dart',...       0.717   \n",
       "\n",
       "    avg_score_folds  fold0_score  fold1_score  fold2_score  fold3_score  \\\n",
       "0             0.499        0.498        0.503        0.498        0.505   \n",
       "1             0.500        0.500        0.500        0.500        0.500   \n",
       "2             0.619        0.630        0.599        0.610        0.656   \n",
       "3             0.645        0.656        0.626        0.621        0.658   \n",
       "4             0.649        0.671        0.614        0.627        0.695   \n",
       "5               NaN          NaN          NaN          NaN          NaN   \n",
       "6             0.692        0.686        0.687        0.677        0.724   \n",
       "7             0.695        0.698        0.676        0.683        0.739   \n",
       "8             0.710        0.719        0.692        0.685        0.740   \n",
       "9             0.715        0.728        0.691        0.690        0.757   \n",
       "10            0.729        0.726        0.739        0.705        0.742   \n",
       "\n",
       "    fold4_score  run_time (s)  \n",
       "0         0.492          1.67  \n",
       "1         0.500          1.07  \n",
       "2         0.601          1.18  \n",
       "3         0.666          9.41  \n",
       "4         0.640          2.42  \n",
       "5           NaN        603.00  \n",
       "6         0.683         15.00  \n",
       "7         0.681         24.00  \n",
       "8         0.711         11.00  \n",
       "9         0.708         20.00  \n",
       "10        0.732        143.00  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partons sur les résultats de SMOTE (parce que ça fait partie de l'exercice)\n",
    "même s'ils ne sont pas les meilleurs, ça me donnera une marge de progression\n",
    "pour le fine tuning, au moins.  \n",
    "Je vais utiliser LGBM car il compute un peu plus vite que la LogisticReg et\n",
    "beaucoup plus vite que XGB ou CatB, et que c'est l'algo choisi par les\n",
    "meilleures équipes de la compétition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fine tuning with the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "    \"rf\",\n",
    "    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    10**1,\n",
    "    10**1.5,\n",
    "    10**2,\n",
    "]\n",
    "parm[\"classifier__num_iterations\"] = [\n",
    "    10**3,\n",
    "    10**4,\n",
    "]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "#parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est trop à mon avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "#    \"rf\",\n",
    "#    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "#parm[\"classifier__num_leaves\"] = [\n",
    "#    10**1,\n",
    "#    10**1.5,\n",
    "#    10**2,\n",
    "#]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "#parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok testons ça."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier fine tuning a donné:\n",
    "```\n",
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "#    \"rf\",\n",
    "#    \"dart\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "    ? pas vu dans l'output, peut-être parce que c'est la valeur par\n",
    "    défaut et donc best_estimator_ pense que ce n'est pas nécessaire\n",
    "    de préciser que le meilleur learning_rate trouvé est .1.\n",
    "#parm[\"classifier__num_leaves\"] = [\n",
    "#    10**1,\n",
    "#    10**1.5,\n",
    "#    10**2,\n",
    "#]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .8,\n",
    "    .9,     <------------------------------------------------\n",
    "    .95,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,     <------------------------------------------------\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,     <------------------------------------------------\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    6,\n",
    "    8,     <------------------------------------------------\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .8,\n",
    "    .9,     <------------------------------------------------\n",
    "    .95,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]\n",
    "```\n",
    "\n",
    "Je teste maintenant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,\n",
    "    10**-1,\n",
    "    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,\n",
    "    30,\n",
    "    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "#parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je trouve:\n",
    "```\n",
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,     <-------------------------------------- sans SMOTE\n",
    "    10**-1,     <-------------------------------------- avec SMOTE (car\n",
    "                                                        rien d'indiqué)\n",
    "    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,     <-------------------------------------- avec SMOTE\n",
    "    30,     <-------------------------------------- sans SMOTE\n",
    "    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,     <-------------------------------------- sans SMOTE\n",
    "    8,     <-------------------------------------- avec SMOTE\n",
    "    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]\n",
    "```\n",
    "Choisissons une seule valeur par paramètre pour pouvoir run plus rapidement\n",
    "pendant le debug des étapes suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,\n",
    "#    10**-1,\n",
    "#    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,\n",
    "#    30,\n",
    "#    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,\n",
    "#    8,\n",
    "#    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "#parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Sans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "Score: 0.752\n",
      "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9, lambda_l1=0.01,\n",
      "               lambda_l2=0.03162277660168379, learning_rate=0.07943282347242814,\n",
      "               max_depth=7, min_gain_to_split=0.02, min_sum_hessian_in_leaf=1,\n",
      "               num_leaves=25, random_state=42, verbose=-1)\n",
      "run_time per search (s) 19.0\n",
      "Fine tuning with class_weight and LGBM - done in 39s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Fine tuning with class_weight and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    parm[\"classifier__class_weight\"] = cws\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", None),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    gs_N = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_N.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_N.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_N = gs_N.best_estimator_.predict(X_test)\n",
    "# On peut aussi faire un predict_proba avec un seuil < .5 au lieu de faire un\n",
    "# oversampling ou de jouer sur class_weights.\n",
    "y_pred_proba_N = gs_N.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive_N = y_pred_proba_N[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589120796748671"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_N, tpr_N, thresholds_N = roc_curve(y_test, y_pred_proba_positive_N)\n",
    "roc_auc_N = auc(fpr_N, tpr_N)\n",
    "roc_auc_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr_N, tpr_N, label='ROC curve (area = %0.2f)' % roc_auc_N)\n",
    "ax.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Avec SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03162277660168379, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03162277660168379\n",
      "Score: 0.735\n",
      "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9, lambda_l1=0.01,\n",
      "               lambda_l2=0.03162277660168379, learning_rate=0.07943282347242814,\n",
      "               max_depth=7, min_gain_to_split=0.02, min_sum_hessian_in_leaf=1,\n",
      "               num_leaves=25, random_state=42, verbose=-1)\n",
      "run_time per search (s) 64.0\n",
      "Fine tuning with SMOTE and LGBM - done in 128s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    gs_S = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=\"roc_auc\",\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_S.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_S.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = rs_cv.best_estimator_[best_algorithm].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_S = gs_S.best_estimator_.predict(X_test)\n",
    "# On peut aussi faire un predict_proba avec un seuil < .5 au lieu de faire un\n",
    "# oversampling ou de jouer sur class_weights.\n",
    "y_pred_proba_S = gs_S.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive_S = y_pred_proba_S[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7424031012767514"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_S, tpr_S, thresholds_S = roc_curve(y_test, y_pred_proba_positive_S)\n",
    "roc_auc_S = auc(fpr_S, tpr_S)\n",
    "roc_auc_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr_N, tpr_N, label='ROC curve (area = %0.2f)' % roc_auc_N)\n",
    "ax.plot(fpr_S, tpr_S, label='ROC curve (area = %0.2f)' % roc_auc_S)\n",
    "ax.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Fine tuning using an Fbeta scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    91904\n",
       "1.0     8093\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TARGET.value_counts(\n",
    "    sort=True,\n",
    "    ascending=False,\n",
    "    dropna=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    18386\n",
       "1.0     1614\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(\n",
    "    sort=True,\n",
    "    ascending=False,\n",
    "    dropna=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"Prédiction des données\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>18362</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>1597</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive  Negative\n",
       "Positive     18362        24\n",
       "Negative      1597        17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion                 Predicted\n",
    "    Matrix               Positive    Negative\n",
    "              Positive      TP          FN     →  Recall (= Sensitivity, TPR)\n",
    "    Actual    Negative      FP          TN     →  FP Rate (= 1-Specificity)\n",
    "                            ↓           ↓\n",
    "                        Precision  False Omission Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           1812          8\n",
    "                            171      \t9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les \"négatifs\" sont les zéros et correspondent aux prêts remboursés.\n",
    "- Les \"positifs\" sont les uns et correspondent aux prêts non-remboursés.\n",
    "\n",
    "Donc là le modèle a accordé 171 prêts qui ne seront pas remboursés et a refusé\n",
    "8 prêts qui auraient été remboursés.  \n",
    "C'est très problématique.  \n",
    "- Un FN (prêt accordé non-remboursé) nous fait perdre la part non-remboursée du\n",
    "prêt. 100% dans le pire des cas.\n",
    "- Un FP (prêt remboursable non-accordé) nous fait perdre seulement le taux\n",
    "d'intérêts qu'on aurait touchés sur ce prêt, disons 10%.\n",
    "\n",
    "Considérons donc qu'un FN nous est 10fois plus coûteux qu'un FP et qu'on veut\n",
    "donc minimiser le nombre (10FN+FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je veux donc un $\\beta$ qui beaucoup plus d'importance au fait d'avoir un Recall\n",
    "proche de 1 qu'au fait d'avoir une Precision proche de 1.  \n",
    "Dans ce cas il faut choisir $\\beta = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm = {}\n",
    "parm[\"classifier__boosting_type\"] = [\n",
    "    \"gbdt\",\n",
    "]\n",
    "parm[\"classifier__learning_rate\"] = [\n",
    "    10**-1.1,\n",
    "    10**-1,\n",
    "    10**-.9\n",
    "]\n",
    "parm[\"classifier__num_leaves\"] = [\n",
    "    25,\n",
    "    30,\n",
    "    35,\n",
    "]\n",
    "#parm[\"classifier__num_iterations\"] = [\n",
    "#    10**3,\n",
    "#    10**4,\n",
    "#]\n",
    "parm[\"classifier__feature_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "parm[\"classifier__lambda_l1\"] = [\n",
    "    10**-2.5,\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "]\n",
    "parm[\"classifier__lambda_l2\"] = [\n",
    "    10**-2,\n",
    "    10**-1.5,\n",
    "    10**-1,\n",
    "]\n",
    "parm[\"classifier__min_gain_to_split\"] = [.02]\n",
    "parm[\"classifier__max_depth\"] = [\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "]\n",
    "parm[\"classifier__min_sum_hessian_in_leaf\"] = [1]\n",
    "parm[\"classifier__bagging_fraction\"] = [\n",
    "    .9,\n",
    "]\n",
    "#parm[\"classifier__silent\"] = [-1]\n",
    "#parm[\"classifier__verbose\"] = [-1]\n",
    "parm[\"classifier\"] = [clf13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.prod([len(i) for i in parm.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    fbeta_scorer = make_scorer(fbeta_score, beta=.5)\n",
    "    gs_Sb = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=fbeta_scorer,\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_Sb.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_Sb.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "y_pred_Sb = gs_Sb.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm = confusion_matrix(y_test, y_pred_Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              Positive \tNegative\n",
    "    Positive \t1808 \t12\n",
    "    Negative \t172 \t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta = .1\n",
    "#fb_score = fbeta_score(y_test, y_pred_S, beta=beta)\n",
    "#fb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta = .01\n",
    "#fb_score = fbeta_score(y_test, y_pred_S, beta=beta)\n",
    "#fb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    fbeta_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    gs_Sb = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=fbeta_scorer,\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_Sb.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_Sb.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "y_pred_Sb = gs_Sb.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm = confusion_matrix(y_test, y_pred_Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t           Positive \tNegative\n",
    "    Positive \t1809 \t11\n",
    "    Negative \t172 \t8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t           Positive \tNegative\n",
    "    Positive \t18328 \t58\n",
    "    Negative \t1567 \t47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça s'améliore mais c'est tout de même extrêmement mauvais.\n",
    "Je vais essayer de créer ma fonction de scoring à la main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt = [0, 1, 1, 0, 0]\n",
    "yp = [1, 0, 1, 0, 0]\n",
    "a = yt == yp\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics._classification._weighted_sum(a, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt = np.array(yt)\n",
    "yp = np.array(yp)\n",
    "a = yt == yp\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics._classification._weighted_sum(a, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "def my_scoring(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    # multioutput=\"uniform_average\",\n",
    "    normalize=True,\n",
    "):\n",
    "\n",
    "    y_type, y_true, y_pred = sklearn.metrics._classification._check_targets(\n",
    "        y_true, y_pred\n",
    "    )\n",
    "    sklearn.metrics._classification.check_consistent_length(\n",
    "        y_true, y_pred, sample_weight\n",
    "    )\n",
    "    FN = y_true - y_pred == 1\n",
    "    FN = sklearn.metrics._classification._weighted_sum(\n",
    "        FN, sample_weight, normalize\n",
    "    )\n",
    "    FP = y_true - y_pred == -1\n",
    "    FP = sklearn.metrics._classification._weighted_sum(\n",
    "        FP, sample_weight, normalize\n",
    "    )\n",
    "    # score = y_true == y_pred\n",
    "    # score = sklearn.metrics._classification._weighted_sum(\n",
    "    #     score, sample_weight, normalize\n",
    "    # )\n",
    "    score = 1 - (10*FN+FP)/11\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    gs_Sb = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=my_scoring,\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_Sb.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_Sb.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9,\n",
    "               lambda_l1=0.0031622776601683794, lambda_l2=0.01,\n",
    "               learning_rate=0.07943282347242814, max_depth=7,\n",
    "               min_gain_to_split=0.02, min_sum_hessian_in_leaf=1, num_leaves=25,\n",
    "               random_state=42, verbose=-1)\n",
    "run_time per search (s) 55.0\n",
    "Fine tuning with SMOTE and LGBM - done in 13393s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "y_pred_Sb = gs_Sb.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm = confusion_matrix(y_test, y_pred_Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t           Positive \tNegative\n",
    "    Positive \t18372 \t14\n",
    "    Negative \t1598 \t16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "y_pred_proba_Sb = gs_Sb.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive_Sb = y_pred_proba_Sb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "fpr_Sb, tpr_Sb, thresholds_Sb = roc_curve(y_test, y_pred_proba_positive_Sb)\n",
    "roc_auc_Sb = auc(fpr_Sb, tpr_Sb)\n",
    "roc_auc_Sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "def my_scoring(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    # multioutput=\"uniform_average\",\n",
    "    normalize=True,\n",
    "):\n",
    "\n",
    "    y_type, y_true, y_pred = sklearn.metrics._classification._check_targets(\n",
    "        y_true, y_pred\n",
    "    )\n",
    "    sklearn.metrics._classification.check_consistent_length(\n",
    "        y_true, y_pred, sample_weight\n",
    "    )\n",
    "    FN = y_true - y_pred == 1\n",
    "    FN = sklearn.metrics._classification._weighted_sum(\n",
    "        FN, sample_weight, normalize\n",
    "    )\n",
    "    FP = y_true - y_pred == -1\n",
    "    FP = sklearn.metrics._classification._weighted_sum(\n",
    "        FP, sample_weight, normalize\n",
    "    )\n",
    "    # score = y_true == y_pred\n",
    "    # score = sklearn.metrics._classification._weighted_sum(\n",
    "    #     score, sample_weight, normalize\n",
    "    # )\n",
    "    score = 1 - (FN+10*FP)/11\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031622776601683794, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031622776601683794\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Score: nan\n",
      "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9,\n",
      "               lambda_l1=0.0031622776601683794, lambda_l2=0.01,\n",
      "               learning_rate=0.07943282347242814, max_depth=7,\n",
      "               min_gain_to_split=0.02, min_sum_hessian_in_leaf=1, num_leaves=25,\n",
      "               random_state=42, verbose=-1)\n",
      "run_time per search (s) 56.0\n",
      "Fine tuning with SMOTE and LGBM - done in 13620s\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "with timer(\"Fine tuning with SMOTE and LGBM\"):\n",
    "    t_clf = time.perf_counter()\n",
    "    pipeline = imblearn.pipeline.Pipeline([(\"scaler\", scaler),\n",
    "                                           (\"sampler\", oversampler_2),\n",
    "                                           (\"classifier\", clf13)])\n",
    "    # verbose=-1 ne fonctionne pas donc je supprime tous les warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    gs_Sb = GridSearchCV(\n",
    "        pipeline,\n",
    "        parm,\n",
    "        cv=5,\n",
    "        n_jobs=None,\n",
    "        scoring=my_scoring,\n",
    "        # scoring=[\"roc_auc\", \"accuracy\"],\n",
    "        # refit=\"roc_auc\",\n",
    "        # n_iter=5, # seulement pour RandomSearchCV ?\n",
    "        # random_state=42, # seulement pour RandomSearchCV ?\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    n_loops = math.prod([len(i) for i in parm.values()])\n",
    "    t_mean = (time.perf_counter() - t_clf)/(n_loops)\n",
    "    if t_mean > 10:\n",
    "        t_mean = round(t_mean, 0)\n",
    "    elif t_mean > 1:\n",
    "        t_mean = round(t_mean, 2)\n",
    "    elif t_mean > .001:\n",
    "        t_mean = round(t_mean, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Score:\", round(gs_Sb.best_score_, 3))\n",
    "    print(\"Obtained with:\", gs_Sb.best_params_[\"classifier\"])\n",
    "    print(\"run_time per search (s)\", t_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtained with: LGBMClassifier(bagging_fraction=0.9, feature_fraction=0.9,\n",
    "               lambda_l1=0.0031622776601683794, lambda_l2=0.01,\n",
    "               learning_rate=0.07943282347242814, max_depth=7,\n",
    "               min_gain_to_split=0.02, min_sum_hessian_in_leaf=1, num_leaves=25,\n",
    "               random_state=42, verbose=-1)\n",
    "run_time per search (s) 56.0\n",
    "Fine tuning with SMOTE and LGBM - done in 13620s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "y_pred_Sb = gs_Sb.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "cm = confusion_matrix(y_test, y_pred_Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>18364</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>1598</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive  Negative\n",
       "Positive     18364        22\n",
       "Negative      1598        16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%script echo\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t            \tPositive \tNegative\n",
    "    Positive \t18364 \t22\n",
    "    Negative \t1598 \t16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "y_pred_proba_Sb = gs_Sb.best_estimator_.predict_proba(X_test)\n",
    "y_pred_proba_positive_Sb = y_pred_proba_Sb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7386866738080304"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo\n",
    "fpr_Sb, tpr_Sb, thresholds_Sb = roc_curve(y_test, y_pred_proba_positive_Sb)\n",
    "roc_auc_Sb = auc(fpr_Sb, tpr_Sb)\n",
    "roc_auc_Sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(gs_Sb.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (870885351.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [132]\u001b[1;36m\u001b[0m\n\u001b[1;33m    =r\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "=r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=-1 will be ignored. Current value: num_threads=-1\n",
      "[1]\ttraining's auc: 0.712757\ttraining's binary_logloss: 0.280684\tvalid_1's auc: 0.685659\tvalid_1's binary_logloss: 0.280049\n",
      "[2]\ttraining's auc: 0.742221\ttraining's binary_logloss: 0.280074\tvalid_1's auc: 0.714427\tvalid_1's binary_logloss: 0.279515\n",
      "[3]\ttraining's auc: 0.745765\ttraining's binary_logloss: 0.279506\tvalid_1's auc: 0.720094\tvalid_1's binary_logloss: 0.279011\n",
      "[4]\ttraining's auc: 0.745397\ttraining's binary_logloss: 0.279078\tvalid_1's auc: 0.719608\tvalid_1's binary_logloss: 0.278637\n",
      "[5]\ttraining's auc: 0.747396\ttraining's binary_logloss: 0.278503\tvalid_1's auc: 0.721718\tvalid_1's binary_logloss: 0.278129\n",
      "[6]\ttraining's auc: 0.747921\ttraining's binary_logloss: 0.278061\tvalid_1's auc: 0.721898\tvalid_1's binary_logloss: 0.27775\n",
      "[7]\ttraining's auc: 0.757719\ttraining's binary_logloss: 0.277742\tvalid_1's auc: 0.728349\tvalid_1's binary_logloss: 0.277478\n",
      "[8]\ttraining's auc: 0.762694\ttraining's binary_logloss: 0.277293\tvalid_1's auc: 0.732725\tvalid_1's binary_logloss: 0.277115\n",
      "[9]\ttraining's auc: 0.765512\ttraining's binary_logloss: 0.276966\tvalid_1's auc: 0.734767\tvalid_1's binary_logloss: 0.276846\n",
      "[10]\ttraining's auc: 0.764869\ttraining's binary_logloss: 0.276431\tvalid_1's auc: 0.734783\tvalid_1's binary_logloss: 0.276375\n",
      "[11]\ttraining's auc: 0.76664\ttraining's binary_logloss: 0.276167\tvalid_1's auc: 0.734945\tvalid_1's binary_logloss: 0.276192\n",
      "[12]\ttraining's auc: 0.767667\ttraining's binary_logloss: 0.275744\tvalid_1's auc: 0.7366\tvalid_1's binary_logloss: 0.275836\n",
      "[13]\ttraining's auc: 0.767765\ttraining's binary_logloss: 0.275251\tvalid_1's auc: 0.737416\tvalid_1's binary_logloss: 0.275397\n",
      "[14]\ttraining's auc: 0.767025\ttraining's binary_logloss: 0.274849\tvalid_1's auc: 0.736962\tvalid_1's binary_logloss: 0.275042\n",
      "[15]\ttraining's auc: 0.768186\ttraining's binary_logloss: 0.27446\tvalid_1's auc: 0.738709\tvalid_1's binary_logloss: 0.274718\n",
      "[16]\ttraining's auc: 0.768643\ttraining's binary_logloss: 0.274082\tvalid_1's auc: 0.739245\tvalid_1's binary_logloss: 0.274423\n",
      "[17]\ttraining's auc: 0.768839\ttraining's binary_logloss: 0.273722\tvalid_1's auc: 0.739355\tvalid_1's binary_logloss: 0.274135\n",
      "[18]\ttraining's auc: 0.769352\ttraining's binary_logloss: 0.273491\tvalid_1's auc: 0.739512\tvalid_1's binary_logloss: 0.273967\n",
      "[19]\ttraining's auc: 0.769253\ttraining's binary_logloss: 0.273134\tvalid_1's auc: 0.739316\tvalid_1's binary_logloss: 0.273678\n",
      "[20]\ttraining's auc: 0.770711\ttraining's binary_logloss: 0.272762\tvalid_1's auc: 0.740358\tvalid_1's binary_logloss: 0.273356\n",
      "[21]\ttraining's auc: 0.770676\ttraining's binary_logloss: 0.272305\tvalid_1's auc: 0.740766\tvalid_1's binary_logloss: 0.272959\n",
      "[22]\ttraining's auc: 0.77067\ttraining's binary_logloss: 0.271856\tvalid_1's auc: 0.741265\tvalid_1's binary_logloss: 0.272562\n",
      "[23]\ttraining's auc: 0.770908\ttraining's binary_logloss: 0.271412\tvalid_1's auc: 0.741586\tvalid_1's binary_logloss: 0.272172\n",
      "[24]\ttraining's auc: 0.770816\ttraining's binary_logloss: 0.270988\tvalid_1's auc: 0.741687\tvalid_1's binary_logloss: 0.2718\n",
      "[25]\ttraining's auc: 0.77111\ttraining's binary_logloss: 0.270625\tvalid_1's auc: 0.741751\tvalid_1's binary_logloss: 0.271497\n",
      "[26]\ttraining's auc: 0.771694\ttraining's binary_logloss: 0.270301\tvalid_1's auc: 0.742262\tvalid_1's binary_logloss: 0.271224\n",
      "[27]\ttraining's auc: 0.771702\ttraining's binary_logloss: 0.269886\tvalid_1's auc: 0.742386\tvalid_1's binary_logloss: 0.270856\n",
      "[28]\ttraining's auc: 0.772513\ttraining's binary_logloss: 0.269647\tvalid_1's auc: 0.742402\tvalid_1's binary_logloss: 0.270688\n",
      "[29]\ttraining's auc: 0.77259\ttraining's binary_logloss: 0.269351\tvalid_1's auc: 0.742663\tvalid_1's binary_logloss: 0.270445\n",
      "[30]\ttraining's auc: 0.772617\ttraining's binary_logloss: 0.268949\tvalid_1's auc: 0.742726\tvalid_1's binary_logloss: 0.270099\n",
      "[31]\ttraining's auc: 0.772494\ttraining's binary_logloss: 0.268578\tvalid_1's auc: 0.742719\tvalid_1's binary_logloss: 0.269784\n",
      "[32]\ttraining's auc: 0.772589\ttraining's binary_logloss: 0.268279\tvalid_1's auc: 0.742751\tvalid_1's binary_logloss: 0.269543\n",
      "[33]\ttraining's auc: 0.772758\ttraining's binary_logloss: 0.267987\tvalid_1's auc: 0.74302\tvalid_1's binary_logloss: 0.269307\n",
      "[34]\ttraining's auc: 0.773762\ttraining's binary_logloss: 0.267761\tvalid_1's auc: 0.743438\tvalid_1's binary_logloss: 0.269153\n",
      "[35]\ttraining's auc: 0.773734\ttraining's binary_logloss: 0.267405\tvalid_1's auc: 0.743581\tvalid_1's binary_logloss: 0.268837\n",
      "[36]\ttraining's auc: 0.774477\ttraining's binary_logloss: 0.267195\tvalid_1's auc: 0.744051\tvalid_1's binary_logloss: 0.268666\n",
      "[37]\ttraining's auc: 0.774294\ttraining's binary_logloss: 0.266832\tvalid_1's auc: 0.743825\tvalid_1's binary_logloss: 0.268366\n",
      "[38]\ttraining's auc: 0.774719\ttraining's binary_logloss: 0.266514\tvalid_1's auc: 0.743858\tvalid_1's binary_logloss: 0.268117\n",
      "[39]\ttraining's auc: 0.775076\ttraining's binary_logloss: 0.266328\tvalid_1's auc: 0.743939\tvalid_1's binary_logloss: 0.267985\n",
      "[40]\ttraining's auc: 0.774849\ttraining's binary_logloss: 0.265987\tvalid_1's auc: 0.743788\tvalid_1's binary_logloss: 0.2677\n",
      "[41]\ttraining's auc: 0.775046\ttraining's binary_logloss: 0.265695\tvalid_1's auc: 0.743721\tvalid_1's binary_logloss: 0.267468\n",
      "[42]\ttraining's auc: 0.775152\ttraining's binary_logloss: 0.265424\tvalid_1's auc: 0.743395\tvalid_1's binary_logloss: 0.267266\n",
      "[43]\ttraining's auc: 0.775267\ttraining's binary_logloss: 0.265172\tvalid_1's auc: 0.743469\tvalid_1's binary_logloss: 0.26706\n",
      "[44]\ttraining's auc: 0.775396\ttraining's binary_logloss: 0.26489\tvalid_1's auc: 0.743653\tvalid_1's binary_logloss: 0.266821\n",
      "[45]\ttraining's auc: 0.775826\ttraining's binary_logloss: 0.264688\tvalid_1's auc: 0.743903\tvalid_1's binary_logloss: 0.266665\n",
      "[46]\ttraining's auc: 0.776128\ttraining's binary_logloss: 0.26441\tvalid_1's auc: 0.744008\tvalid_1's binary_logloss: 0.266436\n",
      "[47]\ttraining's auc: 0.776714\ttraining's binary_logloss: 0.26419\tvalid_1's auc: 0.744267\tvalid_1's binary_logloss: 0.266264\n",
      "[48]\ttraining's auc: 0.776831\ttraining's binary_logloss: 0.263921\tvalid_1's auc: 0.744129\tvalid_1's binary_logloss: 0.266048\n",
      "[49]\ttraining's auc: 0.777306\ttraining's binary_logloss: 0.263732\tvalid_1's auc: 0.744252\tvalid_1's binary_logloss: 0.265922\n",
      "[50]\ttraining's auc: 0.777573\ttraining's binary_logloss: 0.263481\tvalid_1's auc: 0.74454\tvalid_1's binary_logloss: 0.26571\n",
      "[51]\ttraining's auc: 0.777656\ttraining's binary_logloss: 0.263176\tvalid_1's auc: 0.744743\tvalid_1's binary_logloss: 0.265448\n",
      "[52]\ttraining's auc: 0.777595\ttraining's binary_logloss: 0.26288\tvalid_1's auc: 0.744796\tvalid_1's binary_logloss: 0.265193\n",
      "[53]\ttraining's auc: 0.777746\ttraining's binary_logloss: 0.262639\tvalid_1's auc: 0.744847\tvalid_1's binary_logloss: 0.265003\n",
      "[54]\ttraining's auc: 0.777871\ttraining's binary_logloss: 0.262383\tvalid_1's auc: 0.744756\tvalid_1's binary_logloss: 0.264807\n",
      "[55]\ttraining's auc: 0.777948\ttraining's binary_logloss: 0.262149\tvalid_1's auc: 0.744776\tvalid_1's binary_logloss: 0.264627\n",
      "[56]\ttraining's auc: 0.777955\ttraining's binary_logloss: 0.261862\tvalid_1's auc: 0.744829\tvalid_1's binary_logloss: 0.264382\n",
      "[57]\ttraining's auc: 0.77804\ttraining's binary_logloss: 0.261578\tvalid_1's auc: 0.744961\tvalid_1's binary_logloss: 0.264138\n",
      "[58]\ttraining's auc: 0.778574\ttraining's binary_logloss: 0.261394\tvalid_1's auc: 0.745294\tvalid_1's binary_logloss: 0.264003\n",
      "[59]\ttraining's auc: 0.778701\ttraining's binary_logloss: 0.261151\tvalid_1's auc: 0.745304\tvalid_1's binary_logloss: 0.263812\n",
      "[60]\ttraining's auc: 0.778995\ttraining's binary_logloss: 0.260919\tvalid_1's auc: 0.745558\tvalid_1's binary_logloss: 0.263618\n",
      "[61]\ttraining's auc: 0.779198\ttraining's binary_logloss: 0.2607\tvalid_1's auc: 0.745702\tvalid_1's binary_logloss: 0.263448\n",
      "[62]\ttraining's auc: 0.779195\ttraining's binary_logloss: 0.260433\tvalid_1's auc: 0.745587\tvalid_1's binary_logloss: 0.263231\n",
      "[63]\ttraining's auc: 0.779168\ttraining's binary_logloss: 0.260172\tvalid_1's auc: 0.745585\tvalid_1's binary_logloss: 0.263011\n",
      "[64]\ttraining's auc: 0.779196\ttraining's binary_logloss: 0.259907\tvalid_1's auc: 0.745602\tvalid_1's binary_logloss: 0.262795\n",
      "[65]\ttraining's auc: 0.779411\ttraining's binary_logloss: 0.25971\tvalid_1's auc: 0.745566\tvalid_1's binary_logloss: 0.262652\n",
      "[66]\ttraining's auc: 0.779757\ttraining's binary_logloss: 0.259489\tvalid_1's auc: 0.745731\tvalid_1's binary_logloss: 0.262488\n",
      "[67]\ttraining's auc: 0.779714\ttraining's binary_logloss: 0.259245\tvalid_1's auc: 0.745713\tvalid_1's binary_logloss: 0.262284\n",
      "[68]\ttraining's auc: 0.779792\ttraining's binary_logloss: 0.259029\tvalid_1's auc: 0.745713\tvalid_1's binary_logloss: 0.26212\n",
      "[69]\ttraining's auc: 0.779976\ttraining's binary_logloss: 0.258829\tvalid_1's auc: 0.745588\tvalid_1's binary_logloss: 0.261974\n",
      "[70]\ttraining's auc: 0.779972\ttraining's binary_logloss: 0.258594\tvalid_1's auc: 0.745583\tvalid_1's binary_logloss: 0.261782\n",
      "[71]\ttraining's auc: 0.780309\ttraining's binary_logloss: 0.258432\tvalid_1's auc: 0.745645\tvalid_1's binary_logloss: 0.261675\n",
      "[72]\ttraining's auc: 0.780438\ttraining's binary_logloss: 0.258223\tvalid_1's auc: 0.745654\tvalid_1's binary_logloss: 0.261509\n",
      "[73]\ttraining's auc: 0.780849\ttraining's binary_logloss: 0.258068\tvalid_1's auc: 0.745889\tvalid_1's binary_logloss: 0.261405\n",
      "[74]\ttraining's auc: 0.781275\ttraining's binary_logloss: 0.257894\tvalid_1's auc: 0.746095\tvalid_1's binary_logloss: 0.261287\n",
      "[75]\ttraining's auc: 0.781343\ttraining's binary_logloss: 0.257711\tvalid_1's auc: 0.74614\tvalid_1's binary_logloss: 0.261143\n",
      "[76]\ttraining's auc: 0.781764\ttraining's binary_logloss: 0.257559\tvalid_1's auc: 0.746311\tvalid_1's binary_logloss: 0.261042\n",
      "[77]\ttraining's auc: 0.781917\ttraining's binary_logloss: 0.257354\tvalid_1's auc: 0.746287\tvalid_1's binary_logloss: 0.260885\n",
      "[78]\ttraining's auc: 0.78202\ttraining's binary_logloss: 0.257161\tvalid_1's auc: 0.746213\tvalid_1's binary_logloss: 0.260746\n",
      "[79]\ttraining's auc: 0.782222\ttraining's binary_logloss: 0.256985\tvalid_1's auc: 0.74629\tvalid_1's binary_logloss: 0.260616\n",
      "[80]\ttraining's auc: 0.782342\ttraining's binary_logloss: 0.256807\tvalid_1's auc: 0.746368\tvalid_1's binary_logloss: 0.260478\n",
      "[81]\ttraining's auc: 0.782532\ttraining's binary_logloss: 0.25661\tvalid_1's auc: 0.746279\tvalid_1's binary_logloss: 0.260342\n",
      "[82]\ttraining's auc: 0.782593\ttraining's binary_logloss: 0.256387\tvalid_1's auc: 0.746291\tvalid_1's binary_logloss: 0.260173\n",
      "[83]\ttraining's auc: 0.782758\ttraining's binary_logloss: 0.256197\tvalid_1's auc: 0.746324\tvalid_1's binary_logloss: 0.26003\n",
      "[84]\ttraining's auc: 0.783023\ttraining's binary_logloss: 0.256014\tvalid_1's auc: 0.746546\tvalid_1's binary_logloss: 0.259881\n",
      "[85]\ttraining's auc: 0.783319\ttraining's binary_logloss: 0.255826\tvalid_1's auc: 0.746761\tvalid_1's binary_logloss: 0.259724\n",
      "[86]\ttraining's auc: 0.783481\ttraining's binary_logloss: 0.255655\tvalid_1's auc: 0.746834\tvalid_1's binary_logloss: 0.259596\n",
      "[87]\ttraining's auc: 0.783683\ttraining's binary_logloss: 0.255479\tvalid_1's auc: 0.747063\tvalid_1's binary_logloss: 0.259457\n",
      "[88]\ttraining's auc: 0.783697\ttraining's binary_logloss: 0.255274\tvalid_1's auc: 0.747037\tvalid_1's binary_logloss: 0.259296\n",
      "[89]\ttraining's auc: 0.78386\ttraining's binary_logloss: 0.255095\tvalid_1's auc: 0.747119\tvalid_1's binary_logloss: 0.259161\n",
      "[90]\ttraining's auc: 0.784112\ttraining's binary_logloss: 0.254918\tvalid_1's auc: 0.747219\tvalid_1's binary_logloss: 0.259028\n",
      "[91]\ttraining's auc: 0.784327\ttraining's binary_logloss: 0.254744\tvalid_1's auc: 0.747265\tvalid_1's binary_logloss: 0.258897\n",
      "[92]\ttraining's auc: 0.784588\ttraining's binary_logloss: 0.254576\tvalid_1's auc: 0.747354\tvalid_1's binary_logloss: 0.258779\n",
      "[93]\ttraining's auc: 0.784952\ttraining's binary_logloss: 0.254447\tvalid_1's auc: 0.74732\tvalid_1's binary_logloss: 0.258713\n",
      "[94]\ttraining's auc: 0.78499\ttraining's binary_logloss: 0.254254\tvalid_1's auc: 0.747274\tvalid_1's binary_logloss: 0.258572\n",
      "[95]\ttraining's auc: 0.785083\ttraining's binary_logloss: 0.254086\tvalid_1's auc: 0.747235\tvalid_1's binary_logloss: 0.258457\n",
      "[96]\ttraining's auc: 0.785405\ttraining's binary_logloss: 0.253948\tvalid_1's auc: 0.747403\tvalid_1's binary_logloss: 0.258363\n",
      "[97]\ttraining's auc: 0.785669\ttraining's binary_logloss: 0.253796\tvalid_1's auc: 0.747541\tvalid_1's binary_logloss: 0.25825\n",
      "[98]\ttraining's auc: 0.785896\ttraining's binary_logloss: 0.253632\tvalid_1's auc: 0.747633\tvalid_1's binary_logloss: 0.258133\n",
      "[99]\ttraining's auc: 0.785951\ttraining's binary_logloss: 0.253449\tvalid_1's auc: 0.747619\tvalid_1's binary_logloss: 0.257998\n",
      "[100]\ttraining's auc: 0.786088\ttraining's binary_logloss: 0.253285\tvalid_1's auc: 0.74758\tvalid_1's binary_logloss: 0.257887\n",
      "[101]\ttraining's auc: 0.786277\ttraining's binary_logloss: 0.25312\tvalid_1's auc: 0.747575\tvalid_1's binary_logloss: 0.257777\n",
      "[102]\ttraining's auc: 0.7864\ttraining's binary_logloss: 0.252934\tvalid_1's auc: 0.747624\tvalid_1's binary_logloss: 0.257641\n",
      "[103]\ttraining's auc: 0.786517\ttraining's binary_logloss: 0.252773\tvalid_1's auc: 0.747543\tvalid_1's binary_logloss: 0.257539\n",
      "[104]\ttraining's auc: 0.786853\ttraining's binary_logloss: 0.25263\tvalid_1's auc: 0.747806\tvalid_1's binary_logloss: 0.257433\n",
      "[105]\ttraining's auc: 0.786902\ttraining's binary_logloss: 0.252455\tvalid_1's auc: 0.747775\tvalid_1's binary_logloss: 0.257303\n",
      "[106]\ttraining's auc: 0.787102\ttraining's binary_logloss: 0.252298\tvalid_1's auc: 0.747864\tvalid_1's binary_logloss: 0.257188\n",
      "[107]\ttraining's auc: 0.787186\ttraining's binary_logloss: 0.252131\tvalid_1's auc: 0.747856\tvalid_1's binary_logloss: 0.257069\n",
      "[108]\ttraining's auc: 0.787283\ttraining's binary_logloss: 0.251988\tvalid_1's auc: 0.747942\tvalid_1's binary_logloss: 0.25697\n",
      "[109]\ttraining's auc: 0.787667\ttraining's binary_logloss: 0.251857\tvalid_1's auc: 0.748181\tvalid_1's binary_logloss: 0.256884\n",
      "[110]\ttraining's auc: 0.78793\ttraining's binary_logloss: 0.251704\tvalid_1's auc: 0.748339\tvalid_1's binary_logloss: 0.256766\n",
      "[111]\ttraining's auc: 0.787943\ttraining's binary_logloss: 0.251542\tvalid_1's auc: 0.748289\tvalid_1's binary_logloss: 0.256654\n",
      "[112]\ttraining's auc: 0.787992\ttraining's binary_logloss: 0.25138\tvalid_1's auc: 0.748373\tvalid_1's binary_logloss: 0.256533\n",
      "[113]\ttraining's auc: 0.788322\ttraining's binary_logloss: 0.251251\tvalid_1's auc: 0.748541\tvalid_1's binary_logloss: 0.256448\n",
      "[114]\ttraining's auc: 0.788588\ttraining's binary_logloss: 0.251127\tvalid_1's auc: 0.748564\tvalid_1's binary_logloss: 0.256377\n",
      "[115]\ttraining's auc: 0.788639\ttraining's binary_logloss: 0.250969\tvalid_1's auc: 0.748561\tvalid_1's binary_logloss: 0.256263\n",
      "[116]\ttraining's auc: 0.788919\ttraining's binary_logloss: 0.25085\tvalid_1's auc: 0.748585\tvalid_1's binary_logloss: 0.256203\n",
      "[117]\ttraining's auc: 0.789099\ttraining's binary_logloss: 0.250712\tvalid_1's auc: 0.748594\tvalid_1's binary_logloss: 0.256117\n",
      "[118]\ttraining's auc: 0.789225\ttraining's binary_logloss: 0.25057\tvalid_1's auc: 0.748519\tvalid_1's binary_logloss: 0.256032\n",
      "[119]\ttraining's auc: 0.789355\ttraining's binary_logloss: 0.250414\tvalid_1's auc: 0.748593\tvalid_1's binary_logloss: 0.255932\n",
      "[120]\ttraining's auc: 0.789593\ttraining's binary_logloss: 0.250304\tvalid_1's auc: 0.748637\tvalid_1's binary_logloss: 0.255875\n",
      "[121]\ttraining's auc: 0.789605\ttraining's binary_logloss: 0.250157\tvalid_1's auc: 0.74857\tvalid_1's binary_logloss: 0.255777\n",
      "[122]\ttraining's auc: 0.78961\ttraining's binary_logloss: 0.250013\tvalid_1's auc: 0.74853\tvalid_1's binary_logloss: 0.255676\n",
      "[123]\ttraining's auc: 0.789757\ttraining's binary_logloss: 0.249886\tvalid_1's auc: 0.748571\tvalid_1's binary_logloss: 0.255591\n",
      "[124]\ttraining's auc: 0.789774\ttraining's binary_logloss: 0.249741\tvalid_1's auc: 0.748488\tvalid_1's binary_logloss: 0.2555\n",
      "[125]\ttraining's auc: 0.789973\ttraining's binary_logloss: 0.249613\tvalid_1's auc: 0.748481\tvalid_1's binary_logloss: 0.255425\n",
      "[126]\ttraining's auc: 0.790013\ttraining's binary_logloss: 0.249476\tvalid_1's auc: 0.748473\tvalid_1's binary_logloss: 0.255332\n",
      "[127]\ttraining's auc: 0.790213\ttraining's binary_logloss: 0.249348\tvalid_1's auc: 0.748586\tvalid_1's binary_logloss: 0.255247\n",
      "[128]\ttraining's auc: 0.79041\ttraining's binary_logloss: 0.249216\tvalid_1's auc: 0.74866\tvalid_1's binary_logloss: 0.255162\n",
      "[129]\ttraining's auc: 0.790507\ttraining's binary_logloss: 0.249092\tvalid_1's auc: 0.748601\tvalid_1's binary_logloss: 0.255092\n",
      "[130]\ttraining's auc: 0.790686\ttraining's binary_logloss: 0.248971\tvalid_1's auc: 0.748656\tvalid_1's binary_logloss: 0.255017\n",
      "[131]\ttraining's auc: 0.790767\ttraining's binary_logloss: 0.248844\tvalid_1's auc: 0.748655\tvalid_1's binary_logloss: 0.254936\n",
      "[132]\ttraining's auc: 0.790986\ttraining's binary_logloss: 0.248709\tvalid_1's auc: 0.7488\tvalid_1's binary_logloss: 0.254837\n",
      "[133]\ttraining's auc: 0.790995\ttraining's binary_logloss: 0.248577\tvalid_1's auc: 0.748808\tvalid_1's binary_logloss: 0.254744\n",
      "[134]\ttraining's auc: 0.791213\ttraining's binary_logloss: 0.248476\tvalid_1's auc: 0.748796\tvalid_1's binary_logloss: 0.254698\n",
      "[135]\ttraining's auc: 0.791318\ttraining's binary_logloss: 0.248341\tvalid_1's auc: 0.748874\tvalid_1's binary_logloss: 0.254609\n",
      "[136]\ttraining's auc: 0.791366\ttraining's binary_logloss: 0.248213\tvalid_1's auc: 0.748856\tvalid_1's binary_logloss: 0.254523\n",
      "[137]\ttraining's auc: 0.79156\ttraining's binary_logloss: 0.248088\tvalid_1's auc: 0.748898\tvalid_1's binary_logloss: 0.254442\n",
      "[138]\ttraining's auc: 0.791736\ttraining's binary_logloss: 0.247966\tvalid_1's auc: 0.748917\tvalid_1's binary_logloss: 0.254372\n",
      "[139]\ttraining's auc: 0.792058\ttraining's binary_logloss: 0.247842\tvalid_1's auc: 0.749006\tvalid_1's binary_logloss: 0.2543\n",
      "[140]\ttraining's auc: 0.792178\ttraining's binary_logloss: 0.247718\tvalid_1's auc: 0.748879\tvalid_1's binary_logloss: 0.254241\n",
      "[141]\ttraining's auc: 0.792258\ttraining's binary_logloss: 0.247589\tvalid_1's auc: 0.748884\tvalid_1's binary_logloss: 0.254161\n",
      "[142]\ttraining's auc: 0.79232\ttraining's binary_logloss: 0.247459\tvalid_1's auc: 0.748847\tvalid_1's binary_logloss: 0.254082\n",
      "[143]\ttraining's auc: 0.792399\ttraining's binary_logloss: 0.247328\tvalid_1's auc: 0.74881\tvalid_1's binary_logloss: 0.254002\n",
      "[144]\ttraining's auc: 0.792419\ttraining's binary_logloss: 0.247205\tvalid_1's auc: 0.748806\tvalid_1's binary_logloss: 0.253922\n",
      "[145]\ttraining's auc: 0.792536\ttraining's binary_logloss: 0.247103\tvalid_1's auc: 0.748867\tvalid_1's binary_logloss: 0.253858\n",
      "[146]\ttraining's auc: 0.792725\ttraining's binary_logloss: 0.246981\tvalid_1's auc: 0.748922\tvalid_1's binary_logloss: 0.253788\n",
      "[147]\ttraining's auc: 0.792793\ttraining's binary_logloss: 0.246856\tvalid_1's auc: 0.74891\tvalid_1's binary_logloss: 0.253711\n",
      "[148]\ttraining's auc: 0.792843\ttraining's binary_logloss: 0.246738\tvalid_1's auc: 0.748861\tvalid_1's binary_logloss: 0.25364\n",
      "[149]\ttraining's auc: 0.793076\ttraining's binary_logloss: 0.24665\tvalid_1's auc: 0.748939\tvalid_1's binary_logloss: 0.253595\n",
      "[150]\ttraining's auc: 0.793243\ttraining's binary_logloss: 0.246537\tvalid_1's auc: 0.749057\tvalid_1's binary_logloss: 0.253521\n",
      "[151]\ttraining's auc: 0.793306\ttraining's binary_logloss: 0.246419\tvalid_1's auc: 0.749047\tvalid_1's binary_logloss: 0.253455\n",
      "[152]\ttraining's auc: 0.793421\ttraining's binary_logloss: 0.246312\tvalid_1's auc: 0.749007\tvalid_1's binary_logloss: 0.253402\n",
      "[153]\ttraining's auc: 0.79346\ttraining's binary_logloss: 0.246194\tvalid_1's auc: 0.748981\tvalid_1's binary_logloss: 0.253333\n",
      "[154]\ttraining's auc: 0.793525\ttraining's binary_logloss: 0.246078\tvalid_1's auc: 0.748949\tvalid_1's binary_logloss: 0.25327\n",
      "[155]\ttraining's auc: 0.793633\ttraining's binary_logloss: 0.245969\tvalid_1's auc: 0.748963\tvalid_1's binary_logloss: 0.253213\n",
      "[156]\ttraining's auc: 0.793773\ttraining's binary_logloss: 0.245865\tvalid_1's auc: 0.748982\tvalid_1's binary_logloss: 0.253159\n",
      "[157]\ttraining's auc: 0.793901\ttraining's binary_logloss: 0.24576\tvalid_1's auc: 0.749045\tvalid_1's binary_logloss: 0.253099\n",
      "[158]\ttraining's auc: 0.794009\ttraining's binary_logloss: 0.245653\tvalid_1's auc: 0.749\tvalid_1's binary_logloss: 0.253038\n",
      "[159]\ttraining's auc: 0.794133\ttraining's binary_logloss: 0.245532\tvalid_1's auc: 0.749096\tvalid_1's binary_logloss: 0.252958\n",
      "[160]\ttraining's auc: 0.794262\ttraining's binary_logloss: 0.245412\tvalid_1's auc: 0.749194\tvalid_1's binary_logloss: 0.252878\n",
      "[161]\ttraining's auc: 0.794373\ttraining's binary_logloss: 0.245311\tvalid_1's auc: 0.749099\tvalid_1's binary_logloss: 0.252834\n",
      "[162]\ttraining's auc: 0.794472\ttraining's binary_logloss: 0.245199\tvalid_1's auc: 0.749111\tvalid_1's binary_logloss: 0.252768\n",
      "[163]\ttraining's auc: 0.794655\ttraining's binary_logloss: 0.245093\tvalid_1's auc: 0.749156\tvalid_1's binary_logloss: 0.252715\n",
      "[164]\ttraining's auc: 0.794818\ttraining's binary_logloss: 0.244984\tvalid_1's auc: 0.749219\tvalid_1's binary_logloss: 0.252652\n",
      "[165]\ttraining's auc: 0.794894\ttraining's binary_logloss: 0.244875\tvalid_1's auc: 0.749242\tvalid_1's binary_logloss: 0.252588\n",
      "[166]\ttraining's auc: 0.795031\ttraining's binary_logloss: 0.244776\tvalid_1's auc: 0.749294\tvalid_1's binary_logloss: 0.252524\n",
      "[167]\ttraining's auc: 0.795095\ttraining's binary_logloss: 0.244671\tvalid_1's auc: 0.749398\tvalid_1's binary_logloss: 0.252451\n",
      "[168]\ttraining's auc: 0.795194\ttraining's binary_logloss: 0.244566\tvalid_1's auc: 0.749464\tvalid_1's binary_logloss: 0.252382\n",
      "[169]\ttraining's auc: 0.795295\ttraining's binary_logloss: 0.244463\tvalid_1's auc: 0.749511\tvalid_1's binary_logloss: 0.252322\n",
      "[170]\ttraining's auc: 0.79537\ttraining's binary_logloss: 0.244365\tvalid_1's auc: 0.749607\tvalid_1's binary_logloss: 0.252259\n",
      "[171]\ttraining's auc: 0.795533\ttraining's binary_logloss: 0.244263\tvalid_1's auc: 0.749664\tvalid_1's binary_logloss: 0.252206\n",
      "[172]\ttraining's auc: 0.795661\ttraining's binary_logloss: 0.244159\tvalid_1's auc: 0.749679\tvalid_1's binary_logloss: 0.252151\n",
      "[173]\ttraining's auc: 0.795897\ttraining's binary_logloss: 0.244064\tvalid_1's auc: 0.749699\tvalid_1's binary_logloss: 0.252111\n",
      "[174]\ttraining's auc: 0.796062\ttraining's binary_logloss: 0.243967\tvalid_1's auc: 0.749834\tvalid_1's binary_logloss: 0.252049\n",
      "[175]\ttraining's auc: 0.796212\ttraining's binary_logloss: 0.24387\tvalid_1's auc: 0.749929\tvalid_1's binary_logloss: 0.251988\n",
      "[176]\ttraining's auc: 0.79647\ttraining's binary_logloss: 0.243771\tvalid_1's auc: 0.750013\tvalid_1's binary_logloss: 0.251932\n",
      "[177]\ttraining's auc: 0.796628\ttraining's binary_logloss: 0.243671\tvalid_1's auc: 0.750042\tvalid_1's binary_logloss: 0.25188\n",
      "[178]\ttraining's auc: 0.79681\ttraining's binary_logloss: 0.243574\tvalid_1's auc: 0.7501\tvalid_1's binary_logloss: 0.251825\n",
      "[179]\ttraining's auc: 0.796914\ttraining's binary_logloss: 0.243473\tvalid_1's auc: 0.750093\tvalid_1's binary_logloss: 0.251771\n",
      "[180]\ttraining's auc: 0.79702\ttraining's binary_logloss: 0.243378\tvalid_1's auc: 0.750095\tvalid_1's binary_logloss: 0.251719\n",
      "[181]\ttraining's auc: 0.797157\ttraining's binary_logloss: 0.243286\tvalid_1's auc: 0.750113\tvalid_1's binary_logloss: 0.251677\n",
      "[182]\ttraining's auc: 0.797226\ttraining's binary_logloss: 0.243192\tvalid_1's auc: 0.750106\tvalid_1's binary_logloss: 0.251626\n",
      "[183]\ttraining's auc: 0.797367\ttraining's binary_logloss: 0.243101\tvalid_1's auc: 0.750133\tvalid_1's binary_logloss: 0.251582\n",
      "[184]\ttraining's auc: 0.797458\ttraining's binary_logloss: 0.243007\tvalid_1's auc: 0.750111\tvalid_1's binary_logloss: 0.251538\n",
      "[185]\ttraining's auc: 0.797651\ttraining's binary_logloss: 0.242918\tvalid_1's auc: 0.750172\tvalid_1's binary_logloss: 0.251493\n",
      "[186]\ttraining's auc: 0.797733\ttraining's binary_logloss: 0.242824\tvalid_1's auc: 0.750165\tvalid_1's binary_logloss: 0.251444\n",
      "[187]\ttraining's auc: 0.797829\ttraining's binary_logloss: 0.242738\tvalid_1's auc: 0.75022\tvalid_1's binary_logloss: 0.251397\n",
      "[188]\ttraining's auc: 0.797931\ttraining's binary_logloss: 0.24265\tvalid_1's auc: 0.750201\tvalid_1's binary_logloss: 0.251354\n",
      "[189]\ttraining's auc: 0.798032\ttraining's binary_logloss: 0.242557\tvalid_1's auc: 0.750252\tvalid_1's binary_logloss: 0.251297\n",
      "[190]\ttraining's auc: 0.798129\ttraining's binary_logloss: 0.242463\tvalid_1's auc: 0.750253\tvalid_1's binary_logloss: 0.25125\n",
      "[191]\ttraining's auc: 0.79822\ttraining's binary_logloss: 0.242373\tvalid_1's auc: 0.750274\tvalid_1's binary_logloss: 0.2512\n",
      "[192]\ttraining's auc: 0.798318\ttraining's binary_logloss: 0.242284\tvalid_1's auc: 0.750238\tvalid_1's binary_logloss: 0.251155\n",
      "[193]\ttraining's auc: 0.798466\ttraining's binary_logloss: 0.242198\tvalid_1's auc: 0.750286\tvalid_1's binary_logloss: 0.25111\n",
      "[194]\ttraining's auc: 0.798588\ttraining's binary_logloss: 0.242114\tvalid_1's auc: 0.750379\tvalid_1's binary_logloss: 0.251055\n",
      "[195]\ttraining's auc: 0.798684\ttraining's binary_logloss: 0.242028\tvalid_1's auc: 0.7504\tvalid_1's binary_logloss: 0.251006\n",
      "[196]\ttraining's auc: 0.798813\ttraining's binary_logloss: 0.241936\tvalid_1's auc: 0.75041\tvalid_1's binary_logloss: 0.250964\n",
      "[197]\ttraining's auc: 0.79891\ttraining's binary_logloss: 0.241847\tvalid_1's auc: 0.750421\tvalid_1's binary_logloss: 0.250922\n",
      "[198]\ttraining's auc: 0.799178\ttraining's binary_logloss: 0.24175\tvalid_1's auc: 0.75052\tvalid_1's binary_logloss: 0.250871\n",
      "[199]\ttraining's auc: 0.799265\ttraining's binary_logloss: 0.241659\tvalid_1's auc: 0.750531\tvalid_1's binary_logloss: 0.250828\n",
      "[200]\ttraining's auc: 0.799479\ttraining's binary_logloss: 0.241569\tvalid_1's auc: 0.750611\tvalid_1's binary_logloss: 0.250787\n",
      "[201]\ttraining's auc: 0.799576\ttraining's binary_logloss: 0.241476\tvalid_1's auc: 0.7507\tvalid_1's binary_logloss: 0.250728\n",
      "[202]\ttraining's auc: 0.799806\ttraining's binary_logloss: 0.241384\tvalid_1's auc: 0.750844\tvalid_1's binary_logloss: 0.250676\n",
      "[203]\ttraining's auc: 0.799964\ttraining's binary_logloss: 0.241295\tvalid_1's auc: 0.750861\tvalid_1's binary_logloss: 0.250633\n",
      "[204]\ttraining's auc: 0.800066\ttraining's binary_logloss: 0.241209\tvalid_1's auc: 0.750938\tvalid_1's binary_logloss: 0.250581\n",
      "[205]\ttraining's auc: 0.800159\ttraining's binary_logloss: 0.241123\tvalid_1's auc: 0.750963\tvalid_1's binary_logloss: 0.250536\n",
      "[206]\ttraining's auc: 0.800282\ttraining's binary_logloss: 0.241029\tvalid_1's auc: 0.751023\tvalid_1's binary_logloss: 0.250492\n",
      "[207]\ttraining's auc: 0.800428\ttraining's binary_logloss: 0.240951\tvalid_1's auc: 0.751111\tvalid_1's binary_logloss: 0.250444\n",
      "[208]\ttraining's auc: 0.800561\ttraining's binary_logloss: 0.24087\tvalid_1's auc: 0.751168\tvalid_1's binary_logloss: 0.250404\n",
      "[209]\ttraining's auc: 0.800634\ttraining's binary_logloss: 0.240791\tvalid_1's auc: 0.751149\tvalid_1's binary_logloss: 0.250371\n",
      "[210]\ttraining's auc: 0.800787\ttraining's binary_logloss: 0.240707\tvalid_1's auc: 0.751204\tvalid_1's binary_logloss: 0.250334\n",
      "[211]\ttraining's auc: 0.80089\ttraining's binary_logloss: 0.24062\tvalid_1's auc: 0.751225\tvalid_1's binary_logloss: 0.250294\n",
      "[212]\ttraining's auc: 0.800989\ttraining's binary_logloss: 0.240534\tvalid_1's auc: 0.751247\tvalid_1's binary_logloss: 0.250254\n",
      "[213]\ttraining's auc: 0.801091\ttraining's binary_logloss: 0.240449\tvalid_1's auc: 0.75128\tvalid_1's binary_logloss: 0.250211\n",
      "[214]\ttraining's auc: 0.801224\ttraining's binary_logloss: 0.240369\tvalid_1's auc: 0.75136\tvalid_1's binary_logloss: 0.250164\n",
      "[215]\ttraining's auc: 0.801442\ttraining's binary_logloss: 0.24028\tvalid_1's auc: 0.751439\tvalid_1's binary_logloss: 0.250124\n",
      "[216]\ttraining's auc: 0.801587\ttraining's binary_logloss: 0.240199\tvalid_1's auc: 0.751513\tvalid_1's binary_logloss: 0.250083\n",
      "[217]\ttraining's auc: 0.801728\ttraining's binary_logloss: 0.240118\tvalid_1's auc: 0.751538\tvalid_1's binary_logloss: 0.250048\n",
      "[218]\ttraining's auc: 0.801836\ttraining's binary_logloss: 0.240041\tvalid_1's auc: 0.751561\tvalid_1's binary_logloss: 0.250018\n",
      "[219]\ttraining's auc: 0.801958\ttraining's binary_logloss: 0.239967\tvalid_1's auc: 0.751584\tvalid_1's binary_logloss: 0.249988\n",
      "[220]\ttraining's auc: 0.802076\ttraining's binary_logloss: 0.239891\tvalid_1's auc: 0.751642\tvalid_1's binary_logloss: 0.249949\n",
      "[221]\ttraining's auc: 0.802163\ttraining's binary_logloss: 0.239815\tvalid_1's auc: 0.751695\tvalid_1's binary_logloss: 0.24991\n",
      "[222]\ttraining's auc: 0.802255\ttraining's binary_logloss: 0.239735\tvalid_1's auc: 0.751704\tvalid_1's binary_logloss: 0.249877\n",
      "[223]\ttraining's auc: 0.802363\ttraining's binary_logloss: 0.239657\tvalid_1's auc: 0.751777\tvalid_1's binary_logloss: 0.24983\n",
      "[224]\ttraining's auc: 0.802459\ttraining's binary_logloss: 0.239578\tvalid_1's auc: 0.751789\tvalid_1's binary_logloss: 0.249793\n",
      "[225]\ttraining's auc: 0.802572\ttraining's binary_logloss: 0.239503\tvalid_1's auc: 0.751851\tvalid_1's binary_logloss: 0.249756\n",
      "[226]\ttraining's auc: 0.802692\ttraining's binary_logloss: 0.239426\tvalid_1's auc: 0.751887\tvalid_1's binary_logloss: 0.249722\n",
      "[227]\ttraining's auc: 0.802866\ttraining's binary_logloss: 0.239354\tvalid_1's auc: 0.751887\tvalid_1's binary_logloss: 0.249703\n",
      "[228]\ttraining's auc: 0.802957\ttraining's binary_logloss: 0.239275\tvalid_1's auc: 0.751903\tvalid_1's binary_logloss: 0.249669\n",
      "[229]\ttraining's auc: 0.803085\ttraining's binary_logloss: 0.239198\tvalid_1's auc: 0.751922\tvalid_1's binary_logloss: 0.249639\n",
      "[230]\ttraining's auc: 0.803178\ttraining's binary_logloss: 0.239122\tvalid_1's auc: 0.751936\tvalid_1's binary_logloss: 0.249609\n",
      "[231]\ttraining's auc: 0.803297\ttraining's binary_logloss: 0.239042\tvalid_1's auc: 0.751943\tvalid_1's binary_logloss: 0.249579\n",
      "[232]\ttraining's auc: 0.803428\ttraining's binary_logloss: 0.238973\tvalid_1's auc: 0.751942\tvalid_1's binary_logloss: 0.249557\n",
      "[233]\ttraining's auc: 0.803526\ttraining's binary_logloss: 0.238893\tvalid_1's auc: 0.751929\tvalid_1's binary_logloss: 0.249529\n",
      "[234]\ttraining's auc: 0.80373\ttraining's binary_logloss: 0.238822\tvalid_1's auc: 0.751985\tvalid_1's binary_logloss: 0.249497\n",
      "[235]\ttraining's auc: 0.803854\ttraining's binary_logloss: 0.238747\tvalid_1's auc: 0.752061\tvalid_1's binary_logloss: 0.249458\n",
      "[236]\ttraining's auc: 0.80397\ttraining's binary_logloss: 0.238673\tvalid_1's auc: 0.752123\tvalid_1's binary_logloss: 0.249424\n",
      "[237]\ttraining's auc: 0.804138\ttraining's binary_logloss: 0.238603\tvalid_1's auc: 0.752142\tvalid_1's binary_logloss: 0.249405\n",
      "[238]\ttraining's auc: 0.804211\ttraining's binary_logloss: 0.238532\tvalid_1's auc: 0.752158\tvalid_1's binary_logloss: 0.249376\n",
      "[239]\ttraining's auc: 0.804377\ttraining's binary_logloss: 0.23845\tvalid_1's auc: 0.752192\tvalid_1's binary_logloss: 0.24935\n",
      "[240]\ttraining's auc: 0.804525\ttraining's binary_logloss: 0.238379\tvalid_1's auc: 0.75224\tvalid_1's binary_logloss: 0.249318\n",
      "[241]\ttraining's auc: 0.804662\ttraining's binary_logloss: 0.238306\tvalid_1's auc: 0.752238\tvalid_1's binary_logloss: 0.2493\n",
      "[242]\ttraining's auc: 0.804795\ttraining's binary_logloss: 0.238232\tvalid_1's auc: 0.752346\tvalid_1's binary_logloss: 0.249261\n",
      "[243]\ttraining's auc: 0.804895\ttraining's binary_logloss: 0.238158\tvalid_1's auc: 0.75234\tvalid_1's binary_logloss: 0.24924\n",
      "[244]\ttraining's auc: 0.805037\ttraining's binary_logloss: 0.238084\tvalid_1's auc: 0.752387\tvalid_1's binary_logloss: 0.249213\n",
      "[245]\ttraining's auc: 0.805161\ttraining's binary_logloss: 0.238013\tvalid_1's auc: 0.752365\tvalid_1's binary_logloss: 0.24919\n",
      "[246]\ttraining's auc: 0.805316\ttraining's binary_logloss: 0.237933\tvalid_1's auc: 0.752375\tvalid_1's binary_logloss: 0.249171\n",
      "[247]\ttraining's auc: 0.805504\ttraining's binary_logloss: 0.237865\tvalid_1's auc: 0.752347\tvalid_1's binary_logloss: 0.249159\n",
      "[248]\ttraining's auc: 0.80572\ttraining's binary_logloss: 0.237781\tvalid_1's auc: 0.752451\tvalid_1's binary_logloss: 0.249123\n",
      "[249]\ttraining's auc: 0.805866\ttraining's binary_logloss: 0.237709\tvalid_1's auc: 0.752533\tvalid_1's binary_logloss: 0.249089\n",
      "[250]\ttraining's auc: 0.806044\ttraining's binary_logloss: 0.237644\tvalid_1's auc: 0.752662\tvalid_1's binary_logloss: 0.249058\n",
      "[251]\ttraining's auc: 0.806179\ttraining's binary_logloss: 0.237571\tvalid_1's auc: 0.752682\tvalid_1's binary_logloss: 0.249031\n",
      "[252]\ttraining's auc: 0.80625\ttraining's binary_logloss: 0.237507\tvalid_1's auc: 0.752698\tvalid_1's binary_logloss: 0.249006\n",
      "[253]\ttraining's auc: 0.806422\ttraining's binary_logloss: 0.237435\tvalid_1's auc: 0.752752\tvalid_1's binary_logloss: 0.248976\n",
      "[254]\ttraining's auc: 0.806537\ttraining's binary_logloss: 0.237373\tvalid_1's auc: 0.752823\tvalid_1's binary_logloss: 0.248944\n",
      "[255]\ttraining's auc: 0.806765\ttraining's binary_logloss: 0.237292\tvalid_1's auc: 0.752921\tvalid_1's binary_logloss: 0.248911\n",
      "[256]\ttraining's auc: 0.806963\ttraining's binary_logloss: 0.237213\tvalid_1's auc: 0.75298\tvalid_1's binary_logloss: 0.248885\n",
      "[257]\ttraining's auc: 0.807094\ttraining's binary_logloss: 0.237146\tvalid_1's auc: 0.753017\tvalid_1's binary_logloss: 0.248865\n",
      "[258]\ttraining's auc: 0.807194\ttraining's binary_logloss: 0.237073\tvalid_1's auc: 0.753032\tvalid_1's binary_logloss: 0.248841\n",
      "[259]\ttraining's auc: 0.80729\ttraining's binary_logloss: 0.237003\tvalid_1's auc: 0.753076\tvalid_1's binary_logloss: 0.248815\n",
      "[260]\ttraining's auc: 0.807434\ttraining's binary_logloss: 0.236937\tvalid_1's auc: 0.753103\tvalid_1's binary_logloss: 0.248793\n",
      "[261]\ttraining's auc: 0.807612\ttraining's binary_logloss: 0.236864\tvalid_1's auc: 0.753196\tvalid_1's binary_logloss: 0.248761\n",
      "[262]\ttraining's auc: 0.807703\ttraining's binary_logloss: 0.236795\tvalid_1's auc: 0.753254\tvalid_1's binary_logloss: 0.248732\n",
      "[263]\ttraining's auc: 0.807838\ttraining's binary_logloss: 0.236727\tvalid_1's auc: 0.753288\tvalid_1's binary_logloss: 0.248704\n",
      "[264]\ttraining's auc: 0.808033\ttraining's binary_logloss: 0.236648\tvalid_1's auc: 0.753384\tvalid_1's binary_logloss: 0.248659\n",
      "[265]\ttraining's auc: 0.808141\ttraining's binary_logloss: 0.236576\tvalid_1's auc: 0.753438\tvalid_1's binary_logloss: 0.24863\n",
      "[266]\ttraining's auc: 0.808227\ttraining's binary_logloss: 0.236504\tvalid_1's auc: 0.753449\tvalid_1's binary_logloss: 0.248606\n",
      "[267]\ttraining's auc: 0.808344\ttraining's binary_logloss: 0.236435\tvalid_1's auc: 0.753438\tvalid_1's binary_logloss: 0.248587\n",
      "[268]\ttraining's auc: 0.808452\ttraining's binary_logloss: 0.23637\tvalid_1's auc: 0.75346\tvalid_1's binary_logloss: 0.248557\n",
      "[269]\ttraining's auc: 0.808566\ttraining's binary_logloss: 0.236302\tvalid_1's auc: 0.753498\tvalid_1's binary_logloss: 0.248531\n",
      "[270]\ttraining's auc: 0.808694\ttraining's binary_logloss: 0.236242\tvalid_1's auc: 0.753497\tvalid_1's binary_logloss: 0.248511\n",
      "[271]\ttraining's auc: 0.808772\ttraining's binary_logloss: 0.236175\tvalid_1's auc: 0.753491\tvalid_1's binary_logloss: 0.248493\n",
      "[272]\ttraining's auc: 0.808965\ttraining's binary_logloss: 0.236108\tvalid_1's auc: 0.753538\tvalid_1's binary_logloss: 0.248472\n",
      "[273]\ttraining's auc: 0.809057\ttraining's binary_logloss: 0.23604\tvalid_1's auc: 0.75353\tvalid_1's binary_logloss: 0.248459\n",
      "[274]\ttraining's auc: 0.809167\ttraining's binary_logloss: 0.23597\tvalid_1's auc: 0.753564\tvalid_1's binary_logloss: 0.248437\n",
      "[275]\ttraining's auc: 0.809246\ttraining's binary_logloss: 0.235903\tvalid_1's auc: 0.753551\tvalid_1's binary_logloss: 0.248416\n",
      "[276]\ttraining's auc: 0.809337\ttraining's binary_logloss: 0.235838\tvalid_1's auc: 0.753605\tvalid_1's binary_logloss: 0.248383\n",
      "[277]\ttraining's auc: 0.809452\ttraining's binary_logloss: 0.235773\tvalid_1's auc: 0.75362\tvalid_1's binary_logloss: 0.248364\n",
      "[278]\ttraining's auc: 0.80969\ttraining's binary_logloss: 0.235697\tvalid_1's auc: 0.753744\tvalid_1's binary_logloss: 0.248325\n",
      "[279]\ttraining's auc: 0.809802\ttraining's binary_logloss: 0.23563\tvalid_1's auc: 0.753813\tvalid_1's binary_logloss: 0.2483\n",
      "[280]\ttraining's auc: 0.809902\ttraining's binary_logloss: 0.235566\tvalid_1's auc: 0.753867\tvalid_1's binary_logloss: 0.24827\n",
      "[281]\ttraining's auc: 0.810055\ttraining's binary_logloss: 0.235493\tvalid_1's auc: 0.753912\tvalid_1's binary_logloss: 0.248245\n",
      "[282]\ttraining's auc: 0.810154\ttraining's binary_logloss: 0.235427\tvalid_1's auc: 0.753959\tvalid_1's binary_logloss: 0.248225\n",
      "[283]\ttraining's auc: 0.810295\ttraining's binary_logloss: 0.235358\tvalid_1's auc: 0.753942\tvalid_1's binary_logloss: 0.248214\n",
      "[284]\ttraining's auc: 0.810422\ttraining's binary_logloss: 0.235293\tvalid_1's auc: 0.753983\tvalid_1's binary_logloss: 0.248189\n",
      "[285]\ttraining's auc: 0.810532\ttraining's binary_logloss: 0.235228\tvalid_1's auc: 0.753991\tvalid_1's binary_logloss: 0.248167\n",
      "[286]\ttraining's auc: 0.810675\ttraining's binary_logloss: 0.235167\tvalid_1's auc: 0.754024\tvalid_1's binary_logloss: 0.248149\n",
      "[287]\ttraining's auc: 0.810824\ttraining's binary_logloss: 0.235102\tvalid_1's auc: 0.753983\tvalid_1's binary_logloss: 0.248142\n",
      "[288]\ttraining's auc: 0.810953\ttraining's binary_logloss: 0.235044\tvalid_1's auc: 0.754017\tvalid_1's binary_logloss: 0.248122\n",
      "[289]\ttraining's auc: 0.811113\ttraining's binary_logloss: 0.234979\tvalid_1's auc: 0.754026\tvalid_1's binary_logloss: 0.248111\n",
      "[290]\ttraining's auc: 0.811223\ttraining's binary_logloss: 0.234918\tvalid_1's auc: 0.754065\tvalid_1's binary_logloss: 0.248088\n",
      "[291]\ttraining's auc: 0.811385\ttraining's binary_logloss: 0.234856\tvalid_1's auc: 0.75411\tvalid_1's binary_logloss: 0.248063\n",
      "[292]\ttraining's auc: 0.8115\ttraining's binary_logloss: 0.234791\tvalid_1's auc: 0.754124\tvalid_1's binary_logloss: 0.248042\n",
      "[293]\ttraining's auc: 0.811579\ttraining's binary_logloss: 0.234729\tvalid_1's auc: 0.75416\tvalid_1's binary_logloss: 0.248017\n",
      "[294]\ttraining's auc: 0.811692\ttraining's binary_logloss: 0.234667\tvalid_1's auc: 0.754132\tvalid_1's binary_logloss: 0.248006\n",
      "[295]\ttraining's auc: 0.811831\ttraining's binary_logloss: 0.234609\tvalid_1's auc: 0.754141\tvalid_1's binary_logloss: 0.247994\n",
      "[296]\ttraining's auc: 0.811938\ttraining's binary_logloss: 0.234549\tvalid_1's auc: 0.754161\tvalid_1's binary_logloss: 0.247978\n",
      "[297]\ttraining's auc: 0.812045\ttraining's binary_logloss: 0.234486\tvalid_1's auc: 0.754203\tvalid_1's binary_logloss: 0.247956\n",
      "[298]\ttraining's auc: 0.812197\ttraining's binary_logloss: 0.234417\tvalid_1's auc: 0.754299\tvalid_1's binary_logloss: 0.247918\n",
      "[299]\ttraining's auc: 0.812322\ttraining's binary_logloss: 0.234354\tvalid_1's auc: 0.754335\tvalid_1's binary_logloss: 0.247894\n",
      "[300]\ttraining's auc: 0.81249\ttraining's binary_logloss: 0.234292\tvalid_1's auc: 0.754422\tvalid_1's binary_logloss: 0.247866\n",
      "[301]\ttraining's auc: 0.812602\ttraining's binary_logloss: 0.234228\tvalid_1's auc: 0.754446\tvalid_1's binary_logloss: 0.247847\n",
      "[302]\ttraining's auc: 0.812689\ttraining's binary_logloss: 0.234168\tvalid_1's auc: 0.754471\tvalid_1's binary_logloss: 0.247821\n",
      "[303]\ttraining's auc: 0.812804\ttraining's binary_logloss: 0.234104\tvalid_1's auc: 0.754492\tvalid_1's binary_logloss: 0.247802\n",
      "[304]\ttraining's auc: 0.812978\ttraining's binary_logloss: 0.234041\tvalid_1's auc: 0.754495\tvalid_1's binary_logloss: 0.247791\n",
      "[305]\ttraining's auc: 0.813098\ttraining's binary_logloss: 0.233979\tvalid_1's auc: 0.75457\tvalid_1's binary_logloss: 0.24776\n",
      "[306]\ttraining's auc: 0.813231\ttraining's binary_logloss: 0.233917\tvalid_1's auc: 0.754552\tvalid_1's binary_logloss: 0.247747\n",
      "[307]\ttraining's auc: 0.813366\ttraining's binary_logloss: 0.233855\tvalid_1's auc: 0.754574\tvalid_1's binary_logloss: 0.247727\n",
      "[308]\ttraining's auc: 0.813502\ttraining's binary_logloss: 0.233794\tvalid_1's auc: 0.754601\tvalid_1's binary_logloss: 0.247708\n",
      "[309]\ttraining's auc: 0.813643\ttraining's binary_logloss: 0.233729\tvalid_1's auc: 0.754629\tvalid_1's binary_logloss: 0.247689\n",
      "[310]\ttraining's auc: 0.813784\ttraining's binary_logloss: 0.233668\tvalid_1's auc: 0.754679\tvalid_1's binary_logloss: 0.247666\n",
      "[311]\ttraining's auc: 0.813907\ttraining's binary_logloss: 0.233604\tvalid_1's auc: 0.754748\tvalid_1's binary_logloss: 0.247637\n",
      "[312]\ttraining's auc: 0.814009\ttraining's binary_logloss: 0.233544\tvalid_1's auc: 0.754761\tvalid_1's binary_logloss: 0.247622\n",
      "[313]\ttraining's auc: 0.814125\ttraining's binary_logloss: 0.233484\tvalid_1's auc: 0.754776\tvalid_1's binary_logloss: 0.247609\n",
      "[314]\ttraining's auc: 0.814265\ttraining's binary_logloss: 0.233428\tvalid_1's auc: 0.754816\tvalid_1's binary_logloss: 0.247593\n",
      "[315]\ttraining's auc: 0.814383\ttraining's binary_logloss: 0.233371\tvalid_1's auc: 0.754827\tvalid_1's binary_logloss: 0.247573\n",
      "[316]\ttraining's auc: 0.814514\ttraining's binary_logloss: 0.233304\tvalid_1's auc: 0.754914\tvalid_1's binary_logloss: 0.247546\n",
      "[317]\ttraining's auc: 0.814618\ttraining's binary_logloss: 0.233244\tvalid_1's auc: 0.754969\tvalid_1's binary_logloss: 0.247519\n",
      "[318]\ttraining's auc: 0.814747\ttraining's binary_logloss: 0.233187\tvalid_1's auc: 0.755009\tvalid_1's binary_logloss: 0.247503\n",
      "[319]\ttraining's auc: 0.814831\ttraining's binary_logloss: 0.233129\tvalid_1's auc: 0.755008\tvalid_1's binary_logloss: 0.247488\n",
      "[320]\ttraining's auc: 0.814922\ttraining's binary_logloss: 0.23307\tvalid_1's auc: 0.754998\tvalid_1's binary_logloss: 0.247479\n",
      "[321]\ttraining's auc: 0.815054\ttraining's binary_logloss: 0.23301\tvalid_1's auc: 0.755043\tvalid_1's binary_logloss: 0.247456\n",
      "[322]\ttraining's auc: 0.815204\ttraining's binary_logloss: 0.232951\tvalid_1's auc: 0.755078\tvalid_1's binary_logloss: 0.247437\n",
      "[323]\ttraining's auc: 0.815359\ttraining's binary_logloss: 0.232889\tvalid_1's auc: 0.755115\tvalid_1's binary_logloss: 0.247421\n",
      "[324]\ttraining's auc: 0.815485\ttraining's binary_logloss: 0.232832\tvalid_1's auc: 0.755159\tvalid_1's binary_logloss: 0.247403\n",
      "[325]\ttraining's auc: 0.815583\ttraining's binary_logloss: 0.232779\tvalid_1's auc: 0.755176\tvalid_1's binary_logloss: 0.247388\n",
      "[326]\ttraining's auc: 0.815708\ttraining's binary_logloss: 0.232719\tvalid_1's auc: 0.755239\tvalid_1's binary_logloss: 0.247369\n",
      "[327]\ttraining's auc: 0.815853\ttraining's binary_logloss: 0.232657\tvalid_1's auc: 0.755265\tvalid_1's binary_logloss: 0.247355\n",
      "[328]\ttraining's auc: 0.815941\ttraining's binary_logloss: 0.232599\tvalid_1's auc: 0.755226\tvalid_1's binary_logloss: 0.247349\n",
      "[329]\ttraining's auc: 0.816048\ttraining's binary_logloss: 0.232539\tvalid_1's auc: 0.755187\tvalid_1's binary_logloss: 0.247346\n",
      "[330]\ttraining's auc: 0.816187\ttraining's binary_logloss: 0.232479\tvalid_1's auc: 0.755249\tvalid_1's binary_logloss: 0.247321\n",
      "[331]\ttraining's auc: 0.816352\ttraining's binary_logloss: 0.232416\tvalid_1's auc: 0.755284\tvalid_1's binary_logloss: 0.247307\n",
      "[332]\ttraining's auc: 0.81646\ttraining's binary_logloss: 0.232366\tvalid_1's auc: 0.755276\tvalid_1's binary_logloss: 0.247295\n",
      "[333]\ttraining's auc: 0.816592\ttraining's binary_logloss: 0.232308\tvalid_1's auc: 0.755301\tvalid_1's binary_logloss: 0.247279\n",
      "[334]\ttraining's auc: 0.816724\ttraining's binary_logloss: 0.23225\tvalid_1's auc: 0.755346\tvalid_1's binary_logloss: 0.247263\n",
      "[335]\ttraining's auc: 0.816838\ttraining's binary_logloss: 0.232189\tvalid_1's auc: 0.755384\tvalid_1's binary_logloss: 0.247243\n",
      "[336]\ttraining's auc: 0.816975\ttraining's binary_logloss: 0.23213\tvalid_1's auc: 0.755386\tvalid_1's binary_logloss: 0.247235\n",
      "[337]\ttraining's auc: 0.817057\ttraining's binary_logloss: 0.232071\tvalid_1's auc: 0.755391\tvalid_1's binary_logloss: 0.247222\n",
      "[338]\ttraining's auc: 0.817181\ttraining's binary_logloss: 0.232016\tvalid_1's auc: 0.755446\tvalid_1's binary_logloss: 0.247204\n",
      "[339]\ttraining's auc: 0.817327\ttraining's binary_logloss: 0.231948\tvalid_1's auc: 0.7555\tvalid_1's binary_logloss: 0.247177\n",
      "[340]\ttraining's auc: 0.817489\ttraining's binary_logloss: 0.23189\tvalid_1's auc: 0.75557\tvalid_1's binary_logloss: 0.247157\n",
      "[341]\ttraining's auc: 0.817605\ttraining's binary_logloss: 0.231831\tvalid_1's auc: 0.755598\tvalid_1's binary_logloss: 0.247142\n",
      "[342]\ttraining's auc: 0.817717\ttraining's binary_logloss: 0.231775\tvalid_1's auc: 0.755642\tvalid_1's binary_logloss: 0.247124\n",
      "[343]\ttraining's auc: 0.817832\ttraining's binary_logloss: 0.231717\tvalid_1's auc: 0.755702\tvalid_1's binary_logloss: 0.2471\n",
      "[344]\ttraining's auc: 0.817962\ttraining's binary_logloss: 0.231659\tvalid_1's auc: 0.755739\tvalid_1's binary_logloss: 0.247084\n",
      "[345]\ttraining's auc: 0.818086\ttraining's binary_logloss: 0.2316\tvalid_1's auc: 0.755819\tvalid_1's binary_logloss: 0.24706\n",
      "[346]\ttraining's auc: 0.818228\ttraining's binary_logloss: 0.231538\tvalid_1's auc: 0.755854\tvalid_1's binary_logloss: 0.247041\n",
      "[347]\ttraining's auc: 0.818368\ttraining's binary_logloss: 0.231479\tvalid_1's auc: 0.755908\tvalid_1's binary_logloss: 0.247023\n",
      "[348]\ttraining's auc: 0.818479\ttraining's binary_logloss: 0.231426\tvalid_1's auc: 0.75591\tvalid_1's binary_logloss: 0.247011\n",
      "[349]\ttraining's auc: 0.818576\ttraining's binary_logloss: 0.231369\tvalid_1's auc: 0.755931\tvalid_1's binary_logloss: 0.246998\n",
      "[350]\ttraining's auc: 0.818689\ttraining's binary_logloss: 0.231308\tvalid_1's auc: 0.755981\tvalid_1's binary_logloss: 0.24698\n",
      "[351]\ttraining's auc: 0.818829\ttraining's binary_logloss: 0.23125\tvalid_1's auc: 0.756012\tvalid_1's binary_logloss: 0.246966\n",
      "[352]\ttraining's auc: 0.818964\ttraining's binary_logloss: 0.231195\tvalid_1's auc: 0.756043\tvalid_1's binary_logloss: 0.246953\n",
      "[353]\ttraining's auc: 0.819081\ttraining's binary_logloss: 0.231135\tvalid_1's auc: 0.756085\tvalid_1's binary_logloss: 0.246934\n",
      "[354]\ttraining's auc: 0.819191\ttraining's binary_logloss: 0.231076\tvalid_1's auc: 0.756114\tvalid_1's binary_logloss: 0.246916\n",
      "[355]\ttraining's auc: 0.819285\ttraining's binary_logloss: 0.231024\tvalid_1's auc: 0.756118\tvalid_1's binary_logloss: 0.246901\n",
      "[356]\ttraining's auc: 0.819399\ttraining's binary_logloss: 0.230968\tvalid_1's auc: 0.756152\tvalid_1's binary_logloss: 0.246887\n",
      "[357]\ttraining's auc: 0.819538\ttraining's binary_logloss: 0.23091\tvalid_1's auc: 0.756188\tvalid_1's binary_logloss: 0.246875\n",
      "[358]\ttraining's auc: 0.819679\ttraining's binary_logloss: 0.230848\tvalid_1's auc: 0.756234\tvalid_1's binary_logloss: 0.246853\n",
      "[359]\ttraining's auc: 0.819818\ttraining's binary_logloss: 0.230793\tvalid_1's auc: 0.75628\tvalid_1's binary_logloss: 0.246837\n",
      "[360]\ttraining's auc: 0.819929\ttraining's binary_logloss: 0.230737\tvalid_1's auc: 0.756315\tvalid_1's binary_logloss: 0.24682\n",
      "[361]\ttraining's auc: 0.820035\ttraining's binary_logloss: 0.230684\tvalid_1's auc: 0.756346\tvalid_1's binary_logloss: 0.246805\n",
      "[362]\ttraining's auc: 0.820153\ttraining's binary_logloss: 0.230628\tvalid_1's auc: 0.756363\tvalid_1's binary_logloss: 0.246794\n",
      "[363]\ttraining's auc: 0.82026\ttraining's binary_logloss: 0.230576\tvalid_1's auc: 0.756417\tvalid_1's binary_logloss: 0.246771\n",
      "[364]\ttraining's auc: 0.820401\ttraining's binary_logloss: 0.230518\tvalid_1's auc: 0.756441\tvalid_1's binary_logloss: 0.24676\n",
      "[365]\ttraining's auc: 0.820525\ttraining's binary_logloss: 0.230466\tvalid_1's auc: 0.756477\tvalid_1's binary_logloss: 0.246747\n",
      "[366]\ttraining's auc: 0.82066\ttraining's binary_logloss: 0.230405\tvalid_1's auc: 0.756519\tvalid_1's binary_logloss: 0.246728\n",
      "[367]\ttraining's auc: 0.820776\ttraining's binary_logloss: 0.230349\tvalid_1's auc: 0.756546\tvalid_1's binary_logloss: 0.246708\n",
      "[368]\ttraining's auc: 0.820879\ttraining's binary_logloss: 0.230295\tvalid_1's auc: 0.756561\tvalid_1's binary_logloss: 0.246697\n",
      "[369]\ttraining's auc: 0.82099\ttraining's binary_logloss: 0.230239\tvalid_1's auc: 0.756556\tvalid_1's binary_logloss: 0.24669\n",
      "[370]\ttraining's auc: 0.821127\ttraining's binary_logloss: 0.230181\tvalid_1's auc: 0.756592\tvalid_1's binary_logloss: 0.246673\n",
      "[371]\ttraining's auc: 0.821256\ttraining's binary_logloss: 0.230126\tvalid_1's auc: 0.756644\tvalid_1's binary_logloss: 0.246658\n",
      "[372]\ttraining's auc: 0.821364\ttraining's binary_logloss: 0.230081\tvalid_1's auc: 0.756676\tvalid_1's binary_logloss: 0.246643\n",
      "[373]\ttraining's auc: 0.821484\ttraining's binary_logloss: 0.230026\tvalid_1's auc: 0.756672\tvalid_1's binary_logloss: 0.246636\n",
      "[374]\ttraining's auc: 0.821607\ttraining's binary_logloss: 0.229969\tvalid_1's auc: 0.756699\tvalid_1's binary_logloss: 0.246623\n",
      "[375]\ttraining's auc: 0.821716\ttraining's binary_logloss: 0.229918\tvalid_1's auc: 0.756736\tvalid_1's binary_logloss: 0.246605\n",
      "[376]\ttraining's auc: 0.821847\ttraining's binary_logloss: 0.229857\tvalid_1's auc: 0.756761\tvalid_1's binary_logloss: 0.246585\n",
      "[377]\ttraining's auc: 0.821995\ttraining's binary_logloss: 0.229794\tvalid_1's auc: 0.756777\tvalid_1's binary_logloss: 0.246571\n",
      "[378]\ttraining's auc: 0.822106\ttraining's binary_logloss: 0.229746\tvalid_1's auc: 0.756811\tvalid_1's binary_logloss: 0.246563\n",
      "[379]\ttraining's auc: 0.822242\ttraining's binary_logloss: 0.229686\tvalid_1's auc: 0.756844\tvalid_1's binary_logloss: 0.246547\n",
      "[380]\ttraining's auc: 0.822374\ttraining's binary_logloss: 0.229635\tvalid_1's auc: 0.756854\tvalid_1's binary_logloss: 0.246538\n",
      "[381]\ttraining's auc: 0.82249\ttraining's binary_logloss: 0.229579\tvalid_1's auc: 0.756877\tvalid_1's binary_logloss: 0.246522\n",
      "[382]\ttraining's auc: 0.822624\ttraining's binary_logloss: 0.229527\tvalid_1's auc: 0.756932\tvalid_1's binary_logloss: 0.246508\n",
      "[383]\ttraining's auc: 0.822772\ttraining's binary_logloss: 0.229465\tvalid_1's auc: 0.756975\tvalid_1's binary_logloss: 0.246491\n",
      "[384]\ttraining's auc: 0.822873\ttraining's binary_logloss: 0.229413\tvalid_1's auc: 0.756986\tvalid_1's binary_logloss: 0.246479\n",
      "[385]\ttraining's auc: 0.823001\ttraining's binary_logloss: 0.229351\tvalid_1's auc: 0.757006\tvalid_1's binary_logloss: 0.246468\n",
      "[386]\ttraining's auc: 0.823135\ttraining's binary_logloss: 0.229294\tvalid_1's auc: 0.757036\tvalid_1's binary_logloss: 0.24645\n",
      "[387]\ttraining's auc: 0.823232\ttraining's binary_logloss: 0.229238\tvalid_1's auc: 0.757111\tvalid_1's binary_logloss: 0.246423\n",
      "[388]\ttraining's auc: 0.823383\ttraining's binary_logloss: 0.229177\tvalid_1's auc: 0.757187\tvalid_1's binary_logloss: 0.246402\n",
      "[389]\ttraining's auc: 0.823479\ttraining's binary_logloss: 0.229127\tvalid_1's auc: 0.757206\tvalid_1's binary_logloss: 0.246388\n",
      "[390]\ttraining's auc: 0.823587\ttraining's binary_logloss: 0.229073\tvalid_1's auc: 0.757235\tvalid_1's binary_logloss: 0.246374\n",
      "[391]\ttraining's auc: 0.823684\ttraining's binary_logloss: 0.229022\tvalid_1's auc: 0.757256\tvalid_1's binary_logloss: 0.246363\n",
      "[392]\ttraining's auc: 0.823786\ttraining's binary_logloss: 0.228974\tvalid_1's auc: 0.757282\tvalid_1's binary_logloss: 0.246347\n",
      "[393]\ttraining's auc: 0.823927\ttraining's binary_logloss: 0.228914\tvalid_1's auc: 0.757284\tvalid_1's binary_logloss: 0.246339\n",
      "[394]\ttraining's auc: 0.824046\ttraining's binary_logloss: 0.228856\tvalid_1's auc: 0.757307\tvalid_1's binary_logloss: 0.246328\n",
      "[395]\ttraining's auc: 0.82416\ttraining's binary_logloss: 0.2288\tvalid_1's auc: 0.757342\tvalid_1's binary_logloss: 0.246314\n",
      "[396]\ttraining's auc: 0.824285\ttraining's binary_logloss: 0.228747\tvalid_1's auc: 0.757399\tvalid_1's binary_logloss: 0.246297\n",
      "[397]\ttraining's auc: 0.8244\ttraining's binary_logloss: 0.228697\tvalid_1's auc: 0.757411\tvalid_1's binary_logloss: 0.246294\n",
      "[398]\ttraining's auc: 0.824487\ttraining's binary_logloss: 0.228651\tvalid_1's auc: 0.757435\tvalid_1's binary_logloss: 0.246286\n",
      "[399]\ttraining's auc: 0.824602\ttraining's binary_logloss: 0.228595\tvalid_1's auc: 0.757452\tvalid_1's binary_logloss: 0.246278\n",
      "[400]\ttraining's auc: 0.824754\ttraining's binary_logloss: 0.228534\tvalid_1's auc: 0.757483\tvalid_1's binary_logloss: 0.246263\n",
      "[401]\ttraining's auc: 0.82491\ttraining's binary_logloss: 0.228479\tvalid_1's auc: 0.757521\tvalid_1's binary_logloss: 0.246248\n",
      "[402]\ttraining's auc: 0.825019\ttraining's binary_logloss: 0.228432\tvalid_1's auc: 0.757533\tvalid_1's binary_logloss: 0.246242\n",
      "[403]\ttraining's auc: 0.825131\ttraining's binary_logloss: 0.228377\tvalid_1's auc: 0.757536\tvalid_1's binary_logloss: 0.246236\n",
      "[404]\ttraining's auc: 0.825266\ttraining's binary_logloss: 0.228322\tvalid_1's auc: 0.75757\tvalid_1's binary_logloss: 0.246224\n",
      "[405]\ttraining's auc: 0.825383\ttraining's binary_logloss: 0.228272\tvalid_1's auc: 0.757632\tvalid_1's binary_logloss: 0.246203\n",
      "[406]\ttraining's auc: 0.825517\ttraining's binary_logloss: 0.228215\tvalid_1's auc: 0.757673\tvalid_1's binary_logloss: 0.246187\n",
      "[407]\ttraining's auc: 0.825625\ttraining's binary_logloss: 0.228162\tvalid_1's auc: 0.75771\tvalid_1's binary_logloss: 0.246172\n",
      "[408]\ttraining's auc: 0.825757\ttraining's binary_logloss: 0.228104\tvalid_1's auc: 0.757721\tvalid_1's binary_logloss: 0.246164\n",
      "[409]\ttraining's auc: 0.825872\ttraining's binary_logloss: 0.228059\tvalid_1's auc: 0.757757\tvalid_1's binary_logloss: 0.246152\n",
      "[410]\ttraining's auc: 0.825979\ttraining's binary_logloss: 0.228004\tvalid_1's auc: 0.75775\tvalid_1's binary_logloss: 0.246147\n",
      "[411]\ttraining's auc: 0.826104\ttraining's binary_logloss: 0.227948\tvalid_1's auc: 0.757774\tvalid_1's binary_logloss: 0.246137\n",
      "[412]\ttraining's auc: 0.826228\ttraining's binary_logloss: 0.227896\tvalid_1's auc: 0.757793\tvalid_1's binary_logloss: 0.246125\n",
      "[413]\ttraining's auc: 0.826341\ttraining's binary_logloss: 0.227847\tvalid_1's auc: 0.757845\tvalid_1's binary_logloss: 0.246111\n",
      "[414]\ttraining's auc: 0.826455\ttraining's binary_logloss: 0.227794\tvalid_1's auc: 0.757898\tvalid_1's binary_logloss: 0.246092\n",
      "[415]\ttraining's auc: 0.82659\ttraining's binary_logloss: 0.227742\tvalid_1's auc: 0.757959\tvalid_1's binary_logloss: 0.246074\n",
      "[416]\ttraining's auc: 0.826725\ttraining's binary_logloss: 0.227686\tvalid_1's auc: 0.758003\tvalid_1's binary_logloss: 0.246059\n",
      "[417]\ttraining's auc: 0.826854\ttraining's binary_logloss: 0.227633\tvalid_1's auc: 0.758044\tvalid_1's binary_logloss: 0.246049\n",
      "[418]\ttraining's auc: 0.826965\ttraining's binary_logloss: 0.227588\tvalid_1's auc: 0.758072\tvalid_1's binary_logloss: 0.246037\n",
      "[419]\ttraining's auc: 0.827073\ttraining's binary_logloss: 0.227543\tvalid_1's auc: 0.758072\tvalid_1's binary_logloss: 0.246031\n",
      "[420]\ttraining's auc: 0.827214\ttraining's binary_logloss: 0.227492\tvalid_1's auc: 0.758064\tvalid_1's binary_logloss: 0.246031\n",
      "[421]\ttraining's auc: 0.827329\ttraining's binary_logloss: 0.227447\tvalid_1's auc: 0.758102\tvalid_1's binary_logloss: 0.246018\n",
      "[422]\ttraining's auc: 0.827436\ttraining's binary_logloss: 0.227398\tvalid_1's auc: 0.75812\tvalid_1's binary_logloss: 0.246008\n",
      "[423]\ttraining's auc: 0.827586\ttraining's binary_logloss: 0.227344\tvalid_1's auc: 0.758176\tvalid_1's binary_logloss: 0.245992\n",
      "[424]\ttraining's auc: 0.827677\ttraining's binary_logloss: 0.227292\tvalid_1's auc: 0.75823\tvalid_1's binary_logloss: 0.245969\n",
      "[425]\ttraining's auc: 0.827788\ttraining's binary_logloss: 0.227243\tvalid_1's auc: 0.758245\tvalid_1's binary_logloss: 0.245963\n",
      "[426]\ttraining's auc: 0.827885\ttraining's binary_logloss: 0.227198\tvalid_1's auc: 0.758285\tvalid_1's binary_logloss: 0.24595\n",
      "[427]\ttraining's auc: 0.828008\ttraining's binary_logloss: 0.227144\tvalid_1's auc: 0.758336\tvalid_1's binary_logloss: 0.245932\n",
      "[428]\ttraining's auc: 0.828123\ttraining's binary_logloss: 0.227095\tvalid_1's auc: 0.758361\tvalid_1's binary_logloss: 0.24592\n",
      "[429]\ttraining's auc: 0.828236\ttraining's binary_logloss: 0.227052\tvalid_1's auc: 0.758391\tvalid_1's binary_logloss: 0.245909\n",
      "[430]\ttraining's auc: 0.828355\ttraining's binary_logloss: 0.227008\tvalid_1's auc: 0.758429\tvalid_1's binary_logloss: 0.245897\n",
      "[431]\ttraining's auc: 0.828467\ttraining's binary_logloss: 0.226956\tvalid_1's auc: 0.75843\tvalid_1's binary_logloss: 0.245892\n",
      "[432]\ttraining's auc: 0.828611\ttraining's binary_logloss: 0.2269\tvalid_1's auc: 0.758451\tvalid_1's binary_logloss: 0.245883\n",
      "[433]\ttraining's auc: 0.82875\ttraining's binary_logloss: 0.226849\tvalid_1's auc: 0.758474\tvalid_1's binary_logloss: 0.245876\n",
      "[434]\ttraining's auc: 0.828883\ttraining's binary_logloss: 0.226797\tvalid_1's auc: 0.758502\tvalid_1's binary_logloss: 0.245864\n",
      "[435]\ttraining's auc: 0.829004\ttraining's binary_logloss: 0.226743\tvalid_1's auc: 0.75853\tvalid_1's binary_logloss: 0.245856\n",
      "[436]\ttraining's auc: 0.829136\ttraining's binary_logloss: 0.22669\tvalid_1's auc: 0.758577\tvalid_1's binary_logloss: 0.245842\n",
      "[437]\ttraining's auc: 0.829262\ttraining's binary_logloss: 0.226639\tvalid_1's auc: 0.758598\tvalid_1's binary_logloss: 0.245833\n",
      "[438]\ttraining's auc: 0.82937\ttraining's binary_logloss: 0.22659\tvalid_1's auc: 0.758627\tvalid_1's binary_logloss: 0.245821\n",
      "[439]\ttraining's auc: 0.829468\ttraining's binary_logloss: 0.22655\tvalid_1's auc: 0.758634\tvalid_1's binary_logloss: 0.245816\n",
      "[440]\ttraining's auc: 0.829596\ttraining's binary_logloss: 0.226496\tvalid_1's auc: 0.758669\tvalid_1's binary_logloss: 0.245804\n",
      "[441]\ttraining's auc: 0.829742\ttraining's binary_logloss: 0.22644\tvalid_1's auc: 0.758672\tvalid_1's binary_logloss: 0.245798\n",
      "[442]\ttraining's auc: 0.829859\ttraining's binary_logloss: 0.226389\tvalid_1's auc: 0.758695\tvalid_1's binary_logloss: 0.245791\n",
      "[443]\ttraining's auc: 0.829974\ttraining's binary_logloss: 0.226339\tvalid_1's auc: 0.758714\tvalid_1's binary_logloss: 0.245782\n",
      "[444]\ttraining's auc: 0.830102\ttraining's binary_logloss: 0.226283\tvalid_1's auc: 0.758713\tvalid_1's binary_logloss: 0.245777\n",
      "[445]\ttraining's auc: 0.83022\ttraining's binary_logloss: 0.22623\tvalid_1's auc: 0.758741\tvalid_1's binary_logloss: 0.245768\n",
      "[446]\ttraining's auc: 0.830371\ttraining's binary_logloss: 0.226176\tvalid_1's auc: 0.758785\tvalid_1's binary_logloss: 0.245755\n",
      "[447]\ttraining's auc: 0.830486\ttraining's binary_logloss: 0.226123\tvalid_1's auc: 0.758815\tvalid_1's binary_logloss: 0.245746\n",
      "[448]\ttraining's auc: 0.830611\ttraining's binary_logloss: 0.226075\tvalid_1's auc: 0.758844\tvalid_1's binary_logloss: 0.245738\n",
      "[449]\ttraining's auc: 0.830723\ttraining's binary_logloss: 0.226029\tvalid_1's auc: 0.758844\tvalid_1's binary_logloss: 0.245734\n",
      "[450]\ttraining's auc: 0.830862\ttraining's binary_logloss: 0.225979\tvalid_1's auc: 0.758873\tvalid_1's binary_logloss: 0.245725\n",
      "[451]\ttraining's auc: 0.83099\ttraining's binary_logloss: 0.225932\tvalid_1's auc: 0.758874\tvalid_1's binary_logloss: 0.245723\n",
      "[452]\ttraining's auc: 0.831102\ttraining's binary_logloss: 0.225883\tvalid_1's auc: 0.75889\tvalid_1's binary_logloss: 0.245718\n",
      "[453]\ttraining's auc: 0.831215\ttraining's binary_logloss: 0.225837\tvalid_1's auc: 0.758875\tvalid_1's binary_logloss: 0.245716\n",
      "[454]\ttraining's auc: 0.831331\ttraining's binary_logloss: 0.225784\tvalid_1's auc: 0.758902\tvalid_1's binary_logloss: 0.245708\n",
      "[455]\ttraining's auc: 0.831435\ttraining's binary_logloss: 0.225742\tvalid_1's auc: 0.758937\tvalid_1's binary_logloss: 0.245698\n",
      "[456]\ttraining's auc: 0.831552\ttraining's binary_logloss: 0.225693\tvalid_1's auc: 0.758952\tvalid_1's binary_logloss: 0.245689\n",
      "[457]\ttraining's auc: 0.831658\ttraining's binary_logloss: 0.22565\tvalid_1's auc: 0.758963\tvalid_1's binary_logloss: 0.245682\n",
      "[458]\ttraining's auc: 0.83177\ttraining's binary_logloss: 0.225601\tvalid_1's auc: 0.758989\tvalid_1's binary_logloss: 0.245676\n",
      "[459]\ttraining's auc: 0.831907\ttraining's binary_logloss: 0.225553\tvalid_1's auc: 0.759\tvalid_1's binary_logloss: 0.245668\n",
      "[460]\ttraining's auc: 0.83204\ttraining's binary_logloss: 0.225502\tvalid_1's auc: 0.75901\tvalid_1's binary_logloss: 0.245661\n",
      "[461]\ttraining's auc: 0.832161\ttraining's binary_logloss: 0.225453\tvalid_1's auc: 0.759037\tvalid_1's binary_logloss: 0.245653\n",
      "[462]\ttraining's auc: 0.832278\ttraining's binary_logloss: 0.225406\tvalid_1's auc: 0.759035\tvalid_1's binary_logloss: 0.245649\n",
      "[463]\ttraining's auc: 0.832374\ttraining's binary_logloss: 0.225355\tvalid_1's auc: 0.759027\tvalid_1's binary_logloss: 0.24565\n",
      "[464]\ttraining's auc: 0.832481\ttraining's binary_logloss: 0.225308\tvalid_1's auc: 0.759051\tvalid_1's binary_logloss: 0.245638\n",
      "[465]\ttraining's auc: 0.832602\ttraining's binary_logloss: 0.225262\tvalid_1's auc: 0.759083\tvalid_1's binary_logloss: 0.245627\n",
      "[466]\ttraining's auc: 0.832717\ttraining's binary_logloss: 0.225211\tvalid_1's auc: 0.759117\tvalid_1's binary_logloss: 0.245619\n",
      "[467]\ttraining's auc: 0.832829\ttraining's binary_logloss: 0.225168\tvalid_1's auc: 0.759147\tvalid_1's binary_logloss: 0.245606\n",
      "[468]\ttraining's auc: 0.832924\ttraining's binary_logloss: 0.225127\tvalid_1's auc: 0.759191\tvalid_1's binary_logloss: 0.245595\n",
      "[469]\ttraining's auc: 0.833025\ttraining's binary_logloss: 0.225084\tvalid_1's auc: 0.759204\tvalid_1's binary_logloss: 0.245592\n",
      "[470]\ttraining's auc: 0.833143\ttraining's binary_logloss: 0.225033\tvalid_1's auc: 0.759209\tvalid_1's binary_logloss: 0.24559\n",
      "[471]\ttraining's auc: 0.833254\ttraining's binary_logloss: 0.22498\tvalid_1's auc: 0.759234\tvalid_1's binary_logloss: 0.245577\n",
      "[472]\ttraining's auc: 0.833365\ttraining's binary_logloss: 0.224929\tvalid_1's auc: 0.759255\tvalid_1's binary_logloss: 0.245569\n",
      "[473]\ttraining's auc: 0.833489\ttraining's binary_logloss: 0.22488\tvalid_1's auc: 0.759262\tvalid_1's binary_logloss: 0.245563\n",
      "[474]\ttraining's auc: 0.833641\ttraining's binary_logloss: 0.224822\tvalid_1's auc: 0.759325\tvalid_1's binary_logloss: 0.245543\n",
      "[475]\ttraining's auc: 0.833734\ttraining's binary_logloss: 0.22478\tvalid_1's auc: 0.759318\tvalid_1's binary_logloss: 0.245539\n",
      "[476]\ttraining's auc: 0.833863\ttraining's binary_logloss: 0.224729\tvalid_1's auc: 0.759337\tvalid_1's binary_logloss: 0.245533\n",
      "[477]\ttraining's auc: 0.833997\ttraining's binary_logloss: 0.224681\tvalid_1's auc: 0.759345\tvalid_1's binary_logloss: 0.245525\n",
      "[478]\ttraining's auc: 0.8341\ttraining's binary_logloss: 0.224634\tvalid_1's auc: 0.759372\tvalid_1's binary_logloss: 0.245517\n",
      "[479]\ttraining's auc: 0.834194\ttraining's binary_logloss: 0.224591\tvalid_1's auc: 0.759418\tvalid_1's binary_logloss: 0.245504\n",
      "[480]\ttraining's auc: 0.834332\ttraining's binary_logloss: 0.224542\tvalid_1's auc: 0.759442\tvalid_1's binary_logloss: 0.245495\n",
      "[481]\ttraining's auc: 0.834451\ttraining's binary_logloss: 0.224491\tvalid_1's auc: 0.759482\tvalid_1's binary_logloss: 0.245485\n",
      "[482]\ttraining's auc: 0.834566\ttraining's binary_logloss: 0.224443\tvalid_1's auc: 0.759484\tvalid_1's binary_logloss: 0.245483\n",
      "[483]\ttraining's auc: 0.834675\ttraining's binary_logloss: 0.2244\tvalid_1's auc: 0.759508\tvalid_1's binary_logloss: 0.245474\n",
      "[484]\ttraining's auc: 0.834783\ttraining's binary_logloss: 0.224352\tvalid_1's auc: 0.759548\tvalid_1's binary_logloss: 0.245464\n",
      "[485]\ttraining's auc: 0.83489\ttraining's binary_logloss: 0.224307\tvalid_1's auc: 0.759577\tvalid_1's binary_logloss: 0.245455\n",
      "[486]\ttraining's auc: 0.834989\ttraining's binary_logloss: 0.224258\tvalid_1's auc: 0.759587\tvalid_1's binary_logloss: 0.245452\n",
      "[487]\ttraining's auc: 0.835099\ttraining's binary_logloss: 0.224218\tvalid_1's auc: 0.759621\tvalid_1's binary_logloss: 0.245443\n",
      "[488]\ttraining's auc: 0.835209\ttraining's binary_logloss: 0.224172\tvalid_1's auc: 0.759633\tvalid_1's binary_logloss: 0.245438\n",
      "[489]\ttraining's auc: 0.835318\ttraining's binary_logloss: 0.224126\tvalid_1's auc: 0.759646\tvalid_1's binary_logloss: 0.245434\n",
      "[490]\ttraining's auc: 0.83544\ttraining's binary_logloss: 0.224079\tvalid_1's auc: 0.759652\tvalid_1's binary_logloss: 0.245428\n",
      "[491]\ttraining's auc: 0.835554\ttraining's binary_logloss: 0.224028\tvalid_1's auc: 0.759647\tvalid_1's binary_logloss: 0.245429\n",
      "[492]\ttraining's auc: 0.835645\ttraining's binary_logloss: 0.223986\tvalid_1's auc: 0.759651\tvalid_1's binary_logloss: 0.245428\n",
      "[493]\ttraining's auc: 0.835768\ttraining's binary_logloss: 0.223943\tvalid_1's auc: 0.759672\tvalid_1's binary_logloss: 0.24542\n",
      "[494]\ttraining's auc: 0.835882\ttraining's binary_logloss: 0.223894\tvalid_1's auc: 0.759712\tvalid_1's binary_logloss: 0.245409\n",
      "[495]\ttraining's auc: 0.835992\ttraining's binary_logloss: 0.223849\tvalid_1's auc: 0.759736\tvalid_1's binary_logloss: 0.245401\n",
      "[496]\ttraining's auc: 0.836096\ttraining's binary_logloss: 0.223806\tvalid_1's auc: 0.759771\tvalid_1's binary_logloss: 0.245391\n",
      "[497]\ttraining's auc: 0.836177\ttraining's binary_logloss: 0.223774\tvalid_1's auc: 0.759774\tvalid_1's binary_logloss: 0.245389\n",
      "[498]\ttraining's auc: 0.836278\ttraining's binary_logloss: 0.223732\tvalid_1's auc: 0.759823\tvalid_1's binary_logloss: 0.245374\n",
      "[499]\ttraining's auc: 0.836396\ttraining's binary_logloss: 0.223682\tvalid_1's auc: 0.759861\tvalid_1's binary_logloss: 0.245364\n",
      "[500]\ttraining's auc: 0.836532\ttraining's binary_logloss: 0.22363\tvalid_1's auc: 0.759875\tvalid_1's binary_logloss: 0.245354\n",
      "[501]\ttraining's auc: 0.836655\ttraining's binary_logloss: 0.223578\tvalid_1's auc: 0.759907\tvalid_1's binary_logloss: 0.245342\n",
      "[502]\ttraining's auc: 0.836757\ttraining's binary_logloss: 0.223535\tvalid_1's auc: 0.759946\tvalid_1's binary_logloss: 0.245333\n",
      "[503]\ttraining's auc: 0.836849\ttraining's binary_logloss: 0.223496\tvalid_1's auc: 0.759975\tvalid_1's binary_logloss: 0.245321\n",
      "[504]\ttraining's auc: 0.836947\ttraining's binary_logloss: 0.223451\tvalid_1's auc: 0.760006\tvalid_1's binary_logloss: 0.24531\n",
      "[505]\ttraining's auc: 0.837034\ttraining's binary_logloss: 0.223414\tvalid_1's auc: 0.76003\tvalid_1's binary_logloss: 0.245302\n",
      "[506]\ttraining's auc: 0.837154\ttraining's binary_logloss: 0.223364\tvalid_1's auc: 0.760037\tvalid_1's binary_logloss: 0.245299\n",
      "[507]\ttraining's auc: 0.837286\ttraining's binary_logloss: 0.223314\tvalid_1's auc: 0.760047\tvalid_1's binary_logloss: 0.245296\n",
      "[508]\ttraining's auc: 0.83739\ttraining's binary_logloss: 0.223268\tvalid_1's auc: 0.760072\tvalid_1's binary_logloss: 0.245287\n",
      "[509]\ttraining's auc: 0.837508\ttraining's binary_logloss: 0.223218\tvalid_1's auc: 0.760127\tvalid_1's binary_logloss: 0.245274\n",
      "[510]\ttraining's auc: 0.83762\ttraining's binary_logloss: 0.223172\tvalid_1's auc: 0.760129\tvalid_1's binary_logloss: 0.245266\n",
      "[511]\ttraining's auc: 0.837729\ttraining's binary_logloss: 0.223129\tvalid_1's auc: 0.760147\tvalid_1's binary_logloss: 0.245258\n",
      "[512]\ttraining's auc: 0.837854\ttraining's binary_logloss: 0.223079\tvalid_1's auc: 0.760173\tvalid_1's binary_logloss: 0.245248\n",
      "[513]\ttraining's auc: 0.837951\ttraining's binary_logloss: 0.223037\tvalid_1's auc: 0.760172\tvalid_1's binary_logloss: 0.245248\n",
      "[514]\ttraining's auc: 0.838054\ttraining's binary_logloss: 0.222994\tvalid_1's auc: 0.76019\tvalid_1's binary_logloss: 0.245241\n",
      "[515]\ttraining's auc: 0.838176\ttraining's binary_logloss: 0.222948\tvalid_1's auc: 0.760183\tvalid_1's binary_logloss: 0.245239\n",
      "[516]\ttraining's auc: 0.838303\ttraining's binary_logloss: 0.2229\tvalid_1's auc: 0.760232\tvalid_1's binary_logloss: 0.245226\n",
      "[517]\ttraining's auc: 0.838413\ttraining's binary_logloss: 0.222852\tvalid_1's auc: 0.760264\tvalid_1's binary_logloss: 0.245217\n",
      "[518]\ttraining's auc: 0.83847\ttraining's binary_logloss: 0.222825\tvalid_1's auc: 0.760282\tvalid_1's binary_logloss: 0.245211\n",
      "[519]\ttraining's auc: 0.838578\ttraining's binary_logloss: 0.222781\tvalid_1's auc: 0.760298\tvalid_1's binary_logloss: 0.245204\n",
      "[520]\ttraining's auc: 0.838695\ttraining's binary_logloss: 0.222736\tvalid_1's auc: 0.760305\tvalid_1's binary_logloss: 0.245205\n",
      "[521]\ttraining's auc: 0.838814\ttraining's binary_logloss: 0.222688\tvalid_1's auc: 0.760343\tvalid_1's binary_logloss: 0.245196\n",
      "[522]\ttraining's auc: 0.838923\ttraining's binary_logloss: 0.222647\tvalid_1's auc: 0.760337\tvalid_1's binary_logloss: 0.245197\n",
      "[523]\ttraining's auc: 0.839035\ttraining's binary_logloss: 0.222601\tvalid_1's auc: 0.760387\tvalid_1's binary_logloss: 0.245182\n",
      "[524]\ttraining's auc: 0.839129\ttraining's binary_logloss: 0.222564\tvalid_1's auc: 0.760399\tvalid_1's binary_logloss: 0.245179\n",
      "[525]\ttraining's auc: 0.839239\ttraining's binary_logloss: 0.222516\tvalid_1's auc: 0.760465\tvalid_1's binary_logloss: 0.245158\n",
      "[526]\ttraining's auc: 0.839343\ttraining's binary_logloss: 0.222472\tvalid_1's auc: 0.76049\tvalid_1's binary_logloss: 0.245153\n",
      "[527]\ttraining's auc: 0.839438\ttraining's binary_logloss: 0.222427\tvalid_1's auc: 0.760517\tvalid_1's binary_logloss: 0.245151\n",
      "[528]\ttraining's auc: 0.839557\ttraining's binary_logloss: 0.222382\tvalid_1's auc: 0.760546\tvalid_1's binary_logloss: 0.245142\n",
      "[529]\ttraining's auc: 0.839698\ttraining's binary_logloss: 0.22233\tvalid_1's auc: 0.760593\tvalid_1's binary_logloss: 0.245129\n",
      "[530]\ttraining's auc: 0.839808\ttraining's binary_logloss: 0.222282\tvalid_1's auc: 0.760623\tvalid_1's binary_logloss: 0.24512\n",
      "[531]\ttraining's auc: 0.839905\ttraining's binary_logloss: 0.222236\tvalid_1's auc: 0.76065\tvalid_1's binary_logloss: 0.245108\n",
      "[532]\ttraining's auc: 0.840003\ttraining's binary_logloss: 0.222191\tvalid_1's auc: 0.760636\tvalid_1's binary_logloss: 0.245109\n",
      "[533]\ttraining's auc: 0.840109\ttraining's binary_logloss: 0.222149\tvalid_1's auc: 0.76065\tvalid_1's binary_logloss: 0.245105\n",
      "[534]\ttraining's auc: 0.840147\ttraining's binary_logloss: 0.222126\tvalid_1's auc: 0.760649\tvalid_1's binary_logloss: 0.2451\n",
      "[535]\ttraining's auc: 0.840201\ttraining's binary_logloss: 0.222103\tvalid_1's auc: 0.760647\tvalid_1's binary_logloss: 0.2451\n",
      "[536]\ttraining's auc: 0.840292\ttraining's binary_logloss: 0.222063\tvalid_1's auc: 0.760672\tvalid_1's binary_logloss: 0.245091\n",
      "[537]\ttraining's auc: 0.840423\ttraining's binary_logloss: 0.222016\tvalid_1's auc: 0.760691\tvalid_1's binary_logloss: 0.245087\n",
      "[538]\ttraining's auc: 0.840532\ttraining's binary_logloss: 0.221969\tvalid_1's auc: 0.760731\tvalid_1's binary_logloss: 0.245075\n",
      "[539]\ttraining's auc: 0.840606\ttraining's binary_logloss: 0.221941\tvalid_1's auc: 0.760762\tvalid_1's binary_logloss: 0.245067\n",
      "[540]\ttraining's auc: 0.840722\ttraining's binary_logloss: 0.221898\tvalid_1's auc: 0.760771\tvalid_1's binary_logloss: 0.245063\n",
      "[541]\ttraining's auc: 0.840829\ttraining's binary_logloss: 0.221853\tvalid_1's auc: 0.76078\tvalid_1's binary_logloss: 0.245058\n",
      "[542]\ttraining's auc: 0.840925\ttraining's binary_logloss: 0.221808\tvalid_1's auc: 0.760802\tvalid_1's binary_logloss: 0.245051\n",
      "[543]\ttraining's auc: 0.840995\ttraining's binary_logloss: 0.221774\tvalid_1's auc: 0.760811\tvalid_1's binary_logloss: 0.245047\n",
      "[544]\ttraining's auc: 0.841097\ttraining's binary_logloss: 0.221733\tvalid_1's auc: 0.760816\tvalid_1's binary_logloss: 0.245043\n",
      "[545]\ttraining's auc: 0.841176\ttraining's binary_logloss: 0.221697\tvalid_1's auc: 0.760823\tvalid_1's binary_logloss: 0.245037\n",
      "[546]\ttraining's auc: 0.841269\ttraining's binary_logloss: 0.221657\tvalid_1's auc: 0.760825\tvalid_1's binary_logloss: 0.245036\n",
      "[547]\ttraining's auc: 0.841382\ttraining's binary_logloss: 0.221617\tvalid_1's auc: 0.760836\tvalid_1's binary_logloss: 0.245031\n",
      "[548]\ttraining's auc: 0.841511\ttraining's binary_logloss: 0.221564\tvalid_1's auc: 0.760866\tvalid_1's binary_logloss: 0.245026\n",
      "[549]\ttraining's auc: 0.841633\ttraining's binary_logloss: 0.221519\tvalid_1's auc: 0.760903\tvalid_1's binary_logloss: 0.24502\n",
      "[550]\ttraining's auc: 0.841756\ttraining's binary_logloss: 0.221472\tvalid_1's auc: 0.760894\tvalid_1's binary_logloss: 0.245014\n",
      "[551]\ttraining's auc: 0.841855\ttraining's binary_logloss: 0.22143\tvalid_1's auc: 0.760896\tvalid_1's binary_logloss: 0.245013\n",
      "[552]\ttraining's auc: 0.841934\ttraining's binary_logloss: 0.221396\tvalid_1's auc: 0.760929\tvalid_1's binary_logloss: 0.245005\n",
      "[553]\ttraining's auc: 0.842063\ttraining's binary_logloss: 0.221349\tvalid_1's auc: 0.760931\tvalid_1's binary_logloss: 0.244997\n",
      "[554]\ttraining's auc: 0.842172\ttraining's binary_logloss: 0.221308\tvalid_1's auc: 0.760964\tvalid_1's binary_logloss: 0.244987\n",
      "[555]\ttraining's auc: 0.842288\ttraining's binary_logloss: 0.221259\tvalid_1's auc: 0.761005\tvalid_1's binary_logloss: 0.244978\n",
      "[556]\ttraining's auc: 0.842365\ttraining's binary_logloss: 0.221228\tvalid_1's auc: 0.761032\tvalid_1's binary_logloss: 0.244972\n",
      "[557]\ttraining's auc: 0.842458\ttraining's binary_logloss: 0.221189\tvalid_1's auc: 0.761046\tvalid_1's binary_logloss: 0.24497\n",
      "[558]\ttraining's auc: 0.842566\ttraining's binary_logloss: 0.221146\tvalid_1's auc: 0.761082\tvalid_1's binary_logloss: 0.244958\n",
      "[559]\ttraining's auc: 0.842641\ttraining's binary_logloss: 0.221117\tvalid_1's auc: 0.761109\tvalid_1's binary_logloss: 0.24495\n",
      "[560]\ttraining's auc: 0.842752\ttraining's binary_logloss: 0.221072\tvalid_1's auc: 0.761124\tvalid_1's binary_logloss: 0.244945\n",
      "[561]\ttraining's auc: 0.842817\ttraining's binary_logloss: 0.221039\tvalid_1's auc: 0.761132\tvalid_1's binary_logloss: 0.244943\n",
      "[562]\ttraining's auc: 0.842915\ttraining's binary_logloss: 0.220996\tvalid_1's auc: 0.761147\tvalid_1's binary_logloss: 0.244936\n",
      "[563]\ttraining's auc: 0.842975\ttraining's binary_logloss: 0.220968\tvalid_1's auc: 0.761131\tvalid_1's binary_logloss: 0.244938\n",
      "[564]\ttraining's auc: 0.843019\ttraining's binary_logloss: 0.220947\tvalid_1's auc: 0.761128\tvalid_1's binary_logloss: 0.244938\n",
      "[565]\ttraining's auc: 0.843149\ttraining's binary_logloss: 0.220897\tvalid_1's auc: 0.76114\tvalid_1's binary_logloss: 0.244934\n",
      "[566]\ttraining's auc: 0.843234\ttraining's binary_logloss: 0.220861\tvalid_1's auc: 0.761137\tvalid_1's binary_logloss: 0.244932\n",
      "[567]\ttraining's auc: 0.843326\ttraining's binary_logloss: 0.220818\tvalid_1's auc: 0.761155\tvalid_1's binary_logloss: 0.24492\n",
      "[568]\ttraining's auc: 0.843423\ttraining's binary_logloss: 0.220776\tvalid_1's auc: 0.761182\tvalid_1's binary_logloss: 0.244915\n",
      "[569]\ttraining's auc: 0.843542\ttraining's binary_logloss: 0.22073\tvalid_1's auc: 0.76118\tvalid_1's binary_logloss: 0.244912\n",
      "[570]\ttraining's auc: 0.843644\ttraining's binary_logloss: 0.220685\tvalid_1's auc: 0.7612\tvalid_1's binary_logloss: 0.244905\n",
      "[571]\ttraining's auc: 0.843757\ttraining's binary_logloss: 0.22064\tvalid_1's auc: 0.761216\tvalid_1's binary_logloss: 0.244896\n",
      "[572]\ttraining's auc: 0.843911\ttraining's binary_logloss: 0.220589\tvalid_1's auc: 0.761248\tvalid_1's binary_logloss: 0.24489\n",
      "[573]\ttraining's auc: 0.844009\ttraining's binary_logloss: 0.220547\tvalid_1's auc: 0.761269\tvalid_1's binary_logloss: 0.24488\n",
      "[574]\ttraining's auc: 0.844107\ttraining's binary_logloss: 0.220507\tvalid_1's auc: 0.761274\tvalid_1's binary_logloss: 0.244879\n",
      "[575]\ttraining's auc: 0.844152\ttraining's binary_logloss: 0.220488\tvalid_1's auc: 0.76129\tvalid_1's binary_logloss: 0.244874\n",
      "[576]\ttraining's auc: 0.844261\ttraining's binary_logloss: 0.220444\tvalid_1's auc: 0.761313\tvalid_1's binary_logloss: 0.244864\n",
      "[577]\ttraining's auc: 0.844379\ttraining's binary_logloss: 0.220399\tvalid_1's auc: 0.761307\tvalid_1's binary_logloss: 0.244864\n",
      "[578]\ttraining's auc: 0.844494\ttraining's binary_logloss: 0.220354\tvalid_1's auc: 0.761325\tvalid_1's binary_logloss: 0.244859\n",
      "[579]\ttraining's auc: 0.84459\ttraining's binary_logloss: 0.220313\tvalid_1's auc: 0.761348\tvalid_1's binary_logloss: 0.244854\n",
      "[580]\ttraining's auc: 0.84468\ttraining's binary_logloss: 0.22028\tvalid_1's auc: 0.76134\tvalid_1's binary_logloss: 0.244856\n",
      "[581]\ttraining's auc: 0.844792\ttraining's binary_logloss: 0.220233\tvalid_1's auc: 0.761366\tvalid_1's binary_logloss: 0.244852\n",
      "[582]\ttraining's auc: 0.844869\ttraining's binary_logloss: 0.220205\tvalid_1's auc: 0.761365\tvalid_1's binary_logloss: 0.244849\n",
      "[583]\ttraining's auc: 0.844958\ttraining's binary_logloss: 0.220164\tvalid_1's auc: 0.761394\tvalid_1's binary_logloss: 0.24484\n",
      "[584]\ttraining's auc: 0.845063\ttraining's binary_logloss: 0.220118\tvalid_1's auc: 0.761421\tvalid_1's binary_logloss: 0.244829\n",
      "[585]\ttraining's auc: 0.845148\ttraining's binary_logloss: 0.220075\tvalid_1's auc: 0.761427\tvalid_1's binary_logloss: 0.244825\n",
      "[586]\ttraining's auc: 0.845284\ttraining's binary_logloss: 0.220029\tvalid_1's auc: 0.761434\tvalid_1's binary_logloss: 0.244823\n",
      "[587]\ttraining's auc: 0.845366\ttraining's binary_logloss: 0.219991\tvalid_1's auc: 0.761433\tvalid_1's binary_logloss: 0.244821\n",
      "[588]\ttraining's auc: 0.845483\ttraining's binary_logloss: 0.219946\tvalid_1's auc: 0.761441\tvalid_1's binary_logloss: 0.244819\n",
      "[589]\ttraining's auc: 0.845585\ttraining's binary_logloss: 0.219905\tvalid_1's auc: 0.76143\tvalid_1's binary_logloss: 0.24482\n",
      "[590]\ttraining's auc: 0.845709\ttraining's binary_logloss: 0.219859\tvalid_1's auc: 0.761446\tvalid_1's binary_logloss: 0.244815\n",
      "[591]\ttraining's auc: 0.845785\ttraining's binary_logloss: 0.219824\tvalid_1's auc: 0.761453\tvalid_1's binary_logloss: 0.244812\n",
      "[592]\ttraining's auc: 0.845845\ttraining's binary_logloss: 0.21979\tvalid_1's auc: 0.761485\tvalid_1's binary_logloss: 0.244806\n",
      "[593]\ttraining's auc: 0.845925\ttraining's binary_logloss: 0.219757\tvalid_1's auc: 0.761502\tvalid_1's binary_logloss: 0.2448\n",
      "[594]\ttraining's auc: 0.845985\ttraining's binary_logloss: 0.219729\tvalid_1's auc: 0.761511\tvalid_1's binary_logloss: 0.244798\n",
      "[595]\ttraining's auc: 0.846031\ttraining's binary_logloss: 0.219707\tvalid_1's auc: 0.761524\tvalid_1's binary_logloss: 0.244794\n",
      "[596]\ttraining's auc: 0.846171\ttraining's binary_logloss: 0.219658\tvalid_1's auc: 0.761518\tvalid_1's binary_logloss: 0.244796\n",
      "[597]\ttraining's auc: 0.846236\ttraining's binary_logloss: 0.219628\tvalid_1's auc: 0.761523\tvalid_1's binary_logloss: 0.244794\n",
      "[598]\ttraining's auc: 0.846318\ttraining's binary_logloss: 0.219592\tvalid_1's auc: 0.761533\tvalid_1's binary_logloss: 0.244795\n",
      "[599]\ttraining's auc: 0.846423\ttraining's binary_logloss: 0.219551\tvalid_1's auc: 0.76152\tvalid_1's binary_logloss: 0.244793\n",
      "[600]\ttraining's auc: 0.846532\ttraining's binary_logloss: 0.219505\tvalid_1's auc: 0.761565\tvalid_1's binary_logloss: 0.244785\n",
      "[601]\ttraining's auc: 0.846607\ttraining's binary_logloss: 0.219473\tvalid_1's auc: 0.761579\tvalid_1's binary_logloss: 0.244785\n",
      "[602]\ttraining's auc: 0.846688\ttraining's binary_logloss: 0.219434\tvalid_1's auc: 0.761596\tvalid_1's binary_logloss: 0.24478\n",
      "[603]\ttraining's auc: 0.846738\ttraining's binary_logloss: 0.219411\tvalid_1's auc: 0.761581\tvalid_1's binary_logloss: 0.244783\n",
      "[604]\ttraining's auc: 0.846847\ttraining's binary_logloss: 0.219365\tvalid_1's auc: 0.761575\tvalid_1's binary_logloss: 0.244784\n",
      "[605]\ttraining's auc: 0.84696\ttraining's binary_logloss: 0.21932\tvalid_1's auc: 0.761597\tvalid_1's binary_logloss: 0.244777\n",
      "[606]\ttraining's auc: 0.847068\ttraining's binary_logloss: 0.219276\tvalid_1's auc: 0.761588\tvalid_1's binary_logloss: 0.244776\n",
      "[607]\ttraining's auc: 0.847132\ttraining's binary_logloss: 0.219246\tvalid_1's auc: 0.761585\tvalid_1's binary_logloss: 0.244777\n",
      "[608]\ttraining's auc: 0.847248\ttraining's binary_logloss: 0.219198\tvalid_1's auc: 0.761604\tvalid_1's binary_logloss: 0.24477\n",
      "[609]\ttraining's auc: 0.847331\ttraining's binary_logloss: 0.219166\tvalid_1's auc: 0.761623\tvalid_1's binary_logloss: 0.244764\n",
      "[610]\ttraining's auc: 0.847425\ttraining's binary_logloss: 0.219132\tvalid_1's auc: 0.761629\tvalid_1's binary_logloss: 0.244762\n",
      "[611]\ttraining's auc: 0.847516\ttraining's binary_logloss: 0.219092\tvalid_1's auc: 0.76164\tvalid_1's binary_logloss: 0.244757\n",
      "[612]\ttraining's auc: 0.847632\ttraining's binary_logloss: 0.219048\tvalid_1's auc: 0.761648\tvalid_1's binary_logloss: 0.244756\n",
      "[613]\ttraining's auc: 0.847732\ttraining's binary_logloss: 0.219006\tvalid_1's auc: 0.761667\tvalid_1's binary_logloss: 0.244749\n",
      "[614]\ttraining's auc: 0.847798\ttraining's binary_logloss: 0.218975\tvalid_1's auc: 0.761684\tvalid_1's binary_logloss: 0.244744\n",
      "[615]\ttraining's auc: 0.847892\ttraining's binary_logloss: 0.218933\tvalid_1's auc: 0.761666\tvalid_1's binary_logloss: 0.244745\n",
      "[616]\ttraining's auc: 0.84798\ttraining's binary_logloss: 0.218898\tvalid_1's auc: 0.761693\tvalid_1's binary_logloss: 0.244739\n",
      "[617]\ttraining's auc: 0.848056\ttraining's binary_logloss: 0.218863\tvalid_1's auc: 0.761721\tvalid_1's binary_logloss: 0.244732\n",
      "[618]\ttraining's auc: 0.848101\ttraining's binary_logloss: 0.218845\tvalid_1's auc: 0.761725\tvalid_1's binary_logloss: 0.24473\n",
      "[619]\ttraining's auc: 0.848203\ttraining's binary_logloss: 0.218805\tvalid_1's auc: 0.761727\tvalid_1's binary_logloss: 0.244726\n",
      "[620]\ttraining's auc: 0.848288\ttraining's binary_logloss: 0.218767\tvalid_1's auc: 0.761737\tvalid_1's binary_logloss: 0.244719\n",
      "[621]\ttraining's auc: 0.848377\ttraining's binary_logloss: 0.218737\tvalid_1's auc: 0.761747\tvalid_1's binary_logloss: 0.244716\n",
      "[622]\ttraining's auc: 0.848478\ttraining's binary_logloss: 0.218693\tvalid_1's auc: 0.761766\tvalid_1's binary_logloss: 0.244712\n",
      "[623]\ttraining's auc: 0.848591\ttraining's binary_logloss: 0.218649\tvalid_1's auc: 0.76176\tvalid_1's binary_logloss: 0.244715\n",
      "[624]\ttraining's auc: 0.848718\ttraining's binary_logloss: 0.218601\tvalid_1's auc: 0.761791\tvalid_1's binary_logloss: 0.244709\n",
      "[625]\ttraining's auc: 0.848838\ttraining's binary_logloss: 0.218554\tvalid_1's auc: 0.761831\tvalid_1's binary_logloss: 0.244698\n",
      "[626]\ttraining's auc: 0.848912\ttraining's binary_logloss: 0.218518\tvalid_1's auc: 0.76185\tvalid_1's binary_logloss: 0.244694\n",
      "[627]\ttraining's auc: 0.849\ttraining's binary_logloss: 0.218482\tvalid_1's auc: 0.761868\tvalid_1's binary_logloss: 0.244689\n",
      "[628]\ttraining's auc: 0.849108\ttraining's binary_logloss: 0.218434\tvalid_1's auc: 0.761886\tvalid_1's binary_logloss: 0.244684\n",
      "[629]\ttraining's auc: 0.849176\ttraining's binary_logloss: 0.218407\tvalid_1's auc: 0.761884\tvalid_1's binary_logloss: 0.244679\n",
      "[630]\ttraining's auc: 0.849284\ttraining's binary_logloss: 0.218363\tvalid_1's auc: 0.761919\tvalid_1's binary_logloss: 0.24467\n",
      "[631]\ttraining's auc: 0.849341\ttraining's binary_logloss: 0.218337\tvalid_1's auc: 0.761926\tvalid_1's binary_logloss: 0.244669\n",
      "[632]\ttraining's auc: 0.849457\ttraining's binary_logloss: 0.218291\tvalid_1's auc: 0.761907\tvalid_1's binary_logloss: 0.24467\n",
      "[633]\ttraining's auc: 0.849553\ttraining's binary_logloss: 0.218249\tvalid_1's auc: 0.761893\tvalid_1's binary_logloss: 0.244669\n",
      "[634]\ttraining's auc: 0.849606\ttraining's binary_logloss: 0.218223\tvalid_1's auc: 0.761888\tvalid_1's binary_logloss: 0.24467\n",
      "[635]\ttraining's auc: 0.849694\ttraining's binary_logloss: 0.218178\tvalid_1's auc: 0.761915\tvalid_1's binary_logloss: 0.244661\n",
      "[636]\ttraining's auc: 0.849774\ttraining's binary_logloss: 0.218142\tvalid_1's auc: 0.761921\tvalid_1's binary_logloss: 0.244659\n",
      "[637]\ttraining's auc: 0.849847\ttraining's binary_logloss: 0.218109\tvalid_1's auc: 0.761917\tvalid_1's binary_logloss: 0.244661\n",
      "[638]\ttraining's auc: 0.849939\ttraining's binary_logloss: 0.218067\tvalid_1's auc: 0.761896\tvalid_1's binary_logloss: 0.244666\n",
      "[639]\ttraining's auc: 0.850044\ttraining's binary_logloss: 0.218024\tvalid_1's auc: 0.761909\tvalid_1's binary_logloss: 0.244663\n",
      "[640]\ttraining's auc: 0.850083\ttraining's binary_logloss: 0.218004\tvalid_1's auc: 0.76191\tvalid_1's binary_logloss: 0.244663\n",
      "[641]\ttraining's auc: 0.85018\ttraining's binary_logloss: 0.217962\tvalid_1's auc: 0.761919\tvalid_1's binary_logloss: 0.244663\n",
      "[642]\ttraining's auc: 0.850281\ttraining's binary_logloss: 0.217912\tvalid_1's auc: 0.761938\tvalid_1's binary_logloss: 0.244657\n",
      "[643]\ttraining's auc: 0.850385\ttraining's binary_logloss: 0.217875\tvalid_1's auc: 0.761943\tvalid_1's binary_logloss: 0.244653\n",
      "[644]\ttraining's auc: 0.850496\ttraining's binary_logloss: 0.21783\tvalid_1's auc: 0.761944\tvalid_1's binary_logloss: 0.244652\n",
      "[645]\ttraining's auc: 0.850583\ttraining's binary_logloss: 0.217794\tvalid_1's auc: 0.761959\tvalid_1's binary_logloss: 0.244646\n",
      "[646]\ttraining's auc: 0.850653\ttraining's binary_logloss: 0.21776\tvalid_1's auc: 0.761973\tvalid_1's binary_logloss: 0.24464\n",
      "[647]\ttraining's auc: 0.850755\ttraining's binary_logloss: 0.21772\tvalid_1's auc: 0.761978\tvalid_1's binary_logloss: 0.244637\n",
      "[648]\ttraining's auc: 0.850836\ttraining's binary_logloss: 0.217689\tvalid_1's auc: 0.76199\tvalid_1's binary_logloss: 0.244635\n",
      "[649]\ttraining's auc: 0.850914\ttraining's binary_logloss: 0.217658\tvalid_1's auc: 0.762004\tvalid_1's binary_logloss: 0.244634\n",
      "[650]\ttraining's auc: 0.851015\ttraining's binary_logloss: 0.217613\tvalid_1's auc: 0.761976\tvalid_1's binary_logloss: 0.244641\n",
      "[651]\ttraining's auc: 0.85112\ttraining's binary_logloss: 0.217568\tvalid_1's auc: 0.762006\tvalid_1's binary_logloss: 0.244633\n",
      "[652]\ttraining's auc: 0.851214\ttraining's binary_logloss: 0.217528\tvalid_1's auc: 0.762037\tvalid_1's binary_logloss: 0.244626\n",
      "[653]\ttraining's auc: 0.851296\ttraining's binary_logloss: 0.217498\tvalid_1's auc: 0.762061\tvalid_1's binary_logloss: 0.244618\n",
      "[654]\ttraining's auc: 0.851399\ttraining's binary_logloss: 0.21746\tvalid_1's auc: 0.76207\tvalid_1's binary_logloss: 0.244614\n",
      "[655]\ttraining's auc: 0.851446\ttraining's binary_logloss: 0.217442\tvalid_1's auc: 0.762063\tvalid_1's binary_logloss: 0.244615\n",
      "[656]\ttraining's auc: 0.851547\ttraining's binary_logloss: 0.217399\tvalid_1's auc: 0.762075\tvalid_1's binary_logloss: 0.244613\n",
      "[657]\ttraining's auc: 0.851622\ttraining's binary_logloss: 0.217373\tvalid_1's auc: 0.762081\tvalid_1's binary_logloss: 0.244612\n",
      "[658]\ttraining's auc: 0.851738\ttraining's binary_logloss: 0.217327\tvalid_1's auc: 0.762089\tvalid_1's binary_logloss: 0.244609\n",
      "[659]\ttraining's auc: 0.851845\ttraining's binary_logloss: 0.217282\tvalid_1's auc: 0.76206\tvalid_1's binary_logloss: 0.244614\n",
      "[660]\ttraining's auc: 0.851948\ttraining's binary_logloss: 0.217236\tvalid_1's auc: 0.762061\tvalid_1's binary_logloss: 0.244608\n",
      "[661]\ttraining's auc: 0.85203\ttraining's binary_logloss: 0.217208\tvalid_1's auc: 0.762073\tvalid_1's binary_logloss: 0.244605\n",
      "[662]\ttraining's auc: 0.852104\ttraining's binary_logloss: 0.217172\tvalid_1's auc: 0.762114\tvalid_1's binary_logloss: 0.244598\n",
      "[663]\ttraining's auc: 0.852194\ttraining's binary_logloss: 0.217135\tvalid_1's auc: 0.762116\tvalid_1's binary_logloss: 0.244598\n",
      "[664]\ttraining's auc: 0.852306\ttraining's binary_logloss: 0.217092\tvalid_1's auc: 0.762147\tvalid_1's binary_logloss: 0.244585\n",
      "[665]\ttraining's auc: 0.852363\ttraining's binary_logloss: 0.217063\tvalid_1's auc: 0.762163\tvalid_1's binary_logloss: 0.244581\n",
      "[666]\ttraining's auc: 0.852455\ttraining's binary_logloss: 0.21703\tvalid_1's auc: 0.762172\tvalid_1's binary_logloss: 0.24458\n",
      "[667]\ttraining's auc: 0.852554\ttraining's binary_logloss: 0.21699\tvalid_1's auc: 0.7622\tvalid_1's binary_logloss: 0.24457\n",
      "[668]\ttraining's auc: 0.85264\ttraining's binary_logloss: 0.216958\tvalid_1's auc: 0.762219\tvalid_1's binary_logloss: 0.244567\n",
      "[669]\ttraining's auc: 0.852679\ttraining's binary_logloss: 0.216938\tvalid_1's auc: 0.762221\tvalid_1's binary_logloss: 0.244565\n",
      "[670]\ttraining's auc: 0.852787\ttraining's binary_logloss: 0.216893\tvalid_1's auc: 0.762224\tvalid_1's binary_logloss: 0.244564\n",
      "[671]\ttraining's auc: 0.852878\ttraining's binary_logloss: 0.216861\tvalid_1's auc: 0.762226\tvalid_1's binary_logloss: 0.244566\n",
      "[672]\ttraining's auc: 0.852917\ttraining's binary_logloss: 0.216837\tvalid_1's auc: 0.762232\tvalid_1's binary_logloss: 0.244565\n",
      "[673]\ttraining's auc: 0.852994\ttraining's binary_logloss: 0.216804\tvalid_1's auc: 0.762223\tvalid_1's binary_logloss: 0.244568\n",
      "[674]\ttraining's auc: 0.853082\ttraining's binary_logloss: 0.216765\tvalid_1's auc: 0.762252\tvalid_1's binary_logloss: 0.244558\n",
      "[675]\ttraining's auc: 0.853144\ttraining's binary_logloss: 0.216739\tvalid_1's auc: 0.762247\tvalid_1's binary_logloss: 0.244561\n",
      "[676]\ttraining's auc: 0.853242\ttraining's binary_logloss: 0.216699\tvalid_1's auc: 0.762215\tvalid_1's binary_logloss: 0.24457\n",
      "[677]\ttraining's auc: 0.853298\ttraining's binary_logloss: 0.216673\tvalid_1's auc: 0.762213\tvalid_1's binary_logloss: 0.244567\n",
      "[678]\ttraining's auc: 0.853413\ttraining's binary_logloss: 0.216631\tvalid_1's auc: 0.762245\tvalid_1's binary_logloss: 0.244556\n",
      "[679]\ttraining's auc: 0.853513\ttraining's binary_logloss: 0.216597\tvalid_1's auc: 0.762268\tvalid_1's binary_logloss: 0.244548\n",
      "[680]\ttraining's auc: 0.853608\ttraining's binary_logloss: 0.216553\tvalid_1's auc: 0.762261\tvalid_1's binary_logloss: 0.244546\n",
      "[681]\ttraining's auc: 0.853647\ttraining's binary_logloss: 0.21653\tvalid_1's auc: 0.762269\tvalid_1's binary_logloss: 0.244543\n",
      "[682]\ttraining's auc: 0.853688\ttraining's binary_logloss: 0.21651\tvalid_1's auc: 0.762275\tvalid_1's binary_logloss: 0.24454\n",
      "[683]\ttraining's auc: 0.853792\ttraining's binary_logloss: 0.216467\tvalid_1's auc: 0.762269\tvalid_1's binary_logloss: 0.244543\n",
      "[684]\ttraining's auc: 0.853869\ttraining's binary_logloss: 0.216433\tvalid_1's auc: 0.762261\tvalid_1's binary_logloss: 0.244544\n",
      "[685]\ttraining's auc: 0.853983\ttraining's binary_logloss: 0.216384\tvalid_1's auc: 0.762269\tvalid_1's binary_logloss: 0.244542\n",
      "[686]\ttraining's auc: 0.854094\ttraining's binary_logloss: 0.216342\tvalid_1's auc: 0.762263\tvalid_1's binary_logloss: 0.244542\n",
      "[687]\ttraining's auc: 0.854189\ttraining's binary_logloss: 0.216304\tvalid_1's auc: 0.762275\tvalid_1's binary_logloss: 0.244538\n",
      "[688]\ttraining's auc: 0.854256\ttraining's binary_logloss: 0.216278\tvalid_1's auc: 0.762283\tvalid_1's binary_logloss: 0.244536\n",
      "[689]\ttraining's auc: 0.854302\ttraining's binary_logloss: 0.216256\tvalid_1's auc: 0.762266\tvalid_1's binary_logloss: 0.244541\n",
      "[690]\ttraining's auc: 0.854338\ttraining's binary_logloss: 0.216239\tvalid_1's auc: 0.762278\tvalid_1's binary_logloss: 0.244538\n",
      "[691]\ttraining's auc: 0.854427\ttraining's binary_logloss: 0.216208\tvalid_1's auc: 0.762281\tvalid_1's binary_logloss: 0.244538\n",
      "[692]\ttraining's auc: 0.854509\ttraining's binary_logloss: 0.216176\tvalid_1's auc: 0.762313\tvalid_1's binary_logloss: 0.244533\n",
      "[693]\ttraining's auc: 0.854632\ttraining's binary_logloss: 0.216131\tvalid_1's auc: 0.762336\tvalid_1's binary_logloss: 0.244527\n",
      "[694]\ttraining's auc: 0.85468\ttraining's binary_logloss: 0.21611\tvalid_1's auc: 0.76234\tvalid_1's binary_logloss: 0.244526\n",
      "[695]\ttraining's auc: 0.854745\ttraining's binary_logloss: 0.216083\tvalid_1's auc: 0.762332\tvalid_1's binary_logloss: 0.244527\n",
      "[696]\ttraining's auc: 0.854855\ttraining's binary_logloss: 0.216038\tvalid_1's auc: 0.762347\tvalid_1's binary_logloss: 0.244523\n",
      "[697]\ttraining's auc: 0.854969\ttraining's binary_logloss: 0.215999\tvalid_1's auc: 0.762354\tvalid_1's binary_logloss: 0.244522\n",
      "[698]\ttraining's auc: 0.855034\ttraining's binary_logloss: 0.21597\tvalid_1's auc: 0.762368\tvalid_1's binary_logloss: 0.244516\n",
      "[699]\ttraining's auc: 0.855122\ttraining's binary_logloss: 0.215934\tvalid_1's auc: 0.762393\tvalid_1's binary_logloss: 0.24451\n",
      "[700]\ttraining's auc: 0.855153\ttraining's binary_logloss: 0.215918\tvalid_1's auc: 0.7624\tvalid_1's binary_logloss: 0.244507\n",
      "[701]\ttraining's auc: 0.855244\ttraining's binary_logloss: 0.215876\tvalid_1's auc: 0.762398\tvalid_1's binary_logloss: 0.244504\n",
      "[702]\ttraining's auc: 0.855343\ttraining's binary_logloss: 0.215833\tvalid_1's auc: 0.762394\tvalid_1's binary_logloss: 0.244503\n",
      "[703]\ttraining's auc: 0.855425\ttraining's binary_logloss: 0.215794\tvalid_1's auc: 0.762415\tvalid_1's binary_logloss: 0.244492\n",
      "[704]\ttraining's auc: 0.855538\ttraining's binary_logloss: 0.215751\tvalid_1's auc: 0.762419\tvalid_1's binary_logloss: 0.244491\n",
      "[705]\ttraining's auc: 0.855615\ttraining's binary_logloss: 0.215721\tvalid_1's auc: 0.762416\tvalid_1's binary_logloss: 0.244488\n",
      "[706]\ttraining's auc: 0.855717\ttraining's binary_logloss: 0.215674\tvalid_1's auc: 0.762439\tvalid_1's binary_logloss: 0.244476\n",
      "[707]\ttraining's auc: 0.855791\ttraining's binary_logloss: 0.215647\tvalid_1's auc: 0.762455\tvalid_1's binary_logloss: 0.244472\n",
      "[708]\ttraining's auc: 0.855898\ttraining's binary_logloss: 0.215604\tvalid_1's auc: 0.76249\tvalid_1's binary_logloss: 0.244465\n",
      "[709]\ttraining's auc: 0.855999\ttraining's binary_logloss: 0.215561\tvalid_1's auc: 0.762501\tvalid_1's binary_logloss: 0.244463\n",
      "[710]\ttraining's auc: 0.856084\ttraining's binary_logloss: 0.215526\tvalid_1's auc: 0.762505\tvalid_1's binary_logloss: 0.244458\n",
      "[711]\ttraining's auc: 0.856193\ttraining's binary_logloss: 0.215484\tvalid_1's auc: 0.762496\tvalid_1's binary_logloss: 0.244457\n",
      "[712]\ttraining's auc: 0.856286\ttraining's binary_logloss: 0.215443\tvalid_1's auc: 0.762493\tvalid_1's binary_logloss: 0.244457\n",
      "[713]\ttraining's auc: 0.856371\ttraining's binary_logloss: 0.21541\tvalid_1's auc: 0.762508\tvalid_1's binary_logloss: 0.244452\n",
      "[714]\ttraining's auc: 0.856468\ttraining's binary_logloss: 0.215367\tvalid_1's auc: 0.762516\tvalid_1's binary_logloss: 0.244448\n",
      "[715]\ttraining's auc: 0.85656\ttraining's binary_logloss: 0.215327\tvalid_1's auc: 0.762517\tvalid_1's binary_logloss: 0.244447\n",
      "[716]\ttraining's auc: 0.856668\ttraining's binary_logloss: 0.215285\tvalid_1's auc: 0.762524\tvalid_1's binary_logloss: 0.244443\n",
      "[717]\ttraining's auc: 0.856748\ttraining's binary_logloss: 0.215245\tvalid_1's auc: 0.762539\tvalid_1's binary_logloss: 0.244439\n",
      "[718]\ttraining's auc: 0.856773\ttraining's binary_logloss: 0.215229\tvalid_1's auc: 0.762552\tvalid_1's binary_logloss: 0.244435\n",
      "[719]\ttraining's auc: 0.856849\ttraining's binary_logloss: 0.2152\tvalid_1's auc: 0.762548\tvalid_1's binary_logloss: 0.244437\n",
      "[720]\ttraining's auc: 0.856956\ttraining's binary_logloss: 0.215162\tvalid_1's auc: 0.762566\tvalid_1's binary_logloss: 0.244434\n",
      "[721]\ttraining's auc: 0.857034\ttraining's binary_logloss: 0.215131\tvalid_1's auc: 0.762585\tvalid_1's binary_logloss: 0.244429\n",
      "[722]\ttraining's auc: 0.857101\ttraining's binary_logloss: 0.215103\tvalid_1's auc: 0.762598\tvalid_1's binary_logloss: 0.244426\n",
      "[723]\ttraining's auc: 0.857196\ttraining's binary_logloss: 0.215066\tvalid_1's auc: 0.762578\tvalid_1's binary_logloss: 0.24443\n",
      "[724]\ttraining's auc: 0.857273\ttraining's binary_logloss: 0.21503\tvalid_1's auc: 0.762582\tvalid_1's binary_logloss: 0.24443\n",
      "[725]\ttraining's auc: 0.857364\ttraining's binary_logloss: 0.214987\tvalid_1's auc: 0.762592\tvalid_1's binary_logloss: 0.244427\n",
      "[726]\ttraining's auc: 0.857407\ttraining's binary_logloss: 0.214967\tvalid_1's auc: 0.762597\tvalid_1's binary_logloss: 0.244425\n",
      "[727]\ttraining's auc: 0.857499\ttraining's binary_logloss: 0.214926\tvalid_1's auc: 0.762595\tvalid_1's binary_logloss: 0.244422\n",
      "[728]\ttraining's auc: 0.857621\ttraining's binary_logloss: 0.214881\tvalid_1's auc: 0.762616\tvalid_1's binary_logloss: 0.244417\n",
      "[729]\ttraining's auc: 0.857661\ttraining's binary_logloss: 0.21486\tvalid_1's auc: 0.762621\tvalid_1's binary_logloss: 0.244419\n",
      "[730]\ttraining's auc: 0.857754\ttraining's binary_logloss: 0.214822\tvalid_1's auc: 0.762602\tvalid_1's binary_logloss: 0.244422\n",
      "[731]\ttraining's auc: 0.857827\ttraining's binary_logloss: 0.214795\tvalid_1's auc: 0.762608\tvalid_1's binary_logloss: 0.244419\n",
      "[732]\ttraining's auc: 0.857948\ttraining's binary_logloss: 0.21475\tvalid_1's auc: 0.762612\tvalid_1's binary_logloss: 0.244419\n",
      "[733]\ttraining's auc: 0.857998\ttraining's binary_logloss: 0.214722\tvalid_1's auc: 0.762596\tvalid_1's binary_logloss: 0.24442\n",
      "[734]\ttraining's auc: 0.858099\ttraining's binary_logloss: 0.214685\tvalid_1's auc: 0.762584\tvalid_1's binary_logloss: 0.244425\n",
      "[735]\ttraining's auc: 0.858172\ttraining's binary_logloss: 0.214654\tvalid_1's auc: 0.762574\tvalid_1's binary_logloss: 0.244422\n",
      "[736]\ttraining's auc: 0.858231\ttraining's binary_logloss: 0.214631\tvalid_1's auc: 0.762566\tvalid_1's binary_logloss: 0.244422\n",
      "[737]\ttraining's auc: 0.858343\ttraining's binary_logloss: 0.214588\tvalid_1's auc: 0.762536\tvalid_1's binary_logloss: 0.24443\n",
      "[738]\ttraining's auc: 0.858413\ttraining's binary_logloss: 0.214558\tvalid_1's auc: 0.762553\tvalid_1's binary_logloss: 0.244427\n",
      "[739]\ttraining's auc: 0.858482\ttraining's binary_logloss: 0.21453\tvalid_1's auc: 0.762567\tvalid_1's binary_logloss: 0.244425\n",
      "[740]\ttraining's auc: 0.858525\ttraining's binary_logloss: 0.21451\tvalid_1's auc: 0.76256\tvalid_1's binary_logloss: 0.244426\n",
      "[741]\ttraining's auc: 0.858629\ttraining's binary_logloss: 0.21447\tvalid_1's auc: 0.762567\tvalid_1's binary_logloss: 0.244425\n",
      "[742]\ttraining's auc: 0.858745\ttraining's binary_logloss: 0.214428\tvalid_1's auc: 0.762558\tvalid_1's binary_logloss: 0.24443\n",
      "[743]\ttraining's auc: 0.858825\ttraining's binary_logloss: 0.21439\tvalid_1's auc: 0.762567\tvalid_1's binary_logloss: 0.244424\n",
      "[744]\ttraining's auc: 0.858933\ttraining's binary_logloss: 0.214347\tvalid_1's auc: 0.76258\tvalid_1's binary_logloss: 0.244418\n",
      "[745]\ttraining's auc: 0.859051\ttraining's binary_logloss: 0.214301\tvalid_1's auc: 0.762589\tvalid_1's binary_logloss: 0.244414\n",
      "[746]\ttraining's auc: 0.859121\ttraining's binary_logloss: 0.21427\tvalid_1's auc: 0.762593\tvalid_1's binary_logloss: 0.244411\n",
      "[747]\ttraining's auc: 0.859216\ttraining's binary_logloss: 0.214233\tvalid_1's auc: 0.76261\tvalid_1's binary_logloss: 0.244404\n",
      "[748]\ttraining's auc: 0.859298\ttraining's binary_logloss: 0.2142\tvalid_1's auc: 0.762605\tvalid_1's binary_logloss: 0.244402\n",
      "[749]\ttraining's auc: 0.859386\ttraining's binary_logloss: 0.214167\tvalid_1's auc: 0.762618\tvalid_1's binary_logloss: 0.244396\n",
      "[750]\ttraining's auc: 0.859489\ttraining's binary_logloss: 0.214121\tvalid_1's auc: 0.76264\tvalid_1's binary_logloss: 0.244389\n",
      "[751]\ttraining's auc: 0.859599\ttraining's binary_logloss: 0.214078\tvalid_1's auc: 0.762618\tvalid_1's binary_logloss: 0.244392\n",
      "[752]\ttraining's auc: 0.859672\ttraining's binary_logloss: 0.214049\tvalid_1's auc: 0.762616\tvalid_1's binary_logloss: 0.244392\n",
      "[753]\ttraining's auc: 0.859746\ttraining's binary_logloss: 0.214015\tvalid_1's auc: 0.762655\tvalid_1's binary_logloss: 0.244378\n",
      "[754]\ttraining's auc: 0.859837\ttraining's binary_logloss: 0.213984\tvalid_1's auc: 0.762652\tvalid_1's binary_logloss: 0.244379\n",
      "[755]\ttraining's auc: 0.859941\ttraining's binary_logloss: 0.213939\tvalid_1's auc: 0.762664\tvalid_1's binary_logloss: 0.244378\n",
      "[756]\ttraining's auc: 0.86003\ttraining's binary_logloss: 0.213907\tvalid_1's auc: 0.762653\tvalid_1's binary_logloss: 0.244378\n",
      "[757]\ttraining's auc: 0.860117\ttraining's binary_logloss: 0.213867\tvalid_1's auc: 0.76265\tvalid_1's binary_logloss: 0.244378\n",
      "[758]\ttraining's auc: 0.860179\ttraining's binary_logloss: 0.213842\tvalid_1's auc: 0.76266\tvalid_1's binary_logloss: 0.244377\n",
      "[759]\ttraining's auc: 0.860278\ttraining's binary_logloss: 0.213804\tvalid_1's auc: 0.76264\tvalid_1's binary_logloss: 0.244383\n",
      "[760]\ttraining's auc: 0.860364\ttraining's binary_logloss: 0.213765\tvalid_1's auc: 0.762624\tvalid_1's binary_logloss: 0.244386\n",
      "[761]\ttraining's auc: 0.860382\ttraining's binary_logloss: 0.213753\tvalid_1's auc: 0.762613\tvalid_1's binary_logloss: 0.24439\n",
      "[762]\ttraining's auc: 0.860472\ttraining's binary_logloss: 0.213711\tvalid_1's auc: 0.762636\tvalid_1's binary_logloss: 0.244384\n",
      "[763]\ttraining's auc: 0.860518\ttraining's binary_logloss: 0.213689\tvalid_1's auc: 0.76264\tvalid_1's binary_logloss: 0.244384\n",
      "[764]\ttraining's auc: 0.860621\ttraining's binary_logloss: 0.213645\tvalid_1's auc: 0.762671\tvalid_1's binary_logloss: 0.244379\n",
      "[765]\ttraining's auc: 0.860702\ttraining's binary_logloss: 0.213609\tvalid_1's auc: 0.762696\tvalid_1's binary_logloss: 0.244373\n",
      "[766]\ttraining's auc: 0.860723\ttraining's binary_logloss: 0.213599\tvalid_1's auc: 0.762694\tvalid_1's binary_logloss: 0.244374\n",
      "[767]\ttraining's auc: 0.860833\ttraining's binary_logloss: 0.213558\tvalid_1's auc: 0.762698\tvalid_1's binary_logloss: 0.244374\n",
      "[768]\ttraining's auc: 0.860923\ttraining's binary_logloss: 0.213519\tvalid_1's auc: 0.762715\tvalid_1's binary_logloss: 0.24437\n",
      "[769]\ttraining's auc: 0.861038\ttraining's binary_logloss: 0.213479\tvalid_1's auc: 0.762693\tvalid_1's binary_logloss: 0.244376\n",
      "[770]\ttraining's auc: 0.861103\ttraining's binary_logloss: 0.213449\tvalid_1's auc: 0.762711\tvalid_1's binary_logloss: 0.244373\n",
      "[771]\ttraining's auc: 0.861139\ttraining's binary_logloss: 0.21343\tvalid_1's auc: 0.762712\tvalid_1's binary_logloss: 0.244371\n",
      "[772]\ttraining's auc: 0.861174\ttraining's binary_logloss: 0.21341\tvalid_1's auc: 0.762716\tvalid_1's binary_logloss: 0.244371\n",
      "[773]\ttraining's auc: 0.861221\ttraining's binary_logloss: 0.213388\tvalid_1's auc: 0.762712\tvalid_1's binary_logloss: 0.244371\n",
      "[774]\ttraining's auc: 0.861258\ttraining's binary_logloss: 0.213369\tvalid_1's auc: 0.762709\tvalid_1's binary_logloss: 0.244371\n",
      "[775]\ttraining's auc: 0.861335\ttraining's binary_logloss: 0.213344\tvalid_1's auc: 0.762716\tvalid_1's binary_logloss: 0.244369\n",
      "[776]\ttraining's auc: 0.861417\ttraining's binary_logloss: 0.21331\tvalid_1's auc: 0.76269\tvalid_1's binary_logloss: 0.244375\n",
      "[777]\ttraining's auc: 0.861451\ttraining's binary_logloss: 0.213293\tvalid_1's auc: 0.762689\tvalid_1's binary_logloss: 0.244375\n",
      "[778]\ttraining's auc: 0.861489\ttraining's binary_logloss: 0.213277\tvalid_1's auc: 0.762709\tvalid_1's binary_logloss: 0.244372\n",
      "[779]\ttraining's auc: 0.86156\ttraining's binary_logloss: 0.213239\tvalid_1's auc: 0.762725\tvalid_1's binary_logloss: 0.244368\n",
      "[780]\ttraining's auc: 0.86166\ttraining's binary_logloss: 0.213199\tvalid_1's auc: 0.762764\tvalid_1's binary_logloss: 0.244358\n",
      "[781]\ttraining's auc: 0.861746\ttraining's binary_logloss: 0.213162\tvalid_1's auc: 0.762773\tvalid_1's binary_logloss: 0.244355\n",
      "[782]\ttraining's auc: 0.861845\ttraining's binary_logloss: 0.21312\tvalid_1's auc: 0.762788\tvalid_1's binary_logloss: 0.244353\n",
      "[783]\ttraining's auc: 0.861943\ttraining's binary_logloss: 0.21308\tvalid_1's auc: 0.762793\tvalid_1's binary_logloss: 0.244352\n",
      "[784]\ttraining's auc: 0.862048\ttraining's binary_logloss: 0.21304\tvalid_1's auc: 0.762804\tvalid_1's binary_logloss: 0.244348\n",
      "[785]\ttraining's auc: 0.862123\ttraining's binary_logloss: 0.213011\tvalid_1's auc: 0.76282\tvalid_1's binary_logloss: 0.244346\n",
      "[786]\ttraining's auc: 0.862178\ttraining's binary_logloss: 0.21299\tvalid_1's auc: 0.762832\tvalid_1's binary_logloss: 0.24434\n",
      "[787]\ttraining's auc: 0.862276\ttraining's binary_logloss: 0.212947\tvalid_1's auc: 0.762829\tvalid_1's binary_logloss: 0.244338\n",
      "[788]\ttraining's auc: 0.862346\ttraining's binary_logloss: 0.212919\tvalid_1's auc: 0.762829\tvalid_1's binary_logloss: 0.244335\n",
      "[789]\ttraining's auc: 0.862411\ttraining's binary_logloss: 0.212891\tvalid_1's auc: 0.762828\tvalid_1's binary_logloss: 0.244337\n",
      "[790]\ttraining's auc: 0.862474\ttraining's binary_logloss: 0.212862\tvalid_1's auc: 0.762834\tvalid_1's binary_logloss: 0.244336\n",
      "[791]\ttraining's auc: 0.862595\ttraining's binary_logloss: 0.212819\tvalid_1's auc: 0.762863\tvalid_1's binary_logloss: 0.244335\n",
      "[792]\ttraining's auc: 0.862694\ttraining's binary_logloss: 0.212776\tvalid_1's auc: 0.762864\tvalid_1's binary_logloss: 0.244332\n",
      "[793]\ttraining's auc: 0.862723\ttraining's binary_logloss: 0.212764\tvalid_1's auc: 0.762861\tvalid_1's binary_logloss: 0.244334\n",
      "[794]\ttraining's auc: 0.862825\ttraining's binary_logloss: 0.212724\tvalid_1's auc: 0.762846\tvalid_1's binary_logloss: 0.244339\n",
      "[795]\ttraining's auc: 0.862855\ttraining's binary_logloss: 0.212709\tvalid_1's auc: 0.762855\tvalid_1's binary_logloss: 0.244337\n",
      "[796]\ttraining's auc: 0.86295\ttraining's binary_logloss: 0.212669\tvalid_1's auc: 0.762845\tvalid_1's binary_logloss: 0.244338\n",
      "[797]\ttraining's auc: 0.863018\ttraining's binary_logloss: 0.21264\tvalid_1's auc: 0.762846\tvalid_1's binary_logloss: 0.244337\n",
      "[798]\ttraining's auc: 0.863122\ttraining's binary_logloss: 0.212598\tvalid_1's auc: 0.762853\tvalid_1's binary_logloss: 0.244334\n",
      "[799]\ttraining's auc: 0.863226\ttraining's binary_logloss: 0.212557\tvalid_1's auc: 0.762869\tvalid_1's binary_logloss: 0.244331\n",
      "[800]\ttraining's auc: 0.863259\ttraining's binary_logloss: 0.212538\tvalid_1's auc: 0.762886\tvalid_1's binary_logloss: 0.244326\n",
      "[801]\ttraining's auc: 0.863329\ttraining's binary_logloss: 0.212508\tvalid_1's auc: 0.762877\tvalid_1's binary_logloss: 0.244328\n",
      "[802]\ttraining's auc: 0.863357\ttraining's binary_logloss: 0.21249\tvalid_1's auc: 0.762887\tvalid_1's binary_logloss: 0.244324\n",
      "[803]\ttraining's auc: 0.863465\ttraining's binary_logloss: 0.212447\tvalid_1's auc: 0.762906\tvalid_1's binary_logloss: 0.24432\n",
      "[804]\ttraining's auc: 0.863584\ttraining's binary_logloss: 0.212401\tvalid_1's auc: 0.762898\tvalid_1's binary_logloss: 0.244321\n",
      "[805]\ttraining's auc: 0.863682\ttraining's binary_logloss: 0.21236\tvalid_1's auc: 0.762909\tvalid_1's binary_logloss: 0.244318\n",
      "[806]\ttraining's auc: 0.863744\ttraining's binary_logloss: 0.212336\tvalid_1's auc: 0.76291\tvalid_1's binary_logloss: 0.244317\n",
      "[807]\ttraining's auc: 0.863831\ttraining's binary_logloss: 0.212298\tvalid_1's auc: 0.76294\tvalid_1's binary_logloss: 0.244308\n",
      "[808]\ttraining's auc: 0.863926\ttraining's binary_logloss: 0.212261\tvalid_1's auc: 0.762917\tvalid_1's binary_logloss: 0.244311\n",
      "[809]\ttraining's auc: 0.86395\ttraining's binary_logloss: 0.212246\tvalid_1's auc: 0.762921\tvalid_1's binary_logloss: 0.24431\n",
      "[810]\ttraining's auc: 0.864011\ttraining's binary_logloss: 0.212217\tvalid_1's auc: 0.762907\tvalid_1's binary_logloss: 0.244312\n",
      "[811]\ttraining's auc: 0.864099\ttraining's binary_logloss: 0.212179\tvalid_1's auc: 0.762906\tvalid_1's binary_logloss: 0.244312\n",
      "[812]\ttraining's auc: 0.86417\ttraining's binary_logloss: 0.212152\tvalid_1's auc: 0.762913\tvalid_1's binary_logloss: 0.24431\n",
      "[813]\ttraining's auc: 0.86426\ttraining's binary_logloss: 0.212114\tvalid_1's auc: 0.762916\tvalid_1's binary_logloss: 0.244308\n",
      "[814]\ttraining's auc: 0.864295\ttraining's binary_logloss: 0.2121\tvalid_1's auc: 0.762919\tvalid_1's binary_logloss: 0.244308\n",
      "[815]\ttraining's auc: 0.864338\ttraining's binary_logloss: 0.212081\tvalid_1's auc: 0.762916\tvalid_1's binary_logloss: 0.244308\n",
      "[816]\ttraining's auc: 0.864404\ttraining's binary_logloss: 0.212053\tvalid_1's auc: 0.762914\tvalid_1's binary_logloss: 0.244308\n",
      "[817]\ttraining's auc: 0.864468\ttraining's binary_logloss: 0.212025\tvalid_1's auc: 0.762937\tvalid_1's binary_logloss: 0.244301\n",
      "[818]\ttraining's auc: 0.864496\ttraining's binary_logloss: 0.212012\tvalid_1's auc: 0.762942\tvalid_1's binary_logloss: 0.2443\n",
      "[819]\ttraining's auc: 0.864579\ttraining's binary_logloss: 0.21198\tvalid_1's auc: 0.762954\tvalid_1's binary_logloss: 0.244296\n",
      "[820]\ttraining's auc: 0.864629\ttraining's binary_logloss: 0.211962\tvalid_1's auc: 0.762936\tvalid_1's binary_logloss: 0.244301\n",
      "[821]\ttraining's auc: 0.864747\ttraining's binary_logloss: 0.21192\tvalid_1's auc: 0.762923\tvalid_1's binary_logloss: 0.2443\n",
      "[822]\ttraining's auc: 0.864806\ttraining's binary_logloss: 0.211894\tvalid_1's auc: 0.76292\tvalid_1's binary_logloss: 0.244302\n",
      "[823]\ttraining's auc: 0.864894\ttraining's binary_logloss: 0.211856\tvalid_1's auc: 0.762923\tvalid_1's binary_logloss: 0.244297\n",
      "[824]\ttraining's auc: 0.864996\ttraining's binary_logloss: 0.211813\tvalid_1's auc: 0.762933\tvalid_1's binary_logloss: 0.244295\n",
      "[825]\ttraining's auc: 0.865089\ttraining's binary_logloss: 0.211772\tvalid_1's auc: 0.762929\tvalid_1's binary_logloss: 0.244294\n",
      "[826]\ttraining's auc: 0.86511\ttraining's binary_logloss: 0.211761\tvalid_1's auc: 0.762919\tvalid_1's binary_logloss: 0.244296\n",
      "[827]\ttraining's auc: 0.865151\ttraining's binary_logloss: 0.211745\tvalid_1's auc: 0.762922\tvalid_1's binary_logloss: 0.244295\n",
      "[828]\ttraining's auc: 0.865258\ttraining's binary_logloss: 0.211701\tvalid_1's auc: 0.762917\tvalid_1's binary_logloss: 0.244293\n",
      "[829]\ttraining's auc: 0.865365\ttraining's binary_logloss: 0.211658\tvalid_1's auc: 0.762961\tvalid_1's binary_logloss: 0.244285\n",
      "[830]\ttraining's auc: 0.865431\ttraining's binary_logloss: 0.211626\tvalid_1's auc: 0.762962\tvalid_1's binary_logloss: 0.244286\n",
      "[831]\ttraining's auc: 0.865512\ttraining's binary_logloss: 0.211591\tvalid_1's auc: 0.762974\tvalid_1's binary_logloss: 0.244281\n",
      "[832]\ttraining's auc: 0.865594\ttraining's binary_logloss: 0.21156\tvalid_1's auc: 0.762981\tvalid_1's binary_logloss: 0.24428\n",
      "[833]\ttraining's auc: 0.865629\ttraining's binary_logloss: 0.211543\tvalid_1's auc: 0.762991\tvalid_1's binary_logloss: 0.24428\n",
      "[834]\ttraining's auc: 0.865724\ttraining's binary_logloss: 0.211508\tvalid_1's auc: 0.762996\tvalid_1's binary_logloss: 0.244278\n",
      "[835]\ttraining's auc: 0.865757\ttraining's binary_logloss: 0.211494\tvalid_1's auc: 0.763009\tvalid_1's binary_logloss: 0.244276\n",
      "[836]\ttraining's auc: 0.865842\ttraining's binary_logloss: 0.211454\tvalid_1's auc: 0.763017\tvalid_1's binary_logloss: 0.244274\n",
      "[837]\ttraining's auc: 0.865916\ttraining's binary_logloss: 0.211426\tvalid_1's auc: 0.763024\tvalid_1's binary_logloss: 0.244273\n",
      "[838]\ttraining's auc: 0.865951\ttraining's binary_logloss: 0.211409\tvalid_1's auc: 0.763028\tvalid_1's binary_logloss: 0.244272\n",
      "[839]\ttraining's auc: 0.865996\ttraining's binary_logloss: 0.211392\tvalid_1's auc: 0.763029\tvalid_1's binary_logloss: 0.244273\n",
      "[840]\ttraining's auc: 0.866036\ttraining's binary_logloss: 0.211368\tvalid_1's auc: 0.763026\tvalid_1's binary_logloss: 0.244269\n",
      "[841]\ttraining's auc: 0.866084\ttraining's binary_logloss: 0.211347\tvalid_1's auc: 0.763022\tvalid_1's binary_logloss: 0.244272\n",
      "[842]\ttraining's auc: 0.866125\ttraining's binary_logloss: 0.211327\tvalid_1's auc: 0.763026\tvalid_1's binary_logloss: 0.244274\n",
      "[843]\ttraining's auc: 0.866225\ttraining's binary_logloss: 0.211285\tvalid_1's auc: 0.763022\tvalid_1's binary_logloss: 0.244272\n",
      "[844]\ttraining's auc: 0.866345\ttraining's binary_logloss: 0.211239\tvalid_1's auc: 0.763048\tvalid_1's binary_logloss: 0.244268\n",
      "[845]\ttraining's auc: 0.866422\ttraining's binary_logloss: 0.211203\tvalid_1's auc: 0.763055\tvalid_1's binary_logloss: 0.244269\n",
      "[846]\ttraining's auc: 0.866463\ttraining's binary_logloss: 0.211184\tvalid_1's auc: 0.763046\tvalid_1's binary_logloss: 0.244272\n",
      "[847]\ttraining's auc: 0.86655\ttraining's binary_logloss: 0.211149\tvalid_1's auc: 0.763057\tvalid_1's binary_logloss: 0.244268\n",
      "[848]\ttraining's auc: 0.866642\ttraining's binary_logloss: 0.211113\tvalid_1's auc: 0.763053\tvalid_1's binary_logloss: 0.244272\n",
      "[849]\ttraining's auc: 0.866734\ttraining's binary_logloss: 0.211074\tvalid_1's auc: 0.763062\tvalid_1's binary_logloss: 0.244266\n",
      "[850]\ttraining's auc: 0.86682\ttraining's binary_logloss: 0.21104\tvalid_1's auc: 0.763079\tvalid_1's binary_logloss: 0.244265\n",
      "[851]\ttraining's auc: 0.866897\ttraining's binary_logloss: 0.211006\tvalid_1's auc: 0.763043\tvalid_1's binary_logloss: 0.244274\n",
      "[852]\ttraining's auc: 0.866941\ttraining's binary_logloss: 0.210984\tvalid_1's auc: 0.763051\tvalid_1's binary_logloss: 0.244273\n",
      "[853]\ttraining's auc: 0.867028\ttraining's binary_logloss: 0.210953\tvalid_1's auc: 0.76305\tvalid_1's binary_logloss: 0.244271\n",
      "[854]\ttraining's auc: 0.867116\ttraining's binary_logloss: 0.210919\tvalid_1's auc: 0.763077\tvalid_1's binary_logloss: 0.244266\n",
      "[855]\ttraining's auc: 0.867202\ttraining's binary_logloss: 0.210887\tvalid_1's auc: 0.763102\tvalid_1's binary_logloss: 0.244259\n",
      "[856]\ttraining's auc: 0.867314\ttraining's binary_logloss: 0.210843\tvalid_1's auc: 0.763116\tvalid_1's binary_logloss: 0.244261\n",
      "[857]\ttraining's auc: 0.867385\ttraining's binary_logloss: 0.210813\tvalid_1's auc: 0.763137\tvalid_1's binary_logloss: 0.244254\n",
      "[858]\ttraining's auc: 0.867428\ttraining's binary_logloss: 0.210796\tvalid_1's auc: 0.763139\tvalid_1's binary_logloss: 0.244253\n",
      "[859]\ttraining's auc: 0.867514\ttraining's binary_logloss: 0.21076\tvalid_1's auc: 0.763141\tvalid_1's binary_logloss: 0.244252\n",
      "[860]\ttraining's auc: 0.867564\ttraining's binary_logloss: 0.210734\tvalid_1's auc: 0.763133\tvalid_1's binary_logloss: 0.244252\n",
      "[861]\ttraining's auc: 0.86765\ttraining's binary_logloss: 0.210697\tvalid_1's auc: 0.763158\tvalid_1's binary_logloss: 0.244242\n",
      "[862]\ttraining's auc: 0.867713\ttraining's binary_logloss: 0.210669\tvalid_1's auc: 0.763173\tvalid_1's binary_logloss: 0.24424\n",
      "[863]\ttraining's auc: 0.86778\ttraining's binary_logloss: 0.210639\tvalid_1's auc: 0.763174\tvalid_1's binary_logloss: 0.244242\n",
      "[864]\ttraining's auc: 0.867871\ttraining's binary_logloss: 0.2106\tvalid_1's auc: 0.763192\tvalid_1's binary_logloss: 0.244234\n",
      "[865]\ttraining's auc: 0.867961\ttraining's binary_logloss: 0.210563\tvalid_1's auc: 0.763211\tvalid_1's binary_logloss: 0.24423\n",
      "[866]\ttraining's auc: 0.868045\ttraining's binary_logloss: 0.210527\tvalid_1's auc: 0.763215\tvalid_1's binary_logloss: 0.24423\n",
      "[867]\ttraining's auc: 0.868121\ttraining's binary_logloss: 0.210493\tvalid_1's auc: 0.763222\tvalid_1's binary_logloss: 0.244226\n",
      "[868]\ttraining's auc: 0.868216\ttraining's binary_logloss: 0.210452\tvalid_1's auc: 0.763221\tvalid_1's binary_logloss: 0.244228\n",
      "[869]\ttraining's auc: 0.8683\ttraining's binary_logloss: 0.210419\tvalid_1's auc: 0.763225\tvalid_1's binary_logloss: 0.244227\n",
      "[870]\ttraining's auc: 0.868331\ttraining's binary_logloss: 0.210403\tvalid_1's auc: 0.763228\tvalid_1's binary_logloss: 0.244225\n",
      "[871]\ttraining's auc: 0.868423\ttraining's binary_logloss: 0.210366\tvalid_1's auc: 0.763225\tvalid_1's binary_logloss: 0.244226\n",
      "[872]\ttraining's auc: 0.86847\ttraining's binary_logloss: 0.210347\tvalid_1's auc: 0.763249\tvalid_1's binary_logloss: 0.244221\n",
      "[873]\ttraining's auc: 0.868542\ttraining's binary_logloss: 0.210317\tvalid_1's auc: 0.763254\tvalid_1's binary_logloss: 0.244218\n",
      "[874]\ttraining's auc: 0.868587\ttraining's binary_logloss: 0.210294\tvalid_1's auc: 0.763275\tvalid_1's binary_logloss: 0.244211\n",
      "[875]\ttraining's auc: 0.868672\ttraining's binary_logloss: 0.21026\tvalid_1's auc: 0.763301\tvalid_1's binary_logloss: 0.244206\n",
      "[876]\ttraining's auc: 0.868708\ttraining's binary_logloss: 0.210246\tvalid_1's auc: 0.763301\tvalid_1's binary_logloss: 0.244206\n",
      "[877]\ttraining's auc: 0.868796\ttraining's binary_logloss: 0.210204\tvalid_1's auc: 0.763276\tvalid_1's binary_logloss: 0.244211\n",
      "[878]\ttraining's auc: 0.868855\ttraining's binary_logloss: 0.210179\tvalid_1's auc: 0.763285\tvalid_1's binary_logloss: 0.24421\n",
      "[879]\ttraining's auc: 0.868902\ttraining's binary_logloss: 0.210159\tvalid_1's auc: 0.763291\tvalid_1's binary_logloss: 0.24421\n",
      "[880]\ttraining's auc: 0.868998\ttraining's binary_logloss: 0.210121\tvalid_1's auc: 0.763289\tvalid_1's binary_logloss: 0.244211\n",
      "[881]\ttraining's auc: 0.869048\ttraining's binary_logloss: 0.210104\tvalid_1's auc: 0.763309\tvalid_1's binary_logloss: 0.244205\n",
      "[882]\ttraining's auc: 0.869113\ttraining's binary_logloss: 0.210077\tvalid_1's auc: 0.76331\tvalid_1's binary_logloss: 0.244205\n",
      "[883]\ttraining's auc: 0.869168\ttraining's binary_logloss: 0.21005\tvalid_1's auc: 0.763301\tvalid_1's binary_logloss: 0.244208\n",
      "[884]\ttraining's auc: 0.869194\ttraining's binary_logloss: 0.210036\tvalid_1's auc: 0.763292\tvalid_1's binary_logloss: 0.244211\n",
      "[885]\ttraining's auc: 0.869289\ttraining's binary_logloss: 0.209997\tvalid_1's auc: 0.763302\tvalid_1's binary_logloss: 0.24421\n",
      "[886]\ttraining's auc: 0.869371\ttraining's binary_logloss: 0.209957\tvalid_1's auc: 0.76333\tvalid_1's binary_logloss: 0.244202\n",
      "[887]\ttraining's auc: 0.869413\ttraining's binary_logloss: 0.209942\tvalid_1's auc: 0.763331\tvalid_1's binary_logloss: 0.244203\n",
      "[888]\ttraining's auc: 0.869515\ttraining's binary_logloss: 0.2099\tvalid_1's auc: 0.763356\tvalid_1's binary_logloss: 0.244198\n",
      "[889]\ttraining's auc: 0.869574\ttraining's binary_logloss: 0.209872\tvalid_1's auc: 0.76335\tvalid_1's binary_logloss: 0.2442\n",
      "[890]\ttraining's auc: 0.869667\ttraining's binary_logloss: 0.20983\tvalid_1's auc: 0.763348\tvalid_1's binary_logloss: 0.244197\n",
      "[891]\ttraining's auc: 0.869746\ttraining's binary_logloss: 0.209801\tvalid_1's auc: 0.763351\tvalid_1's binary_logloss: 0.244197\n",
      "[892]\ttraining's auc: 0.869771\ttraining's binary_logloss: 0.209789\tvalid_1's auc: 0.763349\tvalid_1's binary_logloss: 0.244198\n",
      "[893]\ttraining's auc: 0.869851\ttraining's binary_logloss: 0.20976\tvalid_1's auc: 0.76334\tvalid_1's binary_logloss: 0.2442\n",
      "[894]\ttraining's auc: 0.869947\ttraining's binary_logloss: 0.209723\tvalid_1's auc: 0.763354\tvalid_1's binary_logloss: 0.244197\n",
      "[895]\ttraining's auc: 0.870022\ttraining's binary_logloss: 0.209687\tvalid_1's auc: 0.763367\tvalid_1's binary_logloss: 0.244193\n",
      "[896]\ttraining's auc: 0.870107\ttraining's binary_logloss: 0.20965\tvalid_1's auc: 0.763396\tvalid_1's binary_logloss: 0.244185\n",
      "[897]\ttraining's auc: 0.870187\ttraining's binary_logloss: 0.209616\tvalid_1's auc: 0.763403\tvalid_1's binary_logloss: 0.244186\n",
      "[898]\ttraining's auc: 0.870231\ttraining's binary_logloss: 0.209595\tvalid_1's auc: 0.7634\tvalid_1's binary_logloss: 0.244186\n",
      "[899]\ttraining's auc: 0.870266\ttraining's binary_logloss: 0.209582\tvalid_1's auc: 0.763398\tvalid_1's binary_logloss: 0.244188\n",
      "[900]\ttraining's auc: 0.870348\ttraining's binary_logloss: 0.209543\tvalid_1's auc: 0.763397\tvalid_1's binary_logloss: 0.24419\n",
      "[901]\ttraining's auc: 0.870386\ttraining's binary_logloss: 0.209525\tvalid_1's auc: 0.763408\tvalid_1's binary_logloss: 0.244188\n",
      "[902]\ttraining's auc: 0.870472\ttraining's binary_logloss: 0.209489\tvalid_1's auc: 0.763422\tvalid_1's binary_logloss: 0.244184\n",
      "[903]\ttraining's auc: 0.870557\ttraining's binary_logloss: 0.209451\tvalid_1's auc: 0.763408\tvalid_1's binary_logloss: 0.244186\n",
      "[904]\ttraining's auc: 0.870663\ttraining's binary_logloss: 0.209412\tvalid_1's auc: 0.763415\tvalid_1's binary_logloss: 0.244183\n",
      "[905]\ttraining's auc: 0.870758\ttraining's binary_logloss: 0.209372\tvalid_1's auc: 0.763442\tvalid_1's binary_logloss: 0.244176\n",
      "[906]\ttraining's auc: 0.870819\ttraining's binary_logloss: 0.209345\tvalid_1's auc: 0.763442\tvalid_1's binary_logloss: 0.244178\n",
      "[907]\ttraining's auc: 0.870881\ttraining's binary_logloss: 0.20932\tvalid_1's auc: 0.76345\tvalid_1's binary_logloss: 0.244175\n",
      "[908]\ttraining's auc: 0.870899\ttraining's binary_logloss: 0.20931\tvalid_1's auc: 0.763442\tvalid_1's binary_logloss: 0.244177\n",
      "[909]\ttraining's auc: 0.870941\ttraining's binary_logloss: 0.209291\tvalid_1's auc: 0.763422\tvalid_1's binary_logloss: 0.244179\n",
      "[910]\ttraining's auc: 0.871037\ttraining's binary_logloss: 0.209252\tvalid_1's auc: 0.763437\tvalid_1's binary_logloss: 0.244177\n",
      "[911]\ttraining's auc: 0.871126\ttraining's binary_logloss: 0.209213\tvalid_1's auc: 0.763432\tvalid_1's binary_logloss: 0.244175\n",
      "[912]\ttraining's auc: 0.871204\ttraining's binary_logloss: 0.209179\tvalid_1's auc: 0.76344\tvalid_1's binary_logloss: 0.244172\n",
      "[913]\ttraining's auc: 0.871258\ttraining's binary_logloss: 0.209159\tvalid_1's auc: 0.763447\tvalid_1's binary_logloss: 0.24417\n",
      "[914]\ttraining's auc: 0.871367\ttraining's binary_logloss: 0.209119\tvalid_1's auc: 0.763455\tvalid_1's binary_logloss: 0.24417\n",
      "[915]\ttraining's auc: 0.871417\ttraining's binary_logloss: 0.209099\tvalid_1's auc: 0.763467\tvalid_1's binary_logloss: 0.244168\n",
      "[916]\ttraining's auc: 0.871447\ttraining's binary_logloss: 0.209087\tvalid_1's auc: 0.763466\tvalid_1's binary_logloss: 0.244168\n",
      "[917]\ttraining's auc: 0.871486\ttraining's binary_logloss: 0.209067\tvalid_1's auc: 0.763463\tvalid_1's binary_logloss: 0.24417\n",
      "[918]\ttraining's auc: 0.871569\ttraining's binary_logloss: 0.209029\tvalid_1's auc: 0.763466\tvalid_1's binary_logloss: 0.244172\n",
      "[919]\ttraining's auc: 0.871598\ttraining's binary_logloss: 0.209017\tvalid_1's auc: 0.763471\tvalid_1's binary_logloss: 0.244169\n",
      "[920]\ttraining's auc: 0.871673\ttraining's binary_logloss: 0.208991\tvalid_1's auc: 0.763474\tvalid_1's binary_logloss: 0.244167\n",
      "[921]\ttraining's auc: 0.87177\ttraining's binary_logloss: 0.208953\tvalid_1's auc: 0.763501\tvalid_1's binary_logloss: 0.244162\n",
      "[922]\ttraining's auc: 0.871824\ttraining's binary_logloss: 0.20893\tvalid_1's auc: 0.763498\tvalid_1's binary_logloss: 0.244162\n",
      "[923]\ttraining's auc: 0.871892\ttraining's binary_logloss: 0.208903\tvalid_1's auc: 0.763481\tvalid_1's binary_logloss: 0.244165\n",
      "[924]\ttraining's auc: 0.871978\ttraining's binary_logloss: 0.208863\tvalid_1's auc: 0.763524\tvalid_1's binary_logloss: 0.244157\n",
      "[925]\ttraining's auc: 0.87206\ttraining's binary_logloss: 0.208833\tvalid_1's auc: 0.76351\tvalid_1's binary_logloss: 0.24416\n",
      "[926]\ttraining's auc: 0.872133\ttraining's binary_logloss: 0.208798\tvalid_1's auc: 0.763497\tvalid_1's binary_logloss: 0.244161\n",
      "[927]\ttraining's auc: 0.872212\ttraining's binary_logloss: 0.208767\tvalid_1's auc: 0.76352\tvalid_1's binary_logloss: 0.244155\n",
      "[928]\ttraining's auc: 0.872307\ttraining's binary_logloss: 0.208731\tvalid_1's auc: 0.763526\tvalid_1's binary_logloss: 0.244152\n",
      "[929]\ttraining's auc: 0.872371\ttraining's binary_logloss: 0.208702\tvalid_1's auc: 0.763542\tvalid_1's binary_logloss: 0.244146\n",
      "[930]\ttraining's auc: 0.872451\ttraining's binary_logloss: 0.208672\tvalid_1's auc: 0.76356\tvalid_1's binary_logloss: 0.244143\n",
      "[931]\ttraining's auc: 0.8725\ttraining's binary_logloss: 0.208652\tvalid_1's auc: 0.763553\tvalid_1's binary_logloss: 0.244144\n",
      "[932]\ttraining's auc: 0.87257\ttraining's binary_logloss: 0.20862\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.244145\n",
      "[933]\ttraining's auc: 0.872664\ttraining's binary_logloss: 0.20858\tvalid_1's auc: 0.763574\tvalid_1's binary_logloss: 0.244138\n",
      "[934]\ttraining's auc: 0.87277\ttraining's binary_logloss: 0.20854\tvalid_1's auc: 0.763619\tvalid_1's binary_logloss: 0.244124\n",
      "[935]\ttraining's auc: 0.872848\ttraining's binary_logloss: 0.20851\tvalid_1's auc: 0.763625\tvalid_1's binary_logloss: 0.244122\n",
      "[936]\ttraining's auc: 0.872934\ttraining's binary_logloss: 0.208474\tvalid_1's auc: 0.763598\tvalid_1's binary_logloss: 0.244124\n",
      "[937]\ttraining's auc: 0.873037\ttraining's binary_logloss: 0.208438\tvalid_1's auc: 0.763602\tvalid_1's binary_logloss: 0.244121\n",
      "[938]\ttraining's auc: 0.873099\ttraining's binary_logloss: 0.208414\tvalid_1's auc: 0.763605\tvalid_1's binary_logloss: 0.244121\n",
      "[939]\ttraining's auc: 0.873134\ttraining's binary_logloss: 0.208399\tvalid_1's auc: 0.763595\tvalid_1's binary_logloss: 0.244125\n",
      "[940]\ttraining's auc: 0.873226\ttraining's binary_logloss: 0.208356\tvalid_1's auc: 0.763559\tvalid_1's binary_logloss: 0.244135\n",
      "[941]\ttraining's auc: 0.873323\ttraining's binary_logloss: 0.208311\tvalid_1's auc: 0.763582\tvalid_1's binary_logloss: 0.244123\n",
      "[942]\ttraining's auc: 0.873387\ttraining's binary_logloss: 0.20828\tvalid_1's auc: 0.763591\tvalid_1's binary_logloss: 0.244124\n",
      "[943]\ttraining's auc: 0.873446\ttraining's binary_logloss: 0.208255\tvalid_1's auc: 0.763611\tvalid_1's binary_logloss: 0.244118\n",
      "[944]\ttraining's auc: 0.873487\ttraining's binary_logloss: 0.208237\tvalid_1's auc: 0.763615\tvalid_1's binary_logloss: 0.244116\n",
      "[945]\ttraining's auc: 0.873589\ttraining's binary_logloss: 0.208193\tvalid_1's auc: 0.763623\tvalid_1's binary_logloss: 0.244113\n",
      "[946]\ttraining's auc: 0.873648\ttraining's binary_logloss: 0.208166\tvalid_1's auc: 0.763627\tvalid_1's binary_logloss: 0.244116\n",
      "[947]\ttraining's auc: 0.873723\ttraining's binary_logloss: 0.208139\tvalid_1's auc: 0.763643\tvalid_1's binary_logloss: 0.24411\n",
      "[948]\ttraining's auc: 0.87378\ttraining's binary_logloss: 0.208115\tvalid_1's auc: 0.763638\tvalid_1's binary_logloss: 0.244112\n",
      "[949]\ttraining's auc: 0.873826\ttraining's binary_logloss: 0.208098\tvalid_1's auc: 0.763634\tvalid_1's binary_logloss: 0.244116\n",
      "[950]\ttraining's auc: 0.873919\ttraining's binary_logloss: 0.208059\tvalid_1's auc: 0.763624\tvalid_1's binary_logloss: 0.244115\n",
      "[951]\ttraining's auc: 0.873977\ttraining's binary_logloss: 0.208032\tvalid_1's auc: 0.763646\tvalid_1's binary_logloss: 0.24411\n",
      "[952]\ttraining's auc: 0.87402\ttraining's binary_logloss: 0.208015\tvalid_1's auc: 0.763658\tvalid_1's binary_logloss: 0.244109\n",
      "[953]\ttraining's auc: 0.874093\ttraining's binary_logloss: 0.20798\tvalid_1's auc: 0.763653\tvalid_1's binary_logloss: 0.244108\n",
      "[954]\ttraining's auc: 0.874145\ttraining's binary_logloss: 0.207957\tvalid_1's auc: 0.763656\tvalid_1's binary_logloss: 0.244108\n",
      "[955]\ttraining's auc: 0.87422\ttraining's binary_logloss: 0.207925\tvalid_1's auc: 0.763671\tvalid_1's binary_logloss: 0.244104\n",
      "[956]\ttraining's auc: 0.874258\ttraining's binary_logloss: 0.207909\tvalid_1's auc: 0.763665\tvalid_1's binary_logloss: 0.244105\n",
      "[957]\ttraining's auc: 0.874354\ttraining's binary_logloss: 0.20787\tvalid_1's auc: 0.763671\tvalid_1's binary_logloss: 0.244101\n",
      "[958]\ttraining's auc: 0.874427\ttraining's binary_logloss: 0.207841\tvalid_1's auc: 0.763685\tvalid_1's binary_logloss: 0.244096\n",
      "[959]\ttraining's auc: 0.874488\ttraining's binary_logloss: 0.207817\tvalid_1's auc: 0.763684\tvalid_1's binary_logloss: 0.244098\n",
      "[960]\ttraining's auc: 0.874518\ttraining's binary_logloss: 0.207801\tvalid_1's auc: 0.763691\tvalid_1's binary_logloss: 0.244099\n",
      "[961]\ttraining's auc: 0.874606\ttraining's binary_logloss: 0.207767\tvalid_1's auc: 0.763722\tvalid_1's binary_logloss: 0.244096\n",
      "[962]\ttraining's auc: 0.874668\ttraining's binary_logloss: 0.20774\tvalid_1's auc: 0.763715\tvalid_1's binary_logloss: 0.244099\n",
      "[963]\ttraining's auc: 0.874775\ttraining's binary_logloss: 0.207694\tvalid_1's auc: 0.763729\tvalid_1's binary_logloss: 0.244092\n",
      "[964]\ttraining's auc: 0.874864\ttraining's binary_logloss: 0.207657\tvalid_1's auc: 0.763715\tvalid_1's binary_logloss: 0.244096\n",
      "[965]\ttraining's auc: 0.874926\ttraining's binary_logloss: 0.207628\tvalid_1's auc: 0.763747\tvalid_1's binary_logloss: 0.24409\n",
      "[966]\ttraining's auc: 0.875006\ttraining's binary_logloss: 0.207595\tvalid_1's auc: 0.763714\tvalid_1's binary_logloss: 0.244096\n",
      "[967]\ttraining's auc: 0.875098\ttraining's binary_logloss: 0.207559\tvalid_1's auc: 0.763729\tvalid_1's binary_logloss: 0.244095\n",
      "[968]\ttraining's auc: 0.875179\ttraining's binary_logloss: 0.207529\tvalid_1's auc: 0.763735\tvalid_1's binary_logloss: 0.244093\n",
      "[969]\ttraining's auc: 0.875258\ttraining's binary_logloss: 0.207496\tvalid_1's auc: 0.763724\tvalid_1's binary_logloss: 0.244097\n",
      "[970]\ttraining's auc: 0.875306\ttraining's binary_logloss: 0.207474\tvalid_1's auc: 0.763726\tvalid_1's binary_logloss: 0.244095\n",
      "[971]\ttraining's auc: 0.875377\ttraining's binary_logloss: 0.20744\tvalid_1's auc: 0.763722\tvalid_1's binary_logloss: 0.244095\n",
      "[972]\ttraining's auc: 0.875456\ttraining's binary_logloss: 0.207402\tvalid_1's auc: 0.763738\tvalid_1's binary_logloss: 0.244091\n",
      "[973]\ttraining's auc: 0.875538\ttraining's binary_logloss: 0.207367\tvalid_1's auc: 0.763739\tvalid_1's binary_logloss: 0.244092\n",
      "[974]\ttraining's auc: 0.875625\ttraining's binary_logloss: 0.20733\tvalid_1's auc: 0.763745\tvalid_1's binary_logloss: 0.244094\n",
      "[975]\ttraining's auc: 0.875725\ttraining's binary_logloss: 0.207285\tvalid_1's auc: 0.763758\tvalid_1's binary_logloss: 0.244087\n",
      "[976]\ttraining's auc: 0.875794\ttraining's binary_logloss: 0.207251\tvalid_1's auc: 0.763777\tvalid_1's binary_logloss: 0.244086\n",
      "[977]\ttraining's auc: 0.875871\ttraining's binary_logloss: 0.207219\tvalid_1's auc: 0.763768\tvalid_1's binary_logloss: 0.244089\n",
      "[978]\ttraining's auc: 0.875952\ttraining's binary_logloss: 0.20718\tvalid_1's auc: 0.763809\tvalid_1's binary_logloss: 0.244082\n",
      "[979]\ttraining's auc: 0.876035\ttraining's binary_logloss: 0.207141\tvalid_1's auc: 0.763806\tvalid_1's binary_logloss: 0.244082\n",
      "[980]\ttraining's auc: 0.876138\ttraining's binary_logloss: 0.2071\tvalid_1's auc: 0.76383\tvalid_1's binary_logloss: 0.244077\n",
      "[981]\ttraining's auc: 0.876217\ttraining's binary_logloss: 0.207066\tvalid_1's auc: 0.763831\tvalid_1's binary_logloss: 0.244077\n",
      "[982]\ttraining's auc: 0.876321\ttraining's binary_logloss: 0.207027\tvalid_1's auc: 0.763818\tvalid_1's binary_logloss: 0.244079\n",
      "[983]\ttraining's auc: 0.876411\ttraining's binary_logloss: 0.206987\tvalid_1's auc: 0.763794\tvalid_1's binary_logloss: 0.24408\n",
      "[984]\ttraining's auc: 0.876482\ttraining's binary_logloss: 0.206952\tvalid_1's auc: 0.763793\tvalid_1's binary_logloss: 0.244083\n",
      "[985]\ttraining's auc: 0.876558\ttraining's binary_logloss: 0.206912\tvalid_1's auc: 0.763779\tvalid_1's binary_logloss: 0.244087\n",
      "[986]\ttraining's auc: 0.876653\ttraining's binary_logloss: 0.206871\tvalid_1's auc: 0.763796\tvalid_1's binary_logloss: 0.244078\n",
      "[987]\ttraining's auc: 0.876731\ttraining's binary_logloss: 0.206833\tvalid_1's auc: 0.763825\tvalid_1's binary_logloss: 0.24407\n",
      "[988]\ttraining's auc: 0.876827\ttraining's binary_logloss: 0.206795\tvalid_1's auc: 0.763827\tvalid_1's binary_logloss: 0.244068\n",
      "[989]\ttraining's auc: 0.876849\ttraining's binary_logloss: 0.206783\tvalid_1's auc: 0.763836\tvalid_1's binary_logloss: 0.244069\n",
      "[990]\ttraining's auc: 0.876915\ttraining's binary_logloss: 0.206752\tvalid_1's auc: 0.763843\tvalid_1's binary_logloss: 0.244065\n",
      "[991]\ttraining's auc: 0.876959\ttraining's binary_logloss: 0.206734\tvalid_1's auc: 0.763849\tvalid_1's binary_logloss: 0.244064\n",
      "[992]\ttraining's auc: 0.877061\ttraining's binary_logloss: 0.206695\tvalid_1's auc: 0.763869\tvalid_1's binary_logloss: 0.244059\n",
      "[993]\ttraining's auc: 0.877162\ttraining's binary_logloss: 0.206654\tvalid_1's auc: 0.763877\tvalid_1's binary_logloss: 0.244056\n",
      "[994]\ttraining's auc: 0.877227\ttraining's binary_logloss: 0.20662\tvalid_1's auc: 0.763883\tvalid_1's binary_logloss: 0.244054\n",
      "[995]\ttraining's auc: 0.877321\ttraining's binary_logloss: 0.206584\tvalid_1's auc: 0.763893\tvalid_1's binary_logloss: 0.244054\n",
      "[996]\ttraining's auc: 0.877368\ttraining's binary_logloss: 0.206564\tvalid_1's auc: 0.76389\tvalid_1's binary_logloss: 0.244054\n",
      "[997]\ttraining's auc: 0.877397\ttraining's binary_logloss: 0.20655\tvalid_1's auc: 0.763894\tvalid_1's binary_logloss: 0.244055\n",
      "[998]\ttraining's auc: 0.877494\ttraining's binary_logloss: 0.206508\tvalid_1's auc: 0.763888\tvalid_1's binary_logloss: 0.244059\n",
      "[999]\ttraining's auc: 0.877529\ttraining's binary_logloss: 0.206494\tvalid_1's auc: 0.763892\tvalid_1's binary_logloss: 0.244057\n",
      "[1000]\ttraining's auc: 0.877616\ttraining's binary_logloss: 0.206456\tvalid_1's auc: 0.763875\tvalid_1's binary_logloss: 0.24406\n",
      "[1001]\ttraining's auc: 0.877682\ttraining's binary_logloss: 0.206428\tvalid_1's auc: 0.763886\tvalid_1's binary_logloss: 0.244058\n",
      "[1002]\ttraining's auc: 0.877774\ttraining's binary_logloss: 0.206388\tvalid_1's auc: 0.763879\tvalid_1's binary_logloss: 0.244057\n",
      "[1003]\ttraining's auc: 0.877838\ttraining's binary_logloss: 0.206361\tvalid_1's auc: 0.763887\tvalid_1's binary_logloss: 0.244054\n",
      "[1004]\ttraining's auc: 0.877916\ttraining's binary_logloss: 0.206326\tvalid_1's auc: 0.763912\tvalid_1's binary_logloss: 0.244046\n",
      "[1005]\ttraining's auc: 0.877976\ttraining's binary_logloss: 0.206296\tvalid_1's auc: 0.763919\tvalid_1's binary_logloss: 0.244044\n",
      "[1006]\ttraining's auc: 0.878055\ttraining's binary_logloss: 0.206261\tvalid_1's auc: 0.76393\tvalid_1's binary_logloss: 0.24404\n",
      "[1007]\ttraining's auc: 0.878141\ttraining's binary_logloss: 0.206227\tvalid_1's auc: 0.763956\tvalid_1's binary_logloss: 0.244038\n",
      "[1008]\ttraining's auc: 0.878227\ttraining's binary_logloss: 0.206193\tvalid_1's auc: 0.763962\tvalid_1's binary_logloss: 0.244038\n",
      "[1009]\ttraining's auc: 0.878321\ttraining's binary_logloss: 0.206157\tvalid_1's auc: 0.763977\tvalid_1's binary_logloss: 0.244033\n",
      "[1010]\ttraining's auc: 0.87839\ttraining's binary_logloss: 0.206127\tvalid_1's auc: 0.763966\tvalid_1's binary_logloss: 0.244033\n",
      "[1011]\ttraining's auc: 0.87842\ttraining's binary_logloss: 0.206112\tvalid_1's auc: 0.763951\tvalid_1's binary_logloss: 0.244037\n",
      "[1012]\ttraining's auc: 0.878475\ttraining's binary_logloss: 0.206088\tvalid_1's auc: 0.763953\tvalid_1's binary_logloss: 0.244038\n",
      "[1013]\ttraining's auc: 0.878558\ttraining's binary_logloss: 0.206056\tvalid_1's auc: 0.763967\tvalid_1's binary_logloss: 0.244034\n",
      "[1014]\ttraining's auc: 0.878617\ttraining's binary_logloss: 0.206029\tvalid_1's auc: 0.763974\tvalid_1's binary_logloss: 0.244033\n",
      "[1015]\ttraining's auc: 0.878641\ttraining's binary_logloss: 0.206018\tvalid_1's auc: 0.763973\tvalid_1's binary_logloss: 0.244033\n",
      "[1016]\ttraining's auc: 0.87868\ttraining's binary_logloss: 0.206003\tvalid_1's auc: 0.763977\tvalid_1's binary_logloss: 0.244032\n",
      "[1017]\ttraining's auc: 0.87873\ttraining's binary_logloss: 0.205982\tvalid_1's auc: 0.763981\tvalid_1's binary_logloss: 0.244031\n",
      "[1018]\ttraining's auc: 0.878788\ttraining's binary_logloss: 0.205954\tvalid_1's auc: 0.763999\tvalid_1's binary_logloss: 0.244027\n",
      "[1019]\ttraining's auc: 0.878871\ttraining's binary_logloss: 0.205916\tvalid_1's auc: 0.764007\tvalid_1's binary_logloss: 0.244023\n",
      "[1020]\ttraining's auc: 0.878897\ttraining's binary_logloss: 0.205905\tvalid_1's auc: 0.764016\tvalid_1's binary_logloss: 0.244021\n",
      "[1021]\ttraining's auc: 0.878943\ttraining's binary_logloss: 0.205886\tvalid_1's auc: 0.764009\tvalid_1's binary_logloss: 0.244021\n",
      "[1022]\ttraining's auc: 0.878975\ttraining's binary_logloss: 0.205871\tvalid_1's auc: 0.764008\tvalid_1's binary_logloss: 0.244021\n",
      "[1023]\ttraining's auc: 0.879031\ttraining's binary_logloss: 0.205848\tvalid_1's auc: 0.764017\tvalid_1's binary_logloss: 0.244021\n",
      "[1024]\ttraining's auc: 0.879111\ttraining's binary_logloss: 0.205811\tvalid_1's auc: 0.764016\tvalid_1's binary_logloss: 0.244022\n",
      "[1025]\ttraining's auc: 0.879196\ttraining's binary_logloss: 0.205775\tvalid_1's auc: 0.764013\tvalid_1's binary_logloss: 0.244017\n",
      "[1026]\ttraining's auc: 0.879256\ttraining's binary_logloss: 0.205748\tvalid_1's auc: 0.764028\tvalid_1's binary_logloss: 0.244014\n",
      "[1027]\ttraining's auc: 0.879288\ttraining's binary_logloss: 0.205735\tvalid_1's auc: 0.764024\tvalid_1's binary_logloss: 0.244015\n",
      "[1028]\ttraining's auc: 0.879374\ttraining's binary_logloss: 0.205704\tvalid_1's auc: 0.76404\tvalid_1's binary_logloss: 0.244011\n",
      "[1029]\ttraining's auc: 0.879453\ttraining's binary_logloss: 0.205667\tvalid_1's auc: 0.764044\tvalid_1's binary_logloss: 0.244012\n",
      "[1030]\ttraining's auc: 0.879498\ttraining's binary_logloss: 0.205648\tvalid_1's auc: 0.764054\tvalid_1's binary_logloss: 0.244011\n",
      "[1031]\ttraining's auc: 0.879589\ttraining's binary_logloss: 0.20561\tvalid_1's auc: 0.764023\tvalid_1's binary_logloss: 0.244022\n",
      "[1032]\ttraining's auc: 0.879648\ttraining's binary_logloss: 0.205581\tvalid_1's auc: 0.764025\tvalid_1's binary_logloss: 0.244022\n",
      "[1033]\ttraining's auc: 0.879729\ttraining's binary_logloss: 0.205546\tvalid_1's auc: 0.764042\tvalid_1's binary_logloss: 0.244019\n",
      "[1034]\ttraining's auc: 0.879777\ttraining's binary_logloss: 0.205526\tvalid_1's auc: 0.764027\tvalid_1's binary_logloss: 0.244022\n",
      "[1035]\ttraining's auc: 0.879824\ttraining's binary_logloss: 0.205505\tvalid_1's auc: 0.764036\tvalid_1's binary_logloss: 0.24402\n",
      "[1036]\ttraining's auc: 0.879916\ttraining's binary_logloss: 0.205469\tvalid_1's auc: 0.764033\tvalid_1's binary_logloss: 0.244021\n",
      "[1037]\ttraining's auc: 0.87996\ttraining's binary_logloss: 0.205452\tvalid_1's auc: 0.764026\tvalid_1's binary_logloss: 0.244026\n",
      "[1038]\ttraining's auc: 0.880056\ttraining's binary_logloss: 0.205415\tvalid_1's auc: 0.764039\tvalid_1's binary_logloss: 0.244023\n",
      "[1039]\ttraining's auc: 0.880123\ttraining's binary_logloss: 0.205387\tvalid_1's auc: 0.764026\tvalid_1's binary_logloss: 0.244025\n",
      "[1040]\ttraining's auc: 0.88016\ttraining's binary_logloss: 0.20537\tvalid_1's auc: 0.764029\tvalid_1's binary_logloss: 0.244025\n",
      "[1041]\ttraining's auc: 0.880237\ttraining's binary_logloss: 0.205337\tvalid_1's auc: 0.764048\tvalid_1's binary_logloss: 0.244021\n",
      "[1042]\ttraining's auc: 0.880302\ttraining's binary_logloss: 0.205309\tvalid_1's auc: 0.764044\tvalid_1's binary_logloss: 0.244018\n",
      "[1043]\ttraining's auc: 0.88037\ttraining's binary_logloss: 0.20528\tvalid_1's auc: 0.764061\tvalid_1's binary_logloss: 0.244017\n",
      "[1044]\ttraining's auc: 0.880434\ttraining's binary_logloss: 0.205253\tvalid_1's auc: 0.764034\tvalid_1's binary_logloss: 0.244022\n",
      "[1045]\ttraining's auc: 0.880526\ttraining's binary_logloss: 0.205216\tvalid_1's auc: 0.764026\tvalid_1's binary_logloss: 0.244028\n",
      "[1046]\ttraining's auc: 0.880585\ttraining's binary_logloss: 0.205193\tvalid_1's auc: 0.764036\tvalid_1's binary_logloss: 0.244024\n",
      "[1047]\ttraining's auc: 0.880645\ttraining's binary_logloss: 0.205167\tvalid_1's auc: 0.764042\tvalid_1's binary_logloss: 0.244021\n",
      "[1048]\ttraining's auc: 0.880702\ttraining's binary_logloss: 0.205145\tvalid_1's auc: 0.76405\tvalid_1's binary_logloss: 0.244021\n",
      "[1049]\ttraining's auc: 0.880777\ttraining's binary_logloss: 0.20511\tvalid_1's auc: 0.764025\tvalid_1's binary_logloss: 0.244028\n",
      "[1050]\ttraining's auc: 0.880873\ttraining's binary_logloss: 0.205073\tvalid_1's auc: 0.764031\tvalid_1's binary_logloss: 0.244026\n",
      "[1051]\ttraining's auc: 0.880947\ttraining's binary_logloss: 0.205041\tvalid_1's auc: 0.764033\tvalid_1's binary_logloss: 0.244026\n",
      "[1052]\ttraining's auc: 0.881027\ttraining's binary_logloss: 0.205006\tvalid_1's auc: 0.764036\tvalid_1's binary_logloss: 0.244026\n",
      "[1053]\ttraining's auc: 0.881113\ttraining's binary_logloss: 0.204967\tvalid_1's auc: 0.764035\tvalid_1's binary_logloss: 0.244029\n",
      "[1054]\ttraining's auc: 0.881163\ttraining's binary_logloss: 0.204947\tvalid_1's auc: 0.764037\tvalid_1's binary_logloss: 0.244027\n",
      "[1055]\ttraining's auc: 0.881212\ttraining's binary_logloss: 0.204923\tvalid_1's auc: 0.764053\tvalid_1's binary_logloss: 0.244022\n",
      "[1056]\ttraining's auc: 0.881277\ttraining's binary_logloss: 0.204893\tvalid_1's auc: 0.764087\tvalid_1's binary_logloss: 0.244016\n",
      "[1057]\ttraining's auc: 0.881363\ttraining's binary_logloss: 0.204857\tvalid_1's auc: 0.764105\tvalid_1's binary_logloss: 0.244014\n",
      "[1058]\ttraining's auc: 0.881432\ttraining's binary_logloss: 0.204819\tvalid_1's auc: 0.764109\tvalid_1's binary_logloss: 0.244011\n",
      "[1059]\ttraining's auc: 0.881506\ttraining's binary_logloss: 0.204784\tvalid_1's auc: 0.764117\tvalid_1's binary_logloss: 0.244007\n",
      "[1060]\ttraining's auc: 0.881599\ttraining's binary_logloss: 0.204749\tvalid_1's auc: 0.764145\tvalid_1's binary_logloss: 0.244003\n",
      "[1061]\ttraining's auc: 0.881655\ttraining's binary_logloss: 0.204725\tvalid_1's auc: 0.76415\tvalid_1's binary_logloss: 0.244002\n",
      "[1062]\ttraining's auc: 0.881731\ttraining's binary_logloss: 0.204692\tvalid_1's auc: 0.76414\tvalid_1's binary_logloss: 0.244002\n",
      "[1063]\ttraining's auc: 0.881807\ttraining's binary_logloss: 0.204657\tvalid_1's auc: 0.764114\tvalid_1's binary_logloss: 0.244006\n",
      "[1064]\ttraining's auc: 0.881856\ttraining's binary_logloss: 0.204632\tvalid_1's auc: 0.764108\tvalid_1's binary_logloss: 0.244008\n",
      "[1065]\ttraining's auc: 0.88194\ttraining's binary_logloss: 0.204593\tvalid_1's auc: 0.764123\tvalid_1's binary_logloss: 0.244007\n",
      "[1066]\ttraining's auc: 0.882012\ttraining's binary_logloss: 0.204556\tvalid_1's auc: 0.764128\tvalid_1's binary_logloss: 0.244009\n",
      "[1067]\ttraining's auc: 0.882098\ttraining's binary_logloss: 0.204518\tvalid_1's auc: 0.76414\tvalid_1's binary_logloss: 0.244005\n",
      "[1068]\ttraining's auc: 0.882127\ttraining's binary_logloss: 0.204506\tvalid_1's auc: 0.764113\tvalid_1's binary_logloss: 0.24401\n",
      "[1069]\ttraining's auc: 0.882204\ttraining's binary_logloss: 0.204469\tvalid_1's auc: 0.764106\tvalid_1's binary_logloss: 0.244012\n",
      "[1070]\ttraining's auc: 0.882256\ttraining's binary_logloss: 0.204447\tvalid_1's auc: 0.764122\tvalid_1's binary_logloss: 0.244008\n",
      "[1071]\ttraining's auc: 0.882323\ttraining's binary_logloss: 0.204411\tvalid_1's auc: 0.764138\tvalid_1's binary_logloss: 0.244006\n",
      "[1072]\ttraining's auc: 0.882351\ttraining's binary_logloss: 0.204398\tvalid_1's auc: 0.764141\tvalid_1's binary_logloss: 0.244004\n",
      "[1073]\ttraining's auc: 0.882429\ttraining's binary_logloss: 0.204367\tvalid_1's auc: 0.764121\tvalid_1's binary_logloss: 0.244007\n",
      "[1074]\ttraining's auc: 0.882512\ttraining's binary_logloss: 0.204334\tvalid_1's auc: 0.764138\tvalid_1's binary_logloss: 0.244004\n",
      "[1075]\ttraining's auc: 0.882589\ttraining's binary_logloss: 0.204298\tvalid_1's auc: 0.76415\tvalid_1's binary_logloss: 0.244001\n",
      "[1076]\ttraining's auc: 0.882622\ttraining's binary_logloss: 0.204281\tvalid_1's auc: 0.764132\tvalid_1's binary_logloss: 0.244007\n",
      "[1077]\ttraining's auc: 0.882685\ttraining's binary_logloss: 0.204252\tvalid_1's auc: 0.764139\tvalid_1's binary_logloss: 0.244006\n",
      "[1078]\ttraining's auc: 0.882723\ttraining's binary_logloss: 0.204234\tvalid_1's auc: 0.764143\tvalid_1's binary_logloss: 0.244007\n",
      "[1079]\ttraining's auc: 0.882773\ttraining's binary_logloss: 0.204215\tvalid_1's auc: 0.764135\tvalid_1's binary_logloss: 0.244009\n",
      "[1080]\ttraining's auc: 0.882806\ttraining's binary_logloss: 0.204198\tvalid_1's auc: 0.764127\tvalid_1's binary_logloss: 0.244008\n",
      "[1081]\ttraining's auc: 0.882844\ttraining's binary_logloss: 0.20418\tvalid_1's auc: 0.764128\tvalid_1's binary_logloss: 0.244007\n",
      "[1082]\ttraining's auc: 0.882895\ttraining's binary_logloss: 0.20416\tvalid_1's auc: 0.764118\tvalid_1's binary_logloss: 0.244009\n",
      "[1083]\ttraining's auc: 0.882965\ttraining's binary_logloss: 0.204128\tvalid_1's auc: 0.764126\tvalid_1's binary_logloss: 0.244009\n",
      "[1084]\ttraining's auc: 0.88299\ttraining's binary_logloss: 0.204118\tvalid_1's auc: 0.764117\tvalid_1's binary_logloss: 0.244011\n",
      "[1085]\ttraining's auc: 0.883049\ttraining's binary_logloss: 0.204083\tvalid_1's auc: 0.764119\tvalid_1's binary_logloss: 0.244011\n",
      "[1086]\ttraining's auc: 0.883097\ttraining's binary_logloss: 0.204059\tvalid_1's auc: 0.764095\tvalid_1's binary_logloss: 0.244017\n",
      "[1087]\ttraining's auc: 0.883159\ttraining's binary_logloss: 0.204032\tvalid_1's auc: 0.764111\tvalid_1's binary_logloss: 0.244011\n",
      "[1088]\ttraining's auc: 0.883243\ttraining's binary_logloss: 0.203998\tvalid_1's auc: 0.764138\tvalid_1's binary_logloss: 0.244004\n",
      "[1089]\ttraining's auc: 0.883288\ttraining's binary_logloss: 0.203974\tvalid_1's auc: 0.764131\tvalid_1's binary_logloss: 0.244006\n",
      "[1090]\ttraining's auc: 0.88334\ttraining's binary_logloss: 0.203951\tvalid_1's auc: 0.764145\tvalid_1's binary_logloss: 0.244004\n",
      "[1091]\ttraining's auc: 0.883424\ttraining's binary_logloss: 0.203919\tvalid_1's auc: 0.764154\tvalid_1's binary_logloss: 0.244003\n",
      "[1092]\ttraining's auc: 0.883494\ttraining's binary_logloss: 0.203894\tvalid_1's auc: 0.764153\tvalid_1's binary_logloss: 0.244005\n",
      "[1093]\ttraining's auc: 0.883534\ttraining's binary_logloss: 0.203874\tvalid_1's auc: 0.764148\tvalid_1's binary_logloss: 0.244005\n",
      "[1094]\ttraining's auc: 0.883615\ttraining's binary_logloss: 0.203836\tvalid_1's auc: 0.764158\tvalid_1's binary_logloss: 0.244\n",
      "[1095]\ttraining's auc: 0.883668\ttraining's binary_logloss: 0.203813\tvalid_1's auc: 0.764159\tvalid_1's binary_logloss: 0.244001\n",
      "[1096]\ttraining's auc: 0.883751\ttraining's binary_logloss: 0.203784\tvalid_1's auc: 0.764176\tvalid_1's binary_logloss: 0.243998\n",
      "[1097]\ttraining's auc: 0.883825\ttraining's binary_logloss: 0.203754\tvalid_1's auc: 0.764184\tvalid_1's binary_logloss: 0.243997\n",
      "[1098]\ttraining's auc: 0.883893\ttraining's binary_logloss: 0.20372\tvalid_1's auc: 0.764175\tvalid_1's binary_logloss: 0.243997\n",
      "[1099]\ttraining's auc: 0.883967\ttraining's binary_logloss: 0.203685\tvalid_1's auc: 0.764187\tvalid_1's binary_logloss: 0.243994\n",
      "[1100]\ttraining's auc: 0.884012\ttraining's binary_logloss: 0.203664\tvalid_1's auc: 0.764199\tvalid_1's binary_logloss: 0.243993\n",
      "[1101]\ttraining's auc: 0.88405\ttraining's binary_logloss: 0.203645\tvalid_1's auc: 0.764227\tvalid_1's binary_logloss: 0.243989\n",
      "[1102]\ttraining's auc: 0.88413\ttraining's binary_logloss: 0.203609\tvalid_1's auc: 0.764231\tvalid_1's binary_logloss: 0.243988\n",
      "[1103]\ttraining's auc: 0.884159\ttraining's binary_logloss: 0.203597\tvalid_1's auc: 0.764232\tvalid_1's binary_logloss: 0.243988\n",
      "[1104]\ttraining's auc: 0.884217\ttraining's binary_logloss: 0.20357\tvalid_1's auc: 0.764224\tvalid_1's binary_logloss: 0.243989\n",
      "[1105]\ttraining's auc: 0.88428\ttraining's binary_logloss: 0.203544\tvalid_1's auc: 0.76423\tvalid_1's binary_logloss: 0.243987\n",
      "[1106]\ttraining's auc: 0.88433\ttraining's binary_logloss: 0.203523\tvalid_1's auc: 0.764229\tvalid_1's binary_logloss: 0.243989\n",
      "[1107]\ttraining's auc: 0.884357\ttraining's binary_logloss: 0.203512\tvalid_1's auc: 0.764238\tvalid_1's binary_logloss: 0.243988\n",
      "[1108]\ttraining's auc: 0.884408\ttraining's binary_logloss: 0.203489\tvalid_1's auc: 0.764258\tvalid_1's binary_logloss: 0.243983\n",
      "[1109]\ttraining's auc: 0.884479\ttraining's binary_logloss: 0.203452\tvalid_1's auc: 0.764282\tvalid_1's binary_logloss: 0.243978\n",
      "[1110]\ttraining's auc: 0.884559\ttraining's binary_logloss: 0.203419\tvalid_1's auc: 0.764258\tvalid_1's binary_logloss: 0.243985\n",
      "[1111]\ttraining's auc: 0.884629\ttraining's binary_logloss: 0.203388\tvalid_1's auc: 0.76426\tvalid_1's binary_logloss: 0.243985\n",
      "[1112]\ttraining's auc: 0.884696\ttraining's binary_logloss: 0.203359\tvalid_1's auc: 0.764268\tvalid_1's binary_logloss: 0.24398\n",
      "[1113]\ttraining's auc: 0.884767\ttraining's binary_logloss: 0.203326\tvalid_1's auc: 0.764275\tvalid_1's binary_logloss: 0.243982\n",
      "[1114]\ttraining's auc: 0.884847\ttraining's binary_logloss: 0.203289\tvalid_1's auc: 0.764284\tvalid_1's binary_logloss: 0.243976\n",
      "[1115]\ttraining's auc: 0.884866\ttraining's binary_logloss: 0.20328\tvalid_1's auc: 0.764274\tvalid_1's binary_logloss: 0.243978\n",
      "[1116]\ttraining's auc: 0.884948\ttraining's binary_logloss: 0.203247\tvalid_1's auc: 0.764263\tvalid_1's binary_logloss: 0.243978\n",
      "[1117]\ttraining's auc: 0.88502\ttraining's binary_logloss: 0.203214\tvalid_1's auc: 0.764263\tvalid_1's binary_logloss: 0.243981\n",
      "[1118]\ttraining's auc: 0.885084\ttraining's binary_logloss: 0.203186\tvalid_1's auc: 0.764273\tvalid_1's binary_logloss: 0.243978\n",
      "[1119]\ttraining's auc: 0.885157\ttraining's binary_logloss: 0.203147\tvalid_1's auc: 0.764251\tvalid_1's binary_logloss: 0.243981\n",
      "[1120]\ttraining's auc: 0.885236\ttraining's binary_logloss: 0.203112\tvalid_1's auc: 0.764243\tvalid_1's binary_logloss: 0.243983\n",
      "[1121]\ttraining's auc: 0.885312\ttraining's binary_logloss: 0.20308\tvalid_1's auc: 0.764227\tvalid_1's binary_logloss: 0.243987\n",
      "[1122]\ttraining's auc: 0.885384\ttraining's binary_logloss: 0.203046\tvalid_1's auc: 0.764239\tvalid_1's binary_logloss: 0.243984\n",
      "[1123]\ttraining's auc: 0.885452\ttraining's binary_logloss: 0.203015\tvalid_1's auc: 0.764226\tvalid_1's binary_logloss: 0.243987\n",
      "[1124]\ttraining's auc: 0.885509\ttraining's binary_logloss: 0.202989\tvalid_1's auc: 0.764205\tvalid_1's binary_logloss: 0.243992\n",
      "[1125]\ttraining's auc: 0.885589\ttraining's binary_logloss: 0.202954\tvalid_1's auc: 0.764206\tvalid_1's binary_logloss: 0.243991\n",
      "[1126]\ttraining's auc: 0.885676\ttraining's binary_logloss: 0.202913\tvalid_1's auc: 0.764228\tvalid_1's binary_logloss: 0.243983\n",
      "[1127]\ttraining's auc: 0.885746\ttraining's binary_logloss: 0.202881\tvalid_1's auc: 0.764228\tvalid_1's binary_logloss: 0.243982\n",
      "[1128]\ttraining's auc: 0.885802\ttraining's binary_logloss: 0.202852\tvalid_1's auc: 0.764226\tvalid_1's binary_logloss: 0.243983\n",
      "[1129]\ttraining's auc: 0.885851\ttraining's binary_logloss: 0.20283\tvalid_1's auc: 0.764221\tvalid_1's binary_logloss: 0.243984\n",
      "[1130]\ttraining's auc: 0.885924\ttraining's binary_logloss: 0.202796\tvalid_1's auc: 0.76424\tvalid_1's binary_logloss: 0.24398\n",
      "[1131]\ttraining's auc: 0.885994\ttraining's binary_logloss: 0.202761\tvalid_1's auc: 0.764234\tvalid_1's binary_logloss: 0.243982\n",
      "[1132]\ttraining's auc: 0.886065\ttraining's binary_logloss: 0.202726\tvalid_1's auc: 0.764245\tvalid_1's binary_logloss: 0.243982\n",
      "[1133]\ttraining's auc: 0.886128\ttraining's binary_logloss: 0.2027\tvalid_1's auc: 0.764245\tvalid_1's binary_logloss: 0.243982\n",
      "[1134]\ttraining's auc: 0.886184\ttraining's binary_logloss: 0.202679\tvalid_1's auc: 0.764234\tvalid_1's binary_logloss: 0.243985\n",
      "[1135]\ttraining's auc: 0.886268\ttraining's binary_logloss: 0.202641\tvalid_1's auc: 0.764229\tvalid_1's binary_logloss: 0.243983\n",
      "[1136]\ttraining's auc: 0.88633\ttraining's binary_logloss: 0.20261\tvalid_1's auc: 0.764235\tvalid_1's binary_logloss: 0.243982\n",
      "[1137]\ttraining's auc: 0.886387\ttraining's binary_logloss: 0.202586\tvalid_1's auc: 0.764238\tvalid_1's binary_logloss: 0.243979\n",
      "[1138]\ttraining's auc: 0.886421\ttraining's binary_logloss: 0.202572\tvalid_1's auc: 0.764239\tvalid_1's binary_logloss: 0.243981\n",
      "[1139]\ttraining's auc: 0.886492\ttraining's binary_logloss: 0.202537\tvalid_1's auc: 0.764251\tvalid_1's binary_logloss: 0.243975\n",
      "[1140]\ttraining's auc: 0.886541\ttraining's binary_logloss: 0.202515\tvalid_1's auc: 0.764264\tvalid_1's binary_logloss: 0.243975\n",
      "[1141]\ttraining's auc: 0.886595\ttraining's binary_logloss: 0.202489\tvalid_1's auc: 0.764256\tvalid_1's binary_logloss: 0.243978\n",
      "[1142]\ttraining's auc: 0.886645\ttraining's binary_logloss: 0.202466\tvalid_1's auc: 0.764248\tvalid_1's binary_logloss: 0.243979\n",
      "[1143]\ttraining's auc: 0.886725\ttraining's binary_logloss: 0.202432\tvalid_1's auc: 0.764236\tvalid_1's binary_logloss: 0.243982\n",
      "[1144]\ttraining's auc: 0.886789\ttraining's binary_logloss: 0.202399\tvalid_1's auc: 0.76425\tvalid_1's binary_logloss: 0.243977\n",
      "[1145]\ttraining's auc: 0.886813\ttraining's binary_logloss: 0.202387\tvalid_1's auc: 0.764234\tvalid_1's binary_logloss: 0.243981\n",
      "[1146]\ttraining's auc: 0.886892\ttraining's binary_logloss: 0.202351\tvalid_1's auc: 0.764256\tvalid_1's binary_logloss: 0.24398\n",
      "[1147]\ttraining's auc: 0.886946\ttraining's binary_logloss: 0.202328\tvalid_1's auc: 0.764238\tvalid_1's binary_logloss: 0.243984\n",
      "[1148]\ttraining's auc: 0.887004\ttraining's binary_logloss: 0.202307\tvalid_1's auc: 0.764241\tvalid_1's binary_logloss: 0.243981\n",
      "[1149]\ttraining's auc: 0.887035\ttraining's binary_logloss: 0.20229\tvalid_1's auc: 0.764236\tvalid_1's binary_logloss: 0.243981\n",
      "[1150]\ttraining's auc: 0.887115\ttraining's binary_logloss: 0.202254\tvalid_1's auc: 0.764232\tvalid_1's binary_logloss: 0.243982\n",
      "[1151]\ttraining's auc: 0.88717\ttraining's binary_logloss: 0.202227\tvalid_1's auc: 0.764222\tvalid_1's binary_logloss: 0.243983\n",
      "[1152]\ttraining's auc: 0.887244\ttraining's binary_logloss: 0.202192\tvalid_1's auc: 0.764244\tvalid_1's binary_logloss: 0.243978\n",
      "[1153]\ttraining's auc: 0.887326\ttraining's binary_logloss: 0.202158\tvalid_1's auc: 0.764227\tvalid_1's binary_logloss: 0.243985\n",
      "[1154]\ttraining's auc: 0.887408\ttraining's binary_logloss: 0.202123\tvalid_1's auc: 0.764227\tvalid_1's binary_logloss: 0.243983\n",
      "[1155]\ttraining's auc: 0.887501\ttraining's binary_logloss: 0.202087\tvalid_1's auc: 0.764208\tvalid_1's binary_logloss: 0.243989\n",
      "[1156]\ttraining's auc: 0.887573\ttraining's binary_logloss: 0.202053\tvalid_1's auc: 0.764221\tvalid_1's binary_logloss: 0.243986\n",
      "[1157]\ttraining's auc: 0.887621\ttraining's binary_logloss: 0.202031\tvalid_1's auc: 0.764229\tvalid_1's binary_logloss: 0.243984\n",
      "[1158]\ttraining's auc: 0.887645\ttraining's binary_logloss: 0.20202\tvalid_1's auc: 0.764233\tvalid_1's binary_logloss: 0.243983\n",
      "[1159]\ttraining's auc: 0.887708\ttraining's binary_logloss: 0.201988\tvalid_1's auc: 0.764255\tvalid_1's binary_logloss: 0.243976\n",
      "[1160]\ttraining's auc: 0.887766\ttraining's binary_logloss: 0.201961\tvalid_1's auc: 0.764256\tvalid_1's binary_logloss: 0.243974\n",
      "[1161]\ttraining's auc: 0.887798\ttraining's binary_logloss: 0.201943\tvalid_1's auc: 0.764267\tvalid_1's binary_logloss: 0.243972\n",
      "[1162]\ttraining's auc: 0.887861\ttraining's binary_logloss: 0.20191\tvalid_1's auc: 0.764294\tvalid_1's binary_logloss: 0.243966\n",
      "[1163]\ttraining's auc: 0.887897\ttraining's binary_logloss: 0.201895\tvalid_1's auc: 0.764303\tvalid_1's binary_logloss: 0.243964\n",
      "[1164]\ttraining's auc: 0.887968\ttraining's binary_logloss: 0.201864\tvalid_1's auc: 0.764298\tvalid_1's binary_logloss: 0.243967\n",
      "[1165]\ttraining's auc: 0.888025\ttraining's binary_logloss: 0.201832\tvalid_1's auc: 0.764296\tvalid_1's binary_logloss: 0.243971\n",
      "[1166]\ttraining's auc: 0.888077\ttraining's binary_logloss: 0.201801\tvalid_1's auc: 0.764285\tvalid_1's binary_logloss: 0.243975\n",
      "[1167]\ttraining's auc: 0.888139\ttraining's binary_logloss: 0.201771\tvalid_1's auc: 0.764293\tvalid_1's binary_logloss: 0.243972\n",
      "[1168]\ttraining's auc: 0.888181\ttraining's binary_logloss: 0.20175\tvalid_1's auc: 0.764287\tvalid_1's binary_logloss: 0.243977\n",
      "[1169]\ttraining's auc: 0.888246\ttraining's binary_logloss: 0.201719\tvalid_1's auc: 0.764289\tvalid_1's binary_logloss: 0.243976\n",
      "[1170]\ttraining's auc: 0.888291\ttraining's binary_logloss: 0.201699\tvalid_1's auc: 0.764297\tvalid_1's binary_logloss: 0.243972\n",
      "[1171]\ttraining's auc: 0.888359\ttraining's binary_logloss: 0.201664\tvalid_1's auc: 0.764303\tvalid_1's binary_logloss: 0.243971\n",
      "[1172]\ttraining's auc: 0.888392\ttraining's binary_logloss: 0.201648\tvalid_1's auc: 0.764309\tvalid_1's binary_logloss: 0.243968\n",
      "[1173]\ttraining's auc: 0.888439\ttraining's binary_logloss: 0.201631\tvalid_1's auc: 0.76431\tvalid_1's binary_logloss: 0.243968\n",
      "[1174]\ttraining's auc: 0.888503\ttraining's binary_logloss: 0.201597\tvalid_1's auc: 0.764303\tvalid_1's binary_logloss: 0.243972\n",
      "[1175]\ttraining's auc: 0.888572\ttraining's binary_logloss: 0.201565\tvalid_1's auc: 0.764288\tvalid_1's binary_logloss: 0.243975\n",
      "[1176]\ttraining's auc: 0.888618\ttraining's binary_logloss: 0.201542\tvalid_1's auc: 0.764293\tvalid_1's binary_logloss: 0.243975\n",
      "[1177]\ttraining's auc: 0.88864\ttraining's binary_logloss: 0.201531\tvalid_1's auc: 0.764293\tvalid_1's binary_logloss: 0.243974\n",
      "[1178]\ttraining's auc: 0.888679\ttraining's binary_logloss: 0.201511\tvalid_1's auc: 0.764301\tvalid_1's binary_logloss: 0.243971\n",
      "[1179]\ttraining's auc: 0.888748\ttraining's binary_logloss: 0.201482\tvalid_1's auc: 0.764289\tvalid_1's binary_logloss: 0.243975\n",
      "[1180]\ttraining's auc: 0.888812\ttraining's binary_logloss: 0.201456\tvalid_1's auc: 0.764287\tvalid_1's binary_logloss: 0.243973\n",
      "[1181]\ttraining's auc: 0.888843\ttraining's binary_logloss: 0.20144\tvalid_1's auc: 0.764292\tvalid_1's binary_logloss: 0.243974\n",
      "[1182]\ttraining's auc: 0.888922\ttraining's binary_logloss: 0.201405\tvalid_1's auc: 0.764266\tvalid_1's binary_logloss: 0.243981\n",
      "[1183]\ttraining's auc: 0.888986\ttraining's binary_logloss: 0.201378\tvalid_1's auc: 0.764257\tvalid_1's binary_logloss: 0.243983\n",
      "[1184]\ttraining's auc: 0.889064\ttraining's binary_logloss: 0.201351\tvalid_1's auc: 0.764253\tvalid_1's binary_logloss: 0.243987\n",
      "[1185]\ttraining's auc: 0.889124\ttraining's binary_logloss: 0.201321\tvalid_1's auc: 0.764231\tvalid_1's binary_logloss: 0.24399\n",
      "[1186]\ttraining's auc: 0.889184\ttraining's binary_logloss: 0.201293\tvalid_1's auc: 0.764223\tvalid_1's binary_logloss: 0.243994\n",
      "[1187]\ttraining's auc: 0.889236\ttraining's binary_logloss: 0.201268\tvalid_1's auc: 0.764234\tvalid_1's binary_logloss: 0.243992\n",
      "[1188]\ttraining's auc: 0.889322\ttraining's binary_logloss: 0.201228\tvalid_1's auc: 0.764268\tvalid_1's binary_logloss: 0.243982\n",
      "[1189]\ttraining's auc: 0.889378\ttraining's binary_logloss: 0.2012\tvalid_1's auc: 0.764291\tvalid_1's binary_logloss: 0.243975\n",
      "[1190]\ttraining's auc: 0.889405\ttraining's binary_logloss: 0.201187\tvalid_1's auc: 0.76429\tvalid_1's binary_logloss: 0.243975\n",
      "[1191]\ttraining's auc: 0.889472\ttraining's binary_logloss: 0.201155\tvalid_1's auc: 0.764283\tvalid_1's binary_logloss: 0.243976\n",
      "[1192]\ttraining's auc: 0.889485\ttraining's binary_logloss: 0.201147\tvalid_1's auc: 0.764282\tvalid_1's binary_logloss: 0.243974\n",
      "[1193]\ttraining's auc: 0.889546\ttraining's binary_logloss: 0.201116\tvalid_1's auc: 0.764283\tvalid_1's binary_logloss: 0.243975\n",
      "[1194]\ttraining's auc: 0.889608\ttraining's binary_logloss: 0.201082\tvalid_1's auc: 0.764293\tvalid_1's binary_logloss: 0.243973\n",
      "[1195]\ttraining's auc: 0.889665\ttraining's binary_logloss: 0.201052\tvalid_1's auc: 0.764305\tvalid_1's binary_logloss: 0.243972\n",
      "[1196]\ttraining's auc: 0.889749\ttraining's binary_logloss: 0.20102\tvalid_1's auc: 0.764302\tvalid_1's binary_logloss: 0.243973\n",
      "[1197]\ttraining's auc: 0.889792\ttraining's binary_logloss: 0.200995\tvalid_1's auc: 0.764287\tvalid_1's binary_logloss: 0.243977\n",
      "[1198]\ttraining's auc: 0.889836\ttraining's binary_logloss: 0.200977\tvalid_1's auc: 0.76429\tvalid_1's binary_logloss: 0.243976\n",
      "[1199]\ttraining's auc: 0.889861\ttraining's binary_logloss: 0.200962\tvalid_1's auc: 0.764289\tvalid_1's binary_logloss: 0.243977\n",
      "[1200]\ttraining's auc: 0.889914\ttraining's binary_logloss: 0.200941\tvalid_1's auc: 0.764313\tvalid_1's binary_logloss: 0.243972\n",
      "[1201]\ttraining's auc: 0.890004\ttraining's binary_logloss: 0.200905\tvalid_1's auc: 0.764322\tvalid_1's binary_logloss: 0.24397\n",
      "[1202]\ttraining's auc: 0.890078\ttraining's binary_logloss: 0.200872\tvalid_1's auc: 0.764326\tvalid_1's binary_logloss: 0.243973\n",
      "[1203]\ttraining's auc: 0.890159\ttraining's binary_logloss: 0.200839\tvalid_1's auc: 0.76433\tvalid_1's binary_logloss: 0.243971\n",
      "[1204]\ttraining's auc: 0.890204\ttraining's binary_logloss: 0.200819\tvalid_1's auc: 0.764329\tvalid_1's binary_logloss: 0.243971\n",
      "[1205]\ttraining's auc: 0.890269\ttraining's binary_logloss: 0.200789\tvalid_1's auc: 0.764331\tvalid_1's binary_logloss: 0.243973\n",
      "[1206]\ttraining's auc: 0.890342\ttraining's binary_logloss: 0.200756\tvalid_1's auc: 0.764322\tvalid_1's binary_logloss: 0.243975\n",
      "[1207]\ttraining's auc: 0.890403\ttraining's binary_logloss: 0.200718\tvalid_1's auc: 0.764309\tvalid_1's binary_logloss: 0.24398\n",
      "[1208]\ttraining's auc: 0.890478\ttraining's binary_logloss: 0.200683\tvalid_1's auc: 0.764307\tvalid_1's binary_logloss: 0.243977\n",
      "[1209]\ttraining's auc: 0.890535\ttraining's binary_logloss: 0.200657\tvalid_1's auc: 0.764307\tvalid_1's binary_logloss: 0.243977\n",
      "[1210]\ttraining's auc: 0.890614\ttraining's binary_logloss: 0.200623\tvalid_1's auc: 0.764306\tvalid_1's binary_logloss: 0.243977\n",
      "[1211]\ttraining's auc: 0.890684\ttraining's binary_logloss: 0.200591\tvalid_1's auc: 0.764314\tvalid_1's binary_logloss: 0.243978\n",
      "[1212]\ttraining's auc: 0.890712\ttraining's binary_logloss: 0.200575\tvalid_1's auc: 0.764313\tvalid_1's binary_logloss: 0.243977\n",
      "[1213]\ttraining's auc: 0.890775\ttraining's binary_logloss: 0.200547\tvalid_1's auc: 0.764318\tvalid_1's binary_logloss: 0.243976\n",
      "[1214]\ttraining's auc: 0.890826\ttraining's binary_logloss: 0.200523\tvalid_1's auc: 0.764328\tvalid_1's binary_logloss: 0.243975\n",
      "[1215]\ttraining's auc: 0.890868\ttraining's binary_logloss: 0.200503\tvalid_1's auc: 0.764328\tvalid_1's binary_logloss: 0.243977\n",
      "[1216]\ttraining's auc: 0.890918\ttraining's binary_logloss: 0.200482\tvalid_1's auc: 0.764333\tvalid_1's binary_logloss: 0.243975\n",
      "[1217]\ttraining's auc: 0.890983\ttraining's binary_logloss: 0.200451\tvalid_1's auc: 0.764331\tvalid_1's binary_logloss: 0.243976\n",
      "[1218]\ttraining's auc: 0.891067\ttraining's binary_logloss: 0.200416\tvalid_1's auc: 0.764334\tvalid_1's binary_logloss: 0.243977\n",
      "[1219]\ttraining's auc: 0.891116\ttraining's binary_logloss: 0.200394\tvalid_1's auc: 0.764351\tvalid_1's binary_logloss: 0.243973\n",
      "[1220]\ttraining's auc: 0.891175\ttraining's binary_logloss: 0.200363\tvalid_1's auc: 0.764363\tvalid_1's binary_logloss: 0.243969\n",
      "[1221]\ttraining's auc: 0.891256\ttraining's binary_logloss: 0.200325\tvalid_1's auc: 0.764386\tvalid_1's binary_logloss: 0.243968\n",
      "[1222]\ttraining's auc: 0.891316\ttraining's binary_logloss: 0.200296\tvalid_1's auc: 0.764371\tvalid_1's binary_logloss: 0.243971\n",
      "[1223]\ttraining's auc: 0.891349\ttraining's binary_logloss: 0.200284\tvalid_1's auc: 0.764373\tvalid_1's binary_logloss: 0.243971\n",
      "[1224]\ttraining's auc: 0.891397\ttraining's binary_logloss: 0.200262\tvalid_1's auc: 0.764375\tvalid_1's binary_logloss: 0.243971\n",
      "[1225]\ttraining's auc: 0.891423\ttraining's binary_logloss: 0.200249\tvalid_1's auc: 0.76438\tvalid_1's binary_logloss: 0.243969\n",
      "[1226]\ttraining's auc: 0.89148\ttraining's binary_logloss: 0.200223\tvalid_1's auc: 0.76438\tvalid_1's binary_logloss: 0.24397\n",
      "[1227]\ttraining's auc: 0.891519\ttraining's binary_logloss: 0.200204\tvalid_1's auc: 0.764394\tvalid_1's binary_logloss: 0.243969\n",
      "[1228]\ttraining's auc: 0.891592\ttraining's binary_logloss: 0.200168\tvalid_1's auc: 0.764377\tvalid_1's binary_logloss: 0.243974\n",
      "[1229]\ttraining's auc: 0.891658\ttraining's binary_logloss: 0.200139\tvalid_1's auc: 0.764387\tvalid_1's binary_logloss: 0.243973\n",
      "[1230]\ttraining's auc: 0.891719\ttraining's binary_logloss: 0.20011\tvalid_1's auc: 0.764401\tvalid_1's binary_logloss: 0.243968\n",
      "[1231]\ttraining's auc: 0.891763\ttraining's binary_logloss: 0.200089\tvalid_1's auc: 0.764441\tvalid_1's binary_logloss: 0.243961\n",
      "[1232]\ttraining's auc: 0.891798\ttraining's binary_logloss: 0.200072\tvalid_1's auc: 0.764452\tvalid_1's binary_logloss: 0.243961\n",
      "[1233]\ttraining's auc: 0.891872\ttraining's binary_logloss: 0.200038\tvalid_1's auc: 0.76446\tvalid_1's binary_logloss: 0.24396\n",
      "[1234]\ttraining's auc: 0.891927\ttraining's binary_logloss: 0.200015\tvalid_1's auc: 0.76445\tvalid_1's binary_logloss: 0.243962\n",
      "[1235]\ttraining's auc: 0.892002\ttraining's binary_logloss: 0.19998\tvalid_1's auc: 0.764456\tvalid_1's binary_logloss: 0.243963\n",
      "[1236]\ttraining's auc: 0.892031\ttraining's binary_logloss: 0.199965\tvalid_1's auc: 0.764459\tvalid_1's binary_logloss: 0.243963\n",
      "[1237]\ttraining's auc: 0.892059\ttraining's binary_logloss: 0.19995\tvalid_1's auc: 0.764463\tvalid_1's binary_logloss: 0.243962\n",
      "[1238]\ttraining's auc: 0.892117\ttraining's binary_logloss: 0.199924\tvalid_1's auc: 0.764465\tvalid_1's binary_logloss: 0.243961\n",
      "[1239]\ttraining's auc: 0.892189\ttraining's binary_logloss: 0.199892\tvalid_1's auc: 0.764459\tvalid_1's binary_logloss: 0.243964\n",
      "[1240]\ttraining's auc: 0.892258\ttraining's binary_logloss: 0.199864\tvalid_1's auc: 0.764457\tvalid_1's binary_logloss: 0.243965\n",
      "[1241]\ttraining's auc: 0.892327\ttraining's binary_logloss: 0.199834\tvalid_1's auc: 0.764479\tvalid_1's binary_logloss: 0.243955\n",
      "[1242]\ttraining's auc: 0.892391\ttraining's binary_logloss: 0.199802\tvalid_1's auc: 0.76446\tvalid_1's binary_logloss: 0.243959\n",
      "[1243]\ttraining's auc: 0.892455\ttraining's binary_logloss: 0.199778\tvalid_1's auc: 0.764481\tvalid_1's binary_logloss: 0.243953\n",
      "[1244]\ttraining's auc: 0.892532\ttraining's binary_logloss: 0.199746\tvalid_1's auc: 0.764492\tvalid_1's binary_logloss: 0.243949\n",
      "[1245]\ttraining's auc: 0.892581\ttraining's binary_logloss: 0.199726\tvalid_1's auc: 0.764484\tvalid_1's binary_logloss: 0.243952\n",
      "[1246]\ttraining's auc: 0.89261\ttraining's binary_logloss: 0.199713\tvalid_1's auc: 0.764492\tvalid_1's binary_logloss: 0.243951\n",
      "[1247]\ttraining's auc: 0.892646\ttraining's binary_logloss: 0.199695\tvalid_1's auc: 0.764488\tvalid_1's binary_logloss: 0.243952\n",
      "[1248]\ttraining's auc: 0.892723\ttraining's binary_logloss: 0.19966\tvalid_1's auc: 0.764484\tvalid_1's binary_logloss: 0.243952\n",
      "[1249]\ttraining's auc: 0.892796\ttraining's binary_logloss: 0.19963\tvalid_1's auc: 0.764481\tvalid_1's binary_logloss: 0.243953\n",
      "[1250]\ttraining's auc: 0.892858\ttraining's binary_logloss: 0.199598\tvalid_1's auc: 0.764493\tvalid_1's binary_logloss: 0.243949\n",
      "[1251]\ttraining's auc: 0.892933\ttraining's binary_logloss: 0.199566\tvalid_1's auc: 0.764502\tvalid_1's binary_logloss: 0.243948\n",
      "[1252]\ttraining's auc: 0.89301\ttraining's binary_logloss: 0.199537\tvalid_1's auc: 0.764513\tvalid_1's binary_logloss: 0.243945\n",
      "[1253]\ttraining's auc: 0.893037\ttraining's binary_logloss: 0.199523\tvalid_1's auc: 0.764517\tvalid_1's binary_logloss: 0.243945\n",
      "[1254]\ttraining's auc: 0.893113\ttraining's binary_logloss: 0.199492\tvalid_1's auc: 0.764529\tvalid_1's binary_logloss: 0.24394\n",
      "[1255]\ttraining's auc: 0.893131\ttraining's binary_logloss: 0.199481\tvalid_1's auc: 0.764524\tvalid_1's binary_logloss: 0.243943\n",
      "[1256]\ttraining's auc: 0.893184\ttraining's binary_logloss: 0.199453\tvalid_1's auc: 0.764518\tvalid_1's binary_logloss: 0.243949\n",
      "[1257]\ttraining's auc: 0.893213\ttraining's binary_logloss: 0.199439\tvalid_1's auc: 0.764495\tvalid_1's binary_logloss: 0.243954\n",
      "[1258]\ttraining's auc: 0.893235\ttraining's binary_logloss: 0.199427\tvalid_1's auc: 0.764497\tvalid_1's binary_logloss: 0.243953\n",
      "[1259]\ttraining's auc: 0.893286\ttraining's binary_logloss: 0.199402\tvalid_1's auc: 0.764499\tvalid_1's binary_logloss: 0.243952\n",
      "[1260]\ttraining's auc: 0.893364\ttraining's binary_logloss: 0.199363\tvalid_1's auc: 0.764489\tvalid_1's binary_logloss: 0.243955\n",
      "[1261]\ttraining's auc: 0.893399\ttraining's binary_logloss: 0.199348\tvalid_1's auc: 0.764494\tvalid_1's binary_logloss: 0.243952\n",
      "[1262]\ttraining's auc: 0.893468\ttraining's binary_logloss: 0.199312\tvalid_1's auc: 0.764485\tvalid_1's binary_logloss: 0.24395\n",
      "[1263]\ttraining's auc: 0.893502\ttraining's binary_logloss: 0.199296\tvalid_1's auc: 0.764498\tvalid_1's binary_logloss: 0.243947\n",
      "[1264]\ttraining's auc: 0.893571\ttraining's binary_logloss: 0.199263\tvalid_1's auc: 0.764489\tvalid_1's binary_logloss: 0.24395\n",
      "[1265]\ttraining's auc: 0.893644\ttraining's binary_logloss: 0.199228\tvalid_1's auc: 0.764488\tvalid_1's binary_logloss: 0.243948\n",
      "[1266]\ttraining's auc: 0.893715\ttraining's binary_logloss: 0.199199\tvalid_1's auc: 0.764488\tvalid_1's binary_logloss: 0.243951\n",
      "[1267]\ttraining's auc: 0.893781\ttraining's binary_logloss: 0.199169\tvalid_1's auc: 0.764497\tvalid_1's binary_logloss: 0.243948\n",
      "[1268]\ttraining's auc: 0.893843\ttraining's binary_logloss: 0.199138\tvalid_1's auc: 0.764496\tvalid_1's binary_logloss: 0.243945\n",
      "[1269]\ttraining's auc: 0.893859\ttraining's binary_logloss: 0.19913\tvalid_1's auc: 0.764489\tvalid_1's binary_logloss: 0.243948\n",
      "[1270]\ttraining's auc: 0.893944\ttraining's binary_logloss: 0.199093\tvalid_1's auc: 0.764481\tvalid_1's binary_logloss: 0.243954\n",
      "[1271]\ttraining's auc: 0.894013\ttraining's binary_logloss: 0.199063\tvalid_1's auc: 0.764493\tvalid_1's binary_logloss: 0.243953\n",
      "[1272]\ttraining's auc: 0.894042\ttraining's binary_logloss: 0.199046\tvalid_1's auc: 0.764513\tvalid_1's binary_logloss: 0.243951\n",
      "[1273]\ttraining's auc: 0.894078\ttraining's binary_logloss: 0.199029\tvalid_1's auc: 0.764524\tvalid_1's binary_logloss: 0.243946\n",
      "[1274]\ttraining's auc: 0.89416\ttraining's binary_logloss: 0.198995\tvalid_1's auc: 0.764531\tvalid_1's binary_logloss: 0.243945\n",
      "[1275]\ttraining's auc: 0.894204\ttraining's binary_logloss: 0.198976\tvalid_1's auc: 0.764518\tvalid_1's binary_logloss: 0.243948\n",
      "[1276]\ttraining's auc: 0.894267\ttraining's binary_logloss: 0.198949\tvalid_1's auc: 0.764514\tvalid_1's binary_logloss: 0.243946\n",
      "[1277]\ttraining's auc: 0.894302\ttraining's binary_logloss: 0.198931\tvalid_1's auc: 0.764505\tvalid_1's binary_logloss: 0.243949\n",
      "[1278]\ttraining's auc: 0.894363\ttraining's binary_logloss: 0.198903\tvalid_1's auc: 0.764526\tvalid_1's binary_logloss: 0.243947\n",
      "[1279]\ttraining's auc: 0.894397\ttraining's binary_logloss: 0.198887\tvalid_1's auc: 0.764519\tvalid_1's binary_logloss: 0.24395\n",
      "[1280]\ttraining's auc: 0.894465\ttraining's binary_logloss: 0.19885\tvalid_1's auc: 0.764535\tvalid_1's binary_logloss: 0.243946\n",
      "[1281]\ttraining's auc: 0.894512\ttraining's binary_logloss: 0.198824\tvalid_1's auc: 0.764535\tvalid_1's binary_logloss: 0.243948\n",
      "[1282]\ttraining's auc: 0.894576\ttraining's binary_logloss: 0.198795\tvalid_1's auc: 0.76454\tvalid_1's binary_logloss: 0.243947\n",
      "[1283]\ttraining's auc: 0.89463\ttraining's binary_logloss: 0.198764\tvalid_1's auc: 0.764542\tvalid_1's binary_logloss: 0.243947\n",
      "[1284]\ttraining's auc: 0.894683\ttraining's binary_logloss: 0.198744\tvalid_1's auc: 0.764548\tvalid_1's binary_logloss: 0.243948\n",
      "[1285]\ttraining's auc: 0.894712\ttraining's binary_logloss: 0.198727\tvalid_1's auc: 0.764545\tvalid_1's binary_logloss: 0.243947\n",
      "[1286]\ttraining's auc: 0.894789\ttraining's binary_logloss: 0.198697\tvalid_1's auc: 0.764533\tvalid_1's binary_logloss: 0.243954\n",
      "[1287]\ttraining's auc: 0.894813\ttraining's binary_logloss: 0.198683\tvalid_1's auc: 0.764523\tvalid_1's binary_logloss: 0.243957\n",
      "[1288]\ttraining's auc: 0.894881\ttraining's binary_logloss: 0.198652\tvalid_1's auc: 0.764507\tvalid_1's binary_logloss: 0.24396\n",
      "[1289]\ttraining's auc: 0.894898\ttraining's binary_logloss: 0.198642\tvalid_1's auc: 0.764489\tvalid_1's binary_logloss: 0.243963\n",
      "[1290]\ttraining's auc: 0.894931\ttraining's binary_logloss: 0.198625\tvalid_1's auc: 0.764485\tvalid_1's binary_logloss: 0.243963\n",
      "[1291]\ttraining's auc: 0.894984\ttraining's binary_logloss: 0.198598\tvalid_1's auc: 0.764495\tvalid_1's binary_logloss: 0.243961\n",
      "[1292]\ttraining's auc: 0.895032\ttraining's binary_logloss: 0.198576\tvalid_1's auc: 0.764502\tvalid_1's binary_logloss: 0.24396\n",
      "[1293]\ttraining's auc: 0.895072\ttraining's binary_logloss: 0.198556\tvalid_1's auc: 0.76451\tvalid_1's binary_logloss: 0.243957\n",
      "[1294]\ttraining's auc: 0.895104\ttraining's binary_logloss: 0.198541\tvalid_1's auc: 0.764506\tvalid_1's binary_logloss: 0.243958\n",
      "[1295]\ttraining's auc: 0.89513\ttraining's binary_logloss: 0.198526\tvalid_1's auc: 0.764493\tvalid_1's binary_logloss: 0.243962\n",
      "[1296]\ttraining's auc: 0.895194\ttraining's binary_logloss: 0.198494\tvalid_1's auc: 0.764482\tvalid_1's binary_logloss: 0.243963\n",
      "[1297]\ttraining's auc: 0.895239\ttraining's binary_logloss: 0.198471\tvalid_1's auc: 0.764471\tvalid_1's binary_logloss: 0.243963\n",
      "[1298]\ttraining's auc: 0.895297\ttraining's binary_logloss: 0.19844\tvalid_1's auc: 0.76448\tvalid_1's binary_logloss: 0.243961\n",
      "[1299]\ttraining's auc: 0.895371\ttraining's binary_logloss: 0.198407\tvalid_1's auc: 0.764485\tvalid_1's binary_logloss: 0.243961\n",
      "[1300]\ttraining's auc: 0.895419\ttraining's binary_logloss: 0.198385\tvalid_1's auc: 0.764479\tvalid_1's binary_logloss: 0.243961\n",
      "[1301]\ttraining's auc: 0.895479\ttraining's binary_logloss: 0.198355\tvalid_1's auc: 0.764476\tvalid_1's binary_logloss: 0.243959\n",
      "[1302]\ttraining's auc: 0.895537\ttraining's binary_logloss: 0.19832\tvalid_1's auc: 0.764469\tvalid_1's binary_logloss: 0.243961\n",
      "[1303]\ttraining's auc: 0.895566\ttraining's binary_logloss: 0.198305\tvalid_1's auc: 0.764466\tvalid_1's binary_logloss: 0.243962\n",
      "[1304]\ttraining's auc: 0.895618\ttraining's binary_logloss: 0.198279\tvalid_1's auc: 0.764479\tvalid_1's binary_logloss: 0.243959\n",
      "[1305]\ttraining's auc: 0.89569\ttraining's binary_logloss: 0.19825\tvalid_1's auc: 0.764459\tvalid_1's binary_logloss: 0.243964\n",
      "[1306]\ttraining's auc: 0.895752\ttraining's binary_logloss: 0.198223\tvalid_1's auc: 0.764468\tvalid_1's binary_logloss: 0.24396\n",
      "[1307]\ttraining's auc: 0.895821\ttraining's binary_logloss: 0.198192\tvalid_1's auc: 0.764479\tvalid_1's binary_logloss: 0.243959\n",
      "[1308]\ttraining's auc: 0.895866\ttraining's binary_logloss: 0.19817\tvalid_1's auc: 0.764471\tvalid_1's binary_logloss: 0.243963\n",
      "[1309]\ttraining's auc: 0.895939\ttraining's binary_logloss: 0.198135\tvalid_1's auc: 0.764462\tvalid_1's binary_logloss: 0.243963\n",
      "[1310]\ttraining's auc: 0.895978\ttraining's binary_logloss: 0.198106\tvalid_1's auc: 0.764445\tvalid_1's binary_logloss: 0.243968\n",
      "[1311]\ttraining's auc: 0.895995\ttraining's binary_logloss: 0.198097\tvalid_1's auc: 0.764445\tvalid_1's binary_logloss: 0.243968\n",
      "[1312]\ttraining's auc: 0.89606\ttraining's binary_logloss: 0.198063\tvalid_1's auc: 0.764434\tvalid_1's binary_logloss: 0.243973\n",
      "[1313]\ttraining's auc: 0.89609\ttraining's binary_logloss: 0.198048\tvalid_1's auc: 0.764434\tvalid_1's binary_logloss: 0.243975\n",
      "[1314]\ttraining's auc: 0.896154\ttraining's binary_logloss: 0.198021\tvalid_1's auc: 0.764441\tvalid_1's binary_logloss: 0.243976\n",
      "[1315]\ttraining's auc: 0.896177\ttraining's binary_logloss: 0.198009\tvalid_1's auc: 0.764441\tvalid_1's binary_logloss: 0.243977\n",
      "[1316]\ttraining's auc: 0.896247\ttraining's binary_logloss: 0.197975\tvalid_1's auc: 0.764452\tvalid_1's binary_logloss: 0.24397\n",
      "[1317]\ttraining's auc: 0.896312\ttraining's binary_logloss: 0.197944\tvalid_1's auc: 0.764425\tvalid_1's binary_logloss: 0.243976\n",
      "[1318]\ttraining's auc: 0.896334\ttraining's binary_logloss: 0.197932\tvalid_1's auc: 0.764424\tvalid_1's binary_logloss: 0.243977\n",
      "[1319]\ttraining's auc: 0.896388\ttraining's binary_logloss: 0.197903\tvalid_1's auc: 0.764436\tvalid_1's binary_logloss: 0.243972\n",
      "[1320]\ttraining's auc: 0.896422\ttraining's binary_logloss: 0.197885\tvalid_1's auc: 0.764443\tvalid_1's binary_logloss: 0.243972\n",
      "[1321]\ttraining's auc: 0.896455\ttraining's binary_logloss: 0.19787\tvalid_1's auc: 0.764448\tvalid_1's binary_logloss: 0.24397\n",
      "[1322]\ttraining's auc: 0.896469\ttraining's binary_logloss: 0.197861\tvalid_1's auc: 0.764443\tvalid_1's binary_logloss: 0.243971\n",
      "[1323]\ttraining's auc: 0.89653\ttraining's binary_logloss: 0.197828\tvalid_1's auc: 0.76445\tvalid_1's binary_logloss: 0.243969\n",
      "[1324]\ttraining's auc: 0.896586\ttraining's binary_logloss: 0.197802\tvalid_1's auc: 0.764437\tvalid_1's binary_logloss: 0.243969\n",
      "[1325]\ttraining's auc: 0.896629\ttraining's binary_logloss: 0.197781\tvalid_1's auc: 0.764431\tvalid_1's binary_logloss: 0.24397\n",
      "[1326]\ttraining's auc: 0.896681\ttraining's binary_logloss: 0.197757\tvalid_1's auc: 0.764412\tvalid_1's binary_logloss: 0.243974\n",
      "[1327]\ttraining's auc: 0.896737\ttraining's binary_logloss: 0.197728\tvalid_1's auc: 0.764427\tvalid_1's binary_logloss: 0.24397\n",
      "[1328]\ttraining's auc: 0.896789\ttraining's binary_logloss: 0.197702\tvalid_1's auc: 0.764428\tvalid_1's binary_logloss: 0.243973\n",
      "[1329]\ttraining's auc: 0.896857\ttraining's binary_logloss: 0.197671\tvalid_1's auc: 0.764422\tvalid_1's binary_logloss: 0.243976\n",
      "[1330]\ttraining's auc: 0.89693\ttraining's binary_logloss: 0.197635\tvalid_1's auc: 0.764406\tvalid_1's binary_logloss: 0.243976\n",
      "[1331]\ttraining's auc: 0.896983\ttraining's binary_logloss: 0.197607\tvalid_1's auc: 0.764406\tvalid_1's binary_logloss: 0.243976\n",
      "[1332]\ttraining's auc: 0.897002\ttraining's binary_logloss: 0.197598\tvalid_1's auc: 0.764397\tvalid_1's binary_logloss: 0.243978\n",
      "[1333]\ttraining's auc: 0.897015\ttraining's binary_logloss: 0.19759\tvalid_1's auc: 0.764403\tvalid_1's binary_logloss: 0.243977\n",
      "[1334]\ttraining's auc: 0.897067\ttraining's binary_logloss: 0.197565\tvalid_1's auc: 0.7644\tvalid_1's binary_logloss: 0.243979\n",
      "[1335]\ttraining's auc: 0.897122\ttraining's binary_logloss: 0.197537\tvalid_1's auc: 0.764404\tvalid_1's binary_logloss: 0.243978\n",
      "[1336]\ttraining's auc: 0.897188\ttraining's binary_logloss: 0.197507\tvalid_1's auc: 0.764405\tvalid_1's binary_logloss: 0.24398\n",
      "[1337]\ttraining's auc: 0.897261\ttraining's binary_logloss: 0.197473\tvalid_1's auc: 0.7644\tvalid_1's binary_logloss: 0.243982\n",
      "[1338]\ttraining's auc: 0.897309\ttraining's binary_logloss: 0.197444\tvalid_1's auc: 0.76441\tvalid_1's binary_logloss: 0.24398\n",
      "[1339]\ttraining's auc: 0.897384\ttraining's binary_logloss: 0.197411\tvalid_1's auc: 0.764412\tvalid_1's binary_logloss: 0.243979\n",
      "[1340]\ttraining's auc: 0.897471\ttraining's binary_logloss: 0.197376\tvalid_1's auc: 0.764454\tvalid_1's binary_logloss: 0.243969\n",
      "[1341]\ttraining's auc: 0.897528\ttraining's binary_logloss: 0.19734\tvalid_1's auc: 0.764445\tvalid_1's binary_logloss: 0.243972\n",
      "[1342]\ttraining's auc: 0.897587\ttraining's binary_logloss: 0.197309\tvalid_1's auc: 0.764455\tvalid_1's binary_logloss: 0.243968\n",
      "[1343]\ttraining's auc: 0.897611\ttraining's binary_logloss: 0.197295\tvalid_1's auc: 0.76445\tvalid_1's binary_logloss: 0.243969\n",
      "[1344]\ttraining's auc: 0.897675\ttraining's binary_logloss: 0.197263\tvalid_1's auc: 0.764451\tvalid_1's binary_logloss: 0.243966\n",
      "[1345]\ttraining's auc: 0.897744\ttraining's binary_logloss: 0.197233\tvalid_1's auc: 0.764436\tvalid_1's binary_logloss: 0.243971\n",
      "[1346]\ttraining's auc: 0.897776\ttraining's binary_logloss: 0.197218\tvalid_1's auc: 0.764421\tvalid_1's binary_logloss: 0.243975\n",
      "[1347]\ttraining's auc: 0.897838\ttraining's binary_logloss: 0.197189\tvalid_1's auc: 0.764432\tvalid_1's binary_logloss: 0.243971\n",
      "[1348]\ttraining's auc: 0.897907\ttraining's binary_logloss: 0.197156\tvalid_1's auc: 0.764403\tvalid_1's binary_logloss: 0.243977\n",
      "[1349]\ttraining's auc: 0.897931\ttraining's binary_logloss: 0.197143\tvalid_1's auc: 0.764402\tvalid_1's binary_logloss: 0.243978\n",
      "[1350]\ttraining's auc: 0.897988\ttraining's binary_logloss: 0.197117\tvalid_1's auc: 0.764402\tvalid_1's binary_logloss: 0.243979\n",
      "[1351]\ttraining's auc: 0.898036\ttraining's binary_logloss: 0.197094\tvalid_1's auc: 0.764406\tvalid_1's binary_logloss: 0.243979\n",
      "[1352]\ttraining's auc: 0.898084\ttraining's binary_logloss: 0.197062\tvalid_1's auc: 0.764414\tvalid_1's binary_logloss: 0.243976\n",
      "[1353]\ttraining's auc: 0.898116\ttraining's binary_logloss: 0.197049\tvalid_1's auc: 0.764413\tvalid_1's binary_logloss: 0.243975\n",
      "[1354]\ttraining's auc: 0.898146\ttraining's binary_logloss: 0.197034\tvalid_1's auc: 0.764395\tvalid_1's binary_logloss: 0.24398\n",
      "[1355]\ttraining's auc: 0.898204\ttraining's binary_logloss: 0.197009\tvalid_1's auc: 0.764396\tvalid_1's binary_logloss: 0.243978\n",
      "[1356]\ttraining's auc: 0.898234\ttraining's binary_logloss: 0.196992\tvalid_1's auc: 0.76439\tvalid_1's binary_logloss: 0.243981\n",
      "[1357]\ttraining's auc: 0.898276\ttraining's binary_logloss: 0.196978\tvalid_1's auc: 0.764406\tvalid_1's binary_logloss: 0.243978\n",
      "[1358]\ttraining's auc: 0.898341\ttraining's binary_logloss: 0.196951\tvalid_1's auc: 0.764409\tvalid_1's binary_logloss: 0.243979\n",
      "[1359]\ttraining's auc: 0.898355\ttraining's binary_logloss: 0.196944\tvalid_1's auc: 0.764415\tvalid_1's binary_logloss: 0.243976\n",
      "[1360]\ttraining's auc: 0.898433\ttraining's binary_logloss: 0.196913\tvalid_1's auc: 0.764419\tvalid_1's binary_logloss: 0.243975\n",
      "[1361]\ttraining's auc: 0.898496\ttraining's binary_logloss: 0.196889\tvalid_1's auc: 0.76441\tvalid_1's binary_logloss: 0.243976\n",
      "[1362]\ttraining's auc: 0.898568\ttraining's binary_logloss: 0.196855\tvalid_1's auc: 0.764408\tvalid_1's binary_logloss: 0.243976\n",
      "[1363]\ttraining's auc: 0.898644\ttraining's binary_logloss: 0.196822\tvalid_1's auc: 0.764415\tvalid_1's binary_logloss: 0.243973\n",
      "[1364]\ttraining's auc: 0.898721\ttraining's binary_logloss: 0.19679\tvalid_1's auc: 0.764425\tvalid_1's binary_logloss: 0.243971\n",
      "[1365]\ttraining's auc: 0.898785\ttraining's binary_logloss: 0.196758\tvalid_1's auc: 0.764451\tvalid_1's binary_logloss: 0.243965\n",
      "[1366]\ttraining's auc: 0.898871\ttraining's binary_logloss: 0.196725\tvalid_1's auc: 0.76444\tvalid_1's binary_logloss: 0.243967\n",
      "[1367]\ttraining's auc: 0.898892\ttraining's binary_logloss: 0.196715\tvalid_1's auc: 0.764433\tvalid_1's binary_logloss: 0.243968\n",
      "[1368]\ttraining's auc: 0.898969\ttraining's binary_logloss: 0.196679\tvalid_1's auc: 0.764452\tvalid_1's binary_logloss: 0.243964\n",
      "[1369]\ttraining's auc: 0.899045\ttraining's binary_logloss: 0.196647\tvalid_1's auc: 0.764458\tvalid_1's binary_logloss: 0.243961\n",
      "[1370]\ttraining's auc: 0.899118\ttraining's binary_logloss: 0.196617\tvalid_1's auc: 0.76446\tvalid_1's binary_logloss: 0.243963\n",
      "[1371]\ttraining's auc: 0.899172\ttraining's binary_logloss: 0.196594\tvalid_1's auc: 0.764445\tvalid_1's binary_logloss: 0.243965\n",
      "[1372]\ttraining's auc: 0.899204\ttraining's binary_logloss: 0.196578\tvalid_1's auc: 0.764443\tvalid_1's binary_logloss: 0.243965\n",
      "[1373]\ttraining's auc: 0.899223\ttraining's binary_logloss: 0.196566\tvalid_1's auc: 0.76444\tvalid_1's binary_logloss: 0.243965\n",
      "[1374]\ttraining's auc: 0.899285\ttraining's binary_logloss: 0.196537\tvalid_1's auc: 0.764453\tvalid_1's binary_logloss: 0.243962\n",
      "[1375]\ttraining's auc: 0.89931\ttraining's binary_logloss: 0.196527\tvalid_1's auc: 0.764464\tvalid_1's binary_logloss: 0.243962\n",
      "[1376]\ttraining's auc: 0.899342\ttraining's binary_logloss: 0.196516\tvalid_1's auc: 0.764449\tvalid_1's binary_logloss: 0.243966\n",
      "[1377]\ttraining's auc: 0.89942\ttraining's binary_logloss: 0.196482\tvalid_1's auc: 0.764451\tvalid_1's binary_logloss: 0.243967\n",
      "[1378]\ttraining's auc: 0.899502\ttraining's binary_logloss: 0.196446\tvalid_1's auc: 0.76445\tvalid_1's binary_logloss: 0.243969\n",
      "[1379]\ttraining's auc: 0.899574\ttraining's binary_logloss: 0.196413\tvalid_1's auc: 0.764456\tvalid_1's binary_logloss: 0.243967\n",
      "[1380]\ttraining's auc: 0.899633\ttraining's binary_logloss: 0.196387\tvalid_1's auc: 0.764441\tvalid_1's binary_logloss: 0.24397\n",
      "[1381]\ttraining's auc: 0.899698\ttraining's binary_logloss: 0.196357\tvalid_1's auc: 0.764444\tvalid_1's binary_logloss: 0.243968\n",
      "[1382]\ttraining's auc: 0.899763\ttraining's binary_logloss: 0.196326\tvalid_1's auc: 0.764464\tvalid_1's binary_logloss: 0.243967\n",
      "[1383]\ttraining's auc: 0.899786\ttraining's binary_logloss: 0.196315\tvalid_1's auc: 0.764457\tvalid_1's binary_logloss: 0.24397\n",
      "[1384]\ttraining's auc: 0.899867\ttraining's binary_logloss: 0.19628\tvalid_1's auc: 0.764465\tvalid_1's binary_logloss: 0.243966\n",
      "[1385]\ttraining's auc: 0.899892\ttraining's binary_logloss: 0.196267\tvalid_1's auc: 0.764455\tvalid_1's binary_logloss: 0.243968\n",
      "[1386]\ttraining's auc: 0.89995\ttraining's binary_logloss: 0.196239\tvalid_1's auc: 0.764454\tvalid_1's binary_logloss: 0.243966\n",
      "[1387]\ttraining's auc: 0.899973\ttraining's binary_logloss: 0.196226\tvalid_1's auc: 0.764469\tvalid_1's binary_logloss: 0.243961\n",
      "[1388]\ttraining's auc: 0.900025\ttraining's binary_logloss: 0.196202\tvalid_1's auc: 0.764457\tvalid_1's binary_logloss: 0.243964\n",
      "[1389]\ttraining's auc: 0.900043\ttraining's binary_logloss: 0.196192\tvalid_1's auc: 0.764452\tvalid_1's binary_logloss: 0.243965\n",
      "[1390]\ttraining's auc: 0.900076\ttraining's binary_logloss: 0.196176\tvalid_1's auc: 0.76446\tvalid_1's binary_logloss: 0.243964\n",
      "[1391]\ttraining's auc: 0.900138\ttraining's binary_logloss: 0.196145\tvalid_1's auc: 0.764476\tvalid_1's binary_logloss: 0.243961\n",
      "[1392]\ttraining's auc: 0.900202\ttraining's binary_logloss: 0.196116\tvalid_1's auc: 0.764475\tvalid_1's binary_logloss: 0.243962\n",
      "[1393]\ttraining's auc: 0.900238\ttraining's binary_logloss: 0.196103\tvalid_1's auc: 0.764475\tvalid_1's binary_logloss: 0.24396\n",
      "[1394]\ttraining's auc: 0.900263\ttraining's binary_logloss: 0.196083\tvalid_1's auc: 0.764479\tvalid_1's binary_logloss: 0.24396\n",
      "[1395]\ttraining's auc: 0.900314\ttraining's binary_logloss: 0.19606\tvalid_1's auc: 0.764477\tvalid_1's binary_logloss: 0.243962\n",
      "[1396]\ttraining's auc: 0.900362\ttraining's binary_logloss: 0.196038\tvalid_1's auc: 0.764486\tvalid_1's binary_logloss: 0.24396\n",
      "[1397]\ttraining's auc: 0.900379\ttraining's binary_logloss: 0.196026\tvalid_1's auc: 0.76448\tvalid_1's binary_logloss: 0.24396\n",
      "[1398]\ttraining's auc: 0.900413\ttraining's binary_logloss: 0.196011\tvalid_1's auc: 0.764464\tvalid_1's binary_logloss: 0.243965\n",
      "[1399]\ttraining's auc: 0.900472\ttraining's binary_logloss: 0.195982\tvalid_1's auc: 0.764462\tvalid_1's binary_logloss: 0.243964\n",
      "[1400]\ttraining's auc: 0.900539\ttraining's binary_logloss: 0.195951\tvalid_1's auc: 0.76445\tvalid_1's binary_logloss: 0.243964\n",
      "[1401]\ttraining's auc: 0.900578\ttraining's binary_logloss: 0.195926\tvalid_1's auc: 0.764459\tvalid_1's binary_logloss: 0.24396\n",
      "[1402]\ttraining's auc: 0.900655\ttraining's binary_logloss: 0.195892\tvalid_1's auc: 0.764475\tvalid_1's binary_logloss: 0.243958\n",
      "[1403]\ttraining's auc: 0.900689\ttraining's binary_logloss: 0.195873\tvalid_1's auc: 0.764465\tvalid_1's binary_logloss: 0.243962\n",
      "[1404]\ttraining's auc: 0.900751\ttraining's binary_logloss: 0.195845\tvalid_1's auc: 0.764468\tvalid_1's binary_logloss: 0.243964\n",
      "[1405]\ttraining's auc: 0.900777\ttraining's binary_logloss: 0.195832\tvalid_1's auc: 0.764474\tvalid_1's binary_logloss: 0.243964\n",
      "[1406]\ttraining's auc: 0.900854\ttraining's binary_logloss: 0.195795\tvalid_1's auc: 0.764468\tvalid_1's binary_logloss: 0.243969\n",
      "[1407]\ttraining's auc: 0.900935\ttraining's binary_logloss: 0.195762\tvalid_1's auc: 0.764456\tvalid_1's binary_logloss: 0.24397\n",
      "[1408]\ttraining's auc: 0.901009\ttraining's binary_logloss: 0.19573\tvalid_1's auc: 0.764443\tvalid_1's binary_logloss: 0.243977\n",
      "[1409]\ttraining's auc: 0.901074\ttraining's binary_logloss: 0.1957\tvalid_1's auc: 0.764444\tvalid_1's binary_logloss: 0.243977\n",
      "[1410]\ttraining's auc: 0.901106\ttraining's binary_logloss: 0.195681\tvalid_1's auc: 0.764414\tvalid_1's binary_logloss: 0.243984\n",
      "[1411]\ttraining's auc: 0.901168\ttraining's binary_logloss: 0.195646\tvalid_1's auc: 0.764408\tvalid_1's binary_logloss: 0.243987\n",
      "[1412]\ttraining's auc: 0.901222\ttraining's binary_logloss: 0.195612\tvalid_1's auc: 0.764409\tvalid_1's binary_logloss: 0.243987\n",
      "[1413]\ttraining's auc: 0.901283\ttraining's binary_logloss: 0.195584\tvalid_1's auc: 0.764396\tvalid_1's binary_logloss: 0.243994\n",
      "[1414]\ttraining's auc: 0.901359\ttraining's binary_logloss: 0.19555\tvalid_1's auc: 0.764389\tvalid_1's binary_logloss: 0.243999\n",
      "[1415]\ttraining's auc: 0.901375\ttraining's binary_logloss: 0.195539\tvalid_1's auc: 0.764375\tvalid_1's binary_logloss: 0.244003\n",
      "[1416]\ttraining's auc: 0.90142\ttraining's binary_logloss: 0.195521\tvalid_1's auc: 0.764375\tvalid_1's binary_logloss: 0.244003\n",
      "[1417]\ttraining's auc: 0.901449\ttraining's binary_logloss: 0.195503\tvalid_1's auc: 0.764372\tvalid_1's binary_logloss: 0.244006\n",
      "[1418]\ttraining's auc: 0.901514\ttraining's binary_logloss: 0.195469\tvalid_1's auc: 0.764376\tvalid_1's binary_logloss: 0.244003\n",
      "[1419]\ttraining's auc: 0.901578\ttraining's binary_logloss: 0.195437\tvalid_1's auc: 0.764375\tvalid_1's binary_logloss: 0.244004\n",
      "[1420]\ttraining's auc: 0.901638\ttraining's binary_logloss: 0.195406\tvalid_1's auc: 0.764347\tvalid_1's binary_logloss: 0.244015\n",
      "[1421]\ttraining's auc: 0.90168\ttraining's binary_logloss: 0.19538\tvalid_1's auc: 0.764344\tvalid_1's binary_logloss: 0.244015\n",
      "[1422]\ttraining's auc: 0.901709\ttraining's binary_logloss: 0.195366\tvalid_1's auc: 0.76433\tvalid_1's binary_logloss: 0.244019\n",
      "[1423]\ttraining's auc: 0.901769\ttraining's binary_logloss: 0.195337\tvalid_1's auc: 0.764319\tvalid_1's binary_logloss: 0.244023\n",
      "[1424]\ttraining's auc: 0.901821\ttraining's binary_logloss: 0.19531\tvalid_1's auc: 0.764307\tvalid_1's binary_logloss: 0.244026\n",
      "[1425]\ttraining's auc: 0.90189\ttraining's binary_logloss: 0.195276\tvalid_1's auc: 0.764323\tvalid_1's binary_logloss: 0.244024\n",
      "[1426]\ttraining's auc: 0.901949\ttraining's binary_logloss: 0.19525\tvalid_1's auc: 0.764321\tvalid_1's binary_logloss: 0.244023\n",
      "[1427]\ttraining's auc: 0.902011\ttraining's binary_logloss: 0.195225\tvalid_1's auc: 0.764336\tvalid_1's binary_logloss: 0.244016\n",
      "[1428]\ttraining's auc: 0.902045\ttraining's binary_logloss: 0.195208\tvalid_1's auc: 0.764331\tvalid_1's binary_logloss: 0.244018\n",
      "[1429]\ttraining's auc: 0.9021\ttraining's binary_logloss: 0.195184\tvalid_1's auc: 0.764305\tvalid_1's binary_logloss: 0.244025\n",
      "[1430]\ttraining's auc: 0.902173\ttraining's binary_logloss: 0.19515\tvalid_1's auc: 0.764317\tvalid_1's binary_logloss: 0.244022\n",
      "[1431]\ttraining's auc: 0.902232\ttraining's binary_logloss: 0.195121\tvalid_1's auc: 0.764314\tvalid_1's binary_logloss: 0.244023\n",
      "[1432]\ttraining's auc: 0.902297\ttraining's binary_logloss: 0.195088\tvalid_1's auc: 0.764303\tvalid_1's binary_logloss: 0.244025\n",
      "[1433]\ttraining's auc: 0.902362\ttraining's binary_logloss: 0.195055\tvalid_1's auc: 0.764298\tvalid_1's binary_logloss: 0.244028\n",
      "[1434]\ttraining's auc: 0.902425\ttraining's binary_logloss: 0.195023\tvalid_1's auc: 0.764271\tvalid_1's binary_logloss: 0.244037\n",
      "[1435]\ttraining's auc: 0.902446\ttraining's binary_logloss: 0.195011\tvalid_1's auc: 0.764277\tvalid_1's binary_logloss: 0.244035\n",
      "[1436]\ttraining's auc: 0.902482\ttraining's binary_logloss: 0.194997\tvalid_1's auc: 0.764264\tvalid_1's binary_logloss: 0.244037\n",
      "[1437]\ttraining's auc: 0.902505\ttraining's binary_logloss: 0.194985\tvalid_1's auc: 0.764272\tvalid_1's binary_logloss: 0.244034\n",
      "[1438]\ttraining's auc: 0.902528\ttraining's binary_logloss: 0.194974\tvalid_1's auc: 0.764266\tvalid_1's binary_logloss: 0.244035\n",
      "[1439]\ttraining's auc: 0.902587\ttraining's binary_logloss: 0.194943\tvalid_1's auc: 0.764273\tvalid_1's binary_logloss: 0.244034\n",
      "[1440]\ttraining's auc: 0.902606\ttraining's binary_logloss: 0.194934\tvalid_1's auc: 0.764274\tvalid_1's binary_logloss: 0.244034\n",
      "[1441]\ttraining's auc: 0.902658\ttraining's binary_logloss: 0.194903\tvalid_1's auc: 0.764277\tvalid_1's binary_logloss: 0.244034\n",
      "[1442]\ttraining's auc: 0.902711\ttraining's binary_logloss: 0.194874\tvalid_1's auc: 0.764294\tvalid_1's binary_logloss: 0.24403\n",
      "[1443]\ttraining's auc: 0.902769\ttraining's binary_logloss: 0.194843\tvalid_1's auc: 0.764274\tvalid_1's binary_logloss: 0.244036\n",
      "[1444]\ttraining's auc: 0.90279\ttraining's binary_logloss: 0.194832\tvalid_1's auc: 0.764276\tvalid_1's binary_logloss: 0.244036\n",
      "[1445]\ttraining's auc: 0.902857\ttraining's binary_logloss: 0.1948\tvalid_1's auc: 0.764266\tvalid_1's binary_logloss: 0.24404\n",
      "[1446]\ttraining's auc: 0.902928\ttraining's binary_logloss: 0.194766\tvalid_1's auc: 0.764255\tvalid_1's binary_logloss: 0.244042\n",
      "[1447]\ttraining's auc: 0.902975\ttraining's binary_logloss: 0.19474\tvalid_1's auc: 0.764264\tvalid_1's binary_logloss: 0.24404\n",
      "[1448]\ttraining's auc: 0.903054\ttraining's binary_logloss: 0.194706\tvalid_1's auc: 0.764276\tvalid_1's binary_logloss: 0.244038\n",
      "[1449]\ttraining's auc: 0.903118\ttraining's binary_logloss: 0.194678\tvalid_1's auc: 0.76427\tvalid_1's binary_logloss: 0.244041\n",
      "[1450]\ttraining's auc: 0.90316\ttraining's binary_logloss: 0.194655\tvalid_1's auc: 0.764264\tvalid_1's binary_logloss: 0.244041\n",
      "[1451]\ttraining's auc: 0.903198\ttraining's binary_logloss: 0.194634\tvalid_1's auc: 0.764247\tvalid_1's binary_logloss: 0.244044\n",
      "[1452]\ttraining's auc: 0.903229\ttraining's binary_logloss: 0.194616\tvalid_1's auc: 0.764235\tvalid_1's binary_logloss: 0.244047\n",
      "[1453]\ttraining's auc: 0.903279\ttraining's binary_logloss: 0.194591\tvalid_1's auc: 0.764203\tvalid_1's binary_logloss: 0.244055\n",
      "[1454]\ttraining's auc: 0.903346\ttraining's binary_logloss: 0.19456\tvalid_1's auc: 0.764222\tvalid_1's binary_logloss: 0.24405\n",
      "[1455]\ttraining's auc: 0.903407\ttraining's binary_logloss: 0.194529\tvalid_1's auc: 0.764224\tvalid_1's binary_logloss: 0.244051\n",
      "[1456]\ttraining's auc: 0.903468\ttraining's binary_logloss: 0.194502\tvalid_1's auc: 0.764227\tvalid_1's binary_logloss: 0.244051\n",
      "[1457]\ttraining's auc: 0.903517\ttraining's binary_logloss: 0.194479\tvalid_1's auc: 0.764217\tvalid_1's binary_logloss: 0.244054\n",
      "[1458]\ttraining's auc: 0.903531\ttraining's binary_logloss: 0.19447\tvalid_1's auc: 0.764202\tvalid_1's binary_logloss: 0.244057\n",
      "[1459]\ttraining's auc: 0.90359\ttraining's binary_logloss: 0.19444\tvalid_1's auc: 0.764199\tvalid_1's binary_logloss: 0.244059\n",
      "[1460]\ttraining's auc: 0.903636\ttraining's binary_logloss: 0.194412\tvalid_1's auc: 0.764179\tvalid_1's binary_logloss: 0.244064\n",
      "[1461]\ttraining's auc: 0.903692\ttraining's binary_logloss: 0.194385\tvalid_1's auc: 0.764176\tvalid_1's binary_logloss: 0.244065\n",
      "[1462]\ttraining's auc: 0.903711\ttraining's binary_logloss: 0.194375\tvalid_1's auc: 0.764177\tvalid_1's binary_logloss: 0.244066\n",
      "[1463]\ttraining's auc: 0.903778\ttraining's binary_logloss: 0.194345\tvalid_1's auc: 0.764185\tvalid_1's binary_logloss: 0.244064\n",
      "[1464]\ttraining's auc: 0.903824\ttraining's binary_logloss: 0.19432\tvalid_1's auc: 0.764196\tvalid_1's binary_logloss: 0.244063\n",
      "[1465]\ttraining's auc: 0.90384\ttraining's binary_logloss: 0.194313\tvalid_1's auc: 0.764206\tvalid_1's binary_logloss: 0.244061\n",
      "[1466]\ttraining's auc: 0.9039\ttraining's binary_logloss: 0.194285\tvalid_1's auc: 0.764185\tvalid_1's binary_logloss: 0.244069\n",
      "[1467]\ttraining's auc: 0.903973\ttraining's binary_logloss: 0.194253\tvalid_1's auc: 0.76418\tvalid_1's binary_logloss: 0.244071\n",
      "[1468]\ttraining's auc: 0.904024\ttraining's binary_logloss: 0.194232\tvalid_1's auc: 0.764182\tvalid_1's binary_logloss: 0.244071\n",
      "[1469]\ttraining's auc: 0.904091\ttraining's binary_logloss: 0.194204\tvalid_1's auc: 0.764173\tvalid_1's binary_logloss: 0.244071\n",
      "[1470]\ttraining's auc: 0.904137\ttraining's binary_logloss: 0.19418\tvalid_1's auc: 0.764184\tvalid_1's binary_logloss: 0.24407\n",
      "[1471]\ttraining's auc: 0.90421\ttraining's binary_logloss: 0.194145\tvalid_1's auc: 0.764177\tvalid_1's binary_logloss: 0.244069\n",
      "[1472]\ttraining's auc: 0.904265\ttraining's binary_logloss: 0.194117\tvalid_1's auc: 0.764172\tvalid_1's binary_logloss: 0.244069\n",
      "[1473]\ttraining's auc: 0.904329\ttraining's binary_logloss: 0.194084\tvalid_1's auc: 0.764161\tvalid_1's binary_logloss: 0.244068\n",
      "[1474]\ttraining's auc: 0.904374\ttraining's binary_logloss: 0.194055\tvalid_1's auc: 0.76419\tvalid_1's binary_logloss: 0.244062\n",
      "[1475]\ttraining's auc: 0.904441\ttraining's binary_logloss: 0.194021\tvalid_1's auc: 0.764178\tvalid_1's binary_logloss: 0.244068\n",
      "[1476]\ttraining's auc: 0.904509\ttraining's binary_logloss: 0.193989\tvalid_1's auc: 0.764198\tvalid_1's binary_logloss: 0.244061\n",
      "[1477]\ttraining's auc: 0.904551\ttraining's binary_logloss: 0.193966\tvalid_1's auc: 0.764201\tvalid_1's binary_logloss: 0.244061\n",
      "[1478]\ttraining's auc: 0.904621\ttraining's binary_logloss: 0.193934\tvalid_1's auc: 0.764212\tvalid_1's binary_logloss: 0.24406\n",
      "[1479]\ttraining's auc: 0.90467\ttraining's binary_logloss: 0.19391\tvalid_1's auc: 0.764211\tvalid_1's binary_logloss: 0.24406\n",
      "[1480]\ttraining's auc: 0.904725\ttraining's binary_logloss: 0.193884\tvalid_1's auc: 0.76422\tvalid_1's binary_logloss: 0.244059\n",
      "[1481]\ttraining's auc: 0.904778\ttraining's binary_logloss: 0.193856\tvalid_1's auc: 0.764209\tvalid_1's binary_logloss: 0.244061\n",
      "[1482]\ttraining's auc: 0.904801\ttraining's binary_logloss: 0.193847\tvalid_1's auc: 0.764219\tvalid_1's binary_logloss: 0.244059\n",
      "[1483]\ttraining's auc: 0.904841\ttraining's binary_logloss: 0.193826\tvalid_1's auc: 0.764213\tvalid_1's binary_logloss: 0.24406\n",
      "[1484]\ttraining's auc: 0.904911\ttraining's binary_logloss: 0.193792\tvalid_1's auc: 0.764221\tvalid_1's binary_logloss: 0.244059\n",
      "[1485]\ttraining's auc: 0.904966\ttraining's binary_logloss: 0.193761\tvalid_1's auc: 0.764215\tvalid_1's binary_logloss: 0.24406\n",
      "[1486]\ttraining's auc: 0.905011\ttraining's binary_logloss: 0.193736\tvalid_1's auc: 0.764223\tvalid_1's binary_logloss: 0.244058\n",
      "[1487]\ttraining's auc: 0.905065\ttraining's binary_logloss: 0.193704\tvalid_1's auc: 0.7642\tvalid_1's binary_logloss: 0.244064\n",
      "[1488]\ttraining's auc: 0.905122\ttraining's binary_logloss: 0.193672\tvalid_1's auc: 0.764201\tvalid_1's binary_logloss: 0.244061\n",
      "[1489]\ttraining's auc: 0.905172\ttraining's binary_logloss: 0.193648\tvalid_1's auc: 0.764192\tvalid_1's binary_logloss: 0.244064\n",
      "[1490]\ttraining's auc: 0.90523\ttraining's binary_logloss: 0.193618\tvalid_1's auc: 0.764191\tvalid_1's binary_logloss: 0.244067\n",
      "[1491]\ttraining's auc: 0.90529\ttraining's binary_logloss: 0.193584\tvalid_1's auc: 0.764208\tvalid_1's binary_logloss: 0.244067\n",
      "[1492]\ttraining's auc: 0.90531\ttraining's binary_logloss: 0.193572\tvalid_1's auc: 0.764201\tvalid_1's binary_logloss: 0.244069\n",
      "[1493]\ttraining's auc: 0.905373\ttraining's binary_logloss: 0.193544\tvalid_1's auc: 0.764196\tvalid_1's binary_logloss: 0.24407\n",
      "[1494]\ttraining's auc: 0.90544\ttraining's binary_logloss: 0.193515\tvalid_1's auc: 0.764184\tvalid_1's binary_logloss: 0.244073\n",
      "[1495]\ttraining's auc: 0.905504\ttraining's binary_logloss: 0.193482\tvalid_1's auc: 0.76419\tvalid_1's binary_logloss: 0.244073\n",
      "[1496]\ttraining's auc: 0.905562\ttraining's binary_logloss: 0.193452\tvalid_1's auc: 0.764187\tvalid_1's binary_logloss: 0.244074\n",
      "[1497]\ttraining's auc: 0.905618\ttraining's binary_logloss: 0.193422\tvalid_1's auc: 0.76416\tvalid_1's binary_logloss: 0.24408\n",
      "[1498]\ttraining's auc: 0.905671\ttraining's binary_logloss: 0.193392\tvalid_1's auc: 0.764129\tvalid_1's binary_logloss: 0.244088\n",
      "[1499]\ttraining's auc: 0.905723\ttraining's binary_logloss: 0.193368\tvalid_1's auc: 0.764133\tvalid_1's binary_logloss: 0.244085\n",
      "[1500]\ttraining's auc: 0.905793\ttraining's binary_logloss: 0.193336\tvalid_1's auc: 0.764128\tvalid_1's binary_logloss: 0.244086\n",
      "[1501]\ttraining's auc: 0.905844\ttraining's binary_logloss: 0.193309\tvalid_1's auc: 0.764142\tvalid_1's binary_logloss: 0.244087\n",
      "[1502]\ttraining's auc: 0.905904\ttraining's binary_logloss: 0.193278\tvalid_1's auc: 0.764137\tvalid_1's binary_logloss: 0.24409\n",
      "[1503]\ttraining's auc: 0.905959\ttraining's binary_logloss: 0.193249\tvalid_1's auc: 0.764142\tvalid_1's binary_logloss: 0.244085\n",
      "[1504]\ttraining's auc: 0.906016\ttraining's binary_logloss: 0.193214\tvalid_1's auc: 0.764156\tvalid_1's binary_logloss: 0.244081\n",
      "[1505]\ttraining's auc: 0.906043\ttraining's binary_logloss: 0.1932\tvalid_1's auc: 0.764142\tvalid_1's binary_logloss: 0.244086\n",
      "[1506]\ttraining's auc: 0.906123\ttraining's binary_logloss: 0.193167\tvalid_1's auc: 0.764156\tvalid_1's binary_logloss: 0.244087\n",
      "[1507]\ttraining's auc: 0.906179\ttraining's binary_logloss: 0.193138\tvalid_1's auc: 0.76414\tvalid_1's binary_logloss: 0.244095\n",
      "[1508]\ttraining's auc: 0.906249\ttraining's binary_logloss: 0.193103\tvalid_1's auc: 0.764144\tvalid_1's binary_logloss: 0.244093\n",
      "[1509]\ttraining's auc: 0.90632\ttraining's binary_logloss: 0.193068\tvalid_1's auc: 0.764125\tvalid_1's binary_logloss: 0.244101\n",
      "[1510]\ttraining's auc: 0.906379\ttraining's binary_logloss: 0.193038\tvalid_1's auc: 0.764129\tvalid_1's binary_logloss: 0.2441\n",
      "[1511]\ttraining's auc: 0.906422\ttraining's binary_logloss: 0.193014\tvalid_1's auc: 0.764126\tvalid_1's binary_logloss: 0.244101\n",
      "[1512]\ttraining's auc: 0.906479\ttraining's binary_logloss: 0.192986\tvalid_1's auc: 0.764135\tvalid_1's binary_logloss: 0.244102\n",
      "[1513]\ttraining's auc: 0.906506\ttraining's binary_logloss: 0.19297\tvalid_1's auc: 0.764148\tvalid_1's binary_logloss: 0.244099\n",
      "[1514]\ttraining's auc: 0.906563\ttraining's binary_logloss: 0.192941\tvalid_1's auc: 0.764132\tvalid_1's binary_logloss: 0.244105\n",
      "[1515]\ttraining's auc: 0.906614\ttraining's binary_logloss: 0.192913\tvalid_1's auc: 0.76412\tvalid_1's binary_logloss: 0.244107\n",
      "[1516]\ttraining's auc: 0.906672\ttraining's binary_logloss: 0.19288\tvalid_1's auc: 0.764132\tvalid_1's binary_logloss: 0.244106\n",
      "[1517]\ttraining's auc: 0.906726\ttraining's binary_logloss: 0.192851\tvalid_1's auc: 0.764106\tvalid_1's binary_logloss: 0.244113\n",
      "[1518]\ttraining's auc: 0.906795\ttraining's binary_logloss: 0.192819\tvalid_1's auc: 0.764105\tvalid_1's binary_logloss: 0.244115\n",
      "[1519]\ttraining's auc: 0.906845\ttraining's binary_logloss: 0.192789\tvalid_1's auc: 0.764107\tvalid_1's binary_logloss: 0.244113\n",
      "[1520]\ttraining's auc: 0.90692\ttraining's binary_logloss: 0.192757\tvalid_1's auc: 0.764105\tvalid_1's binary_logloss: 0.244112\n",
      "[1521]\ttraining's auc: 0.906977\ttraining's binary_logloss: 0.192729\tvalid_1's auc: 0.7641\tvalid_1's binary_logloss: 0.244114\n",
      "[1522]\ttraining's auc: 0.907039\ttraining's binary_logloss: 0.192695\tvalid_1's auc: 0.764106\tvalid_1's binary_logloss: 0.244117\n",
      "[1523]\ttraining's auc: 0.907112\ttraining's binary_logloss: 0.192666\tvalid_1's auc: 0.764123\tvalid_1's binary_logloss: 0.244114\n",
      "[1524]\ttraining's auc: 0.90717\ttraining's binary_logloss: 0.192634\tvalid_1's auc: 0.764121\tvalid_1's binary_logloss: 0.244115\n",
      "[1525]\ttraining's auc: 0.907227\ttraining's binary_logloss: 0.192607\tvalid_1's auc: 0.764096\tvalid_1's binary_logloss: 0.244121\n",
      "[1526]\ttraining's auc: 0.907307\ttraining's binary_logloss: 0.19257\tvalid_1's auc: 0.764118\tvalid_1's binary_logloss: 0.244115\n",
      "[1527]\ttraining's auc: 0.907374\ttraining's binary_logloss: 0.192541\tvalid_1's auc: 0.7641\tvalid_1's binary_logloss: 0.244122\n",
      "[1528]\ttraining's auc: 0.907398\ttraining's binary_logloss: 0.192529\tvalid_1's auc: 0.764096\tvalid_1's binary_logloss: 0.244123\n",
      "[1529]\ttraining's auc: 0.907444\ttraining's binary_logloss: 0.192502\tvalid_1's auc: 0.764106\tvalid_1's binary_logloss: 0.244124\n",
      "[1530]\ttraining's auc: 0.907481\ttraining's binary_logloss: 0.192483\tvalid_1's auc: 0.764081\tvalid_1's binary_logloss: 0.244129\n",
      "[1531]\ttraining's auc: 0.907531\ttraining's binary_logloss: 0.192457\tvalid_1's auc: 0.764086\tvalid_1's binary_logloss: 0.244127\n",
      "[1532]\ttraining's auc: 0.907571\ttraining's binary_logloss: 0.192437\tvalid_1's auc: 0.764101\tvalid_1's binary_logloss: 0.244123\n",
      "[1533]\ttraining's auc: 0.907636\ttraining's binary_logloss: 0.192404\tvalid_1's auc: 0.764095\tvalid_1's binary_logloss: 0.244126\n",
      "[1534]\ttraining's auc: 0.907697\ttraining's binary_logloss: 0.192372\tvalid_1's auc: 0.764078\tvalid_1's binary_logloss: 0.244132\n",
      "[1535]\ttraining's auc: 0.907755\ttraining's binary_logloss: 0.192344\tvalid_1's auc: 0.764073\tvalid_1's binary_logloss: 0.244135\n",
      "[1536]\ttraining's auc: 0.907836\ttraining's binary_logloss: 0.192314\tvalid_1's auc: 0.764051\tvalid_1's binary_logloss: 0.244139\n",
      "[1537]\ttraining's auc: 0.907901\ttraining's binary_logloss: 0.192284\tvalid_1's auc: 0.764038\tvalid_1's binary_logloss: 0.244143\n",
      "[1538]\ttraining's auc: 0.907958\ttraining's binary_logloss: 0.192253\tvalid_1's auc: 0.764032\tvalid_1's binary_logloss: 0.244146\n",
      "[1539]\ttraining's auc: 0.907995\ttraining's binary_logloss: 0.192234\tvalid_1's auc: 0.764034\tvalid_1's binary_logloss: 0.244144\n",
      "[1540]\ttraining's auc: 0.908065\ttraining's binary_logloss: 0.192204\tvalid_1's auc: 0.764023\tvalid_1's binary_logloss: 0.24415\n",
      "[1541]\ttraining's auc: 0.908116\ttraining's binary_logloss: 0.192176\tvalid_1's auc: 0.764012\tvalid_1's binary_logloss: 0.244152\n",
      "[1542]\ttraining's auc: 0.908149\ttraining's binary_logloss: 0.192159\tvalid_1's auc: 0.763995\tvalid_1's binary_logloss: 0.244156\n",
      "[1543]\ttraining's auc: 0.908204\ttraining's binary_logloss: 0.19213\tvalid_1's auc: 0.763999\tvalid_1's binary_logloss: 0.244155\n",
      "[1544]\ttraining's auc: 0.90826\ttraining's binary_logloss: 0.192102\tvalid_1's auc: 0.764004\tvalid_1's binary_logloss: 0.244153\n",
      "[1545]\ttraining's auc: 0.90833\ttraining's binary_logloss: 0.192069\tvalid_1's auc: 0.764005\tvalid_1's binary_logloss: 0.244153\n",
      "[1546]\ttraining's auc: 0.908375\ttraining's binary_logloss: 0.192048\tvalid_1's auc: 0.763995\tvalid_1's binary_logloss: 0.244156\n",
      "[1547]\ttraining's auc: 0.908411\ttraining's binary_logloss: 0.192029\tvalid_1's auc: 0.763981\tvalid_1's binary_logloss: 0.244158\n",
      "[1548]\ttraining's auc: 0.908467\ttraining's binary_logloss: 0.192003\tvalid_1's auc: 0.763983\tvalid_1's binary_logloss: 0.244158\n",
      "[1549]\ttraining's auc: 0.908494\ttraining's binary_logloss: 0.191987\tvalid_1's auc: 0.763986\tvalid_1's binary_logloss: 0.244159\n",
      "[1550]\ttraining's auc: 0.908536\ttraining's binary_logloss: 0.191967\tvalid_1's auc: 0.763985\tvalid_1's binary_logloss: 0.244161\n",
      "[1551]\ttraining's auc: 0.908565\ttraining's binary_logloss: 0.191951\tvalid_1's auc: 0.763983\tvalid_1's binary_logloss: 0.24416\n",
      "[1552]\ttraining's auc: 0.908621\ttraining's binary_logloss: 0.191922\tvalid_1's auc: 0.763967\tvalid_1's binary_logloss: 0.244166\n",
      "[1553]\ttraining's auc: 0.908672\ttraining's binary_logloss: 0.191898\tvalid_1's auc: 0.763965\tvalid_1's binary_logloss: 0.244169\n",
      "[1554]\ttraining's auc: 0.908721\ttraining's binary_logloss: 0.191873\tvalid_1's auc: 0.763995\tvalid_1's binary_logloss: 0.244161\n",
      "[1555]\ttraining's auc: 0.908757\ttraining's binary_logloss: 0.191856\tvalid_1's auc: 0.764\tvalid_1's binary_logloss: 0.244158\n",
      "[1556]\ttraining's auc: 0.908787\ttraining's binary_logloss: 0.19184\tvalid_1's auc: 0.763983\tvalid_1's binary_logloss: 0.244161\n",
      "[1557]\ttraining's auc: 0.908846\ttraining's binary_logloss: 0.191813\tvalid_1's auc: 0.763977\tvalid_1's binary_logloss: 0.244164\n",
      "[1558]\ttraining's auc: 0.908892\ttraining's binary_logloss: 0.191787\tvalid_1's auc: 0.763979\tvalid_1's binary_logloss: 0.244164\n",
      "[1559]\ttraining's auc: 0.908953\ttraining's binary_logloss: 0.191758\tvalid_1's auc: 0.763975\tvalid_1's binary_logloss: 0.244167\n",
      "[1560]\ttraining's auc: 0.90898\ttraining's binary_logloss: 0.191744\tvalid_1's auc: 0.763979\tvalid_1's binary_logloss: 0.244167\n",
      "[1561]\ttraining's auc: 0.909039\ttraining's binary_logloss: 0.191715\tvalid_1's auc: 0.763975\tvalid_1's binary_logloss: 0.244169\n",
      "[1562]\ttraining's auc: 0.909064\ttraining's binary_logloss: 0.191704\tvalid_1's auc: 0.763984\tvalid_1's binary_logloss: 0.244166\n",
      "[1563]\ttraining's auc: 0.909116\ttraining's binary_logloss: 0.191675\tvalid_1's auc: 0.764001\tvalid_1's binary_logloss: 0.244163\n",
      "[1564]\ttraining's auc: 0.909165\ttraining's binary_logloss: 0.191652\tvalid_1's auc: 0.763985\tvalid_1's binary_logloss: 0.244167\n",
      "[1565]\ttraining's auc: 0.909192\ttraining's binary_logloss: 0.191638\tvalid_1's auc: 0.763989\tvalid_1's binary_logloss: 0.244169\n",
      "[1566]\ttraining's auc: 0.909217\ttraining's binary_logloss: 0.191623\tvalid_1's auc: 0.763975\tvalid_1's binary_logloss: 0.244171\n",
      "[1567]\ttraining's auc: 0.909268\ttraining's binary_logloss: 0.191589\tvalid_1's auc: 0.763961\tvalid_1's binary_logloss: 0.244178\n",
      "[1568]\ttraining's auc: 0.909327\ttraining's binary_logloss: 0.19156\tvalid_1's auc: 0.763962\tvalid_1's binary_logloss: 0.244178\n",
      "[1569]\ttraining's auc: 0.909394\ttraining's binary_logloss: 0.19153\tvalid_1's auc: 0.763976\tvalid_1's binary_logloss: 0.244174\n",
      "[1570]\ttraining's auc: 0.909425\ttraining's binary_logloss: 0.191515\tvalid_1's auc: 0.763976\tvalid_1's binary_logloss: 0.244175\n",
      "[1571]\ttraining's auc: 0.909471\ttraining's binary_logloss: 0.19149\tvalid_1's auc: 0.763991\tvalid_1's binary_logloss: 0.244174\n",
      "[1572]\ttraining's auc: 0.909545\ttraining's binary_logloss: 0.191454\tvalid_1's auc: 0.763999\tvalid_1's binary_logloss: 0.244174\n",
      "[1573]\ttraining's auc: 0.909606\ttraining's binary_logloss: 0.191425\tvalid_1's auc: 0.763985\tvalid_1's binary_logloss: 0.244176\n",
      "[1574]\ttraining's auc: 0.909677\ttraining's binary_logloss: 0.191395\tvalid_1's auc: 0.763977\tvalid_1's binary_logloss: 0.244181\n",
      "[1575]\ttraining's auc: 0.909737\ttraining's binary_logloss: 0.191364\tvalid_1's auc: 0.763954\tvalid_1's binary_logloss: 0.244191\n",
      "[1576]\ttraining's auc: 0.909787\ttraining's binary_logloss: 0.191336\tvalid_1's auc: 0.763961\tvalid_1's binary_logloss: 0.244189\n",
      "[1577]\ttraining's auc: 0.909857\ttraining's binary_logloss: 0.191303\tvalid_1's auc: 0.763971\tvalid_1's binary_logloss: 0.244184\n",
      "[1578]\ttraining's auc: 0.909882\ttraining's binary_logloss: 0.191292\tvalid_1's auc: 0.763973\tvalid_1's binary_logloss: 0.244183\n",
      "[1579]\ttraining's auc: 0.909917\ttraining's binary_logloss: 0.191274\tvalid_1's auc: 0.763972\tvalid_1's binary_logloss: 0.244183\n",
      "[1580]\ttraining's auc: 0.909977\ttraining's binary_logloss: 0.191247\tvalid_1's auc: 0.763963\tvalid_1's binary_logloss: 0.244188\n",
      "[1581]\ttraining's auc: 0.910046\ttraining's binary_logloss: 0.191216\tvalid_1's auc: 0.76396\tvalid_1's binary_logloss: 0.24419\n",
      "[1582]\ttraining's auc: 0.910077\ttraining's binary_logloss: 0.191202\tvalid_1's auc: 0.763963\tvalid_1's binary_logloss: 0.244189\n",
      "[1583]\ttraining's auc: 0.910136\ttraining's binary_logloss: 0.191168\tvalid_1's auc: 0.763975\tvalid_1's binary_logloss: 0.244184\n",
      "[1584]\ttraining's auc: 0.910187\ttraining's binary_logloss: 0.19114\tvalid_1's auc: 0.763984\tvalid_1's binary_logloss: 0.244182\n",
      "[1585]\ttraining's auc: 0.910215\ttraining's binary_logloss: 0.191127\tvalid_1's auc: 0.763974\tvalid_1's binary_logloss: 0.244186\n",
      "[1586]\ttraining's auc: 0.910263\ttraining's binary_logloss: 0.191096\tvalid_1's auc: 0.763971\tvalid_1's binary_logloss: 0.244187\n",
      "[1587]\ttraining's auc: 0.910313\ttraining's binary_logloss: 0.191071\tvalid_1's auc: 0.763993\tvalid_1's binary_logloss: 0.24418\n",
      "[1588]\ttraining's auc: 0.910375\ttraining's binary_logloss: 0.191043\tvalid_1's auc: 0.763974\tvalid_1's binary_logloss: 0.244185\n",
      "[1589]\ttraining's auc: 0.910428\ttraining's binary_logloss: 0.191014\tvalid_1's auc: 0.763976\tvalid_1's binary_logloss: 0.244187\n",
      "[1590]\ttraining's auc: 0.910452\ttraining's binary_logloss: 0.191005\tvalid_1's auc: 0.763968\tvalid_1's binary_logloss: 0.244189\n",
      "[1591]\ttraining's auc: 0.910492\ttraining's binary_logloss: 0.190985\tvalid_1's auc: 0.763954\tvalid_1's binary_logloss: 0.24419\n",
      "[1592]\ttraining's auc: 0.910539\ttraining's binary_logloss: 0.190959\tvalid_1's auc: 0.763955\tvalid_1's binary_logloss: 0.24419\n",
      "[1593]\ttraining's auc: 0.910564\ttraining's binary_logloss: 0.190945\tvalid_1's auc: 0.763932\tvalid_1's binary_logloss: 0.244197\n",
      "[1594]\ttraining's auc: 0.910588\ttraining's binary_logloss: 0.190933\tvalid_1's auc: 0.763928\tvalid_1's binary_logloss: 0.244198\n",
      "[1595]\ttraining's auc: 0.910628\ttraining's binary_logloss: 0.19091\tvalid_1's auc: 0.763913\tvalid_1's binary_logloss: 0.244203\n",
      "[1596]\ttraining's auc: 0.910644\ttraining's binary_logloss: 0.190901\tvalid_1's auc: 0.7639\tvalid_1's binary_logloss: 0.244206\n",
      "[1597]\ttraining's auc: 0.91068\ttraining's binary_logloss: 0.190884\tvalid_1's auc: 0.763904\tvalid_1's binary_logloss: 0.244202\n",
      "[1598]\ttraining's auc: 0.910727\ttraining's binary_logloss: 0.190857\tvalid_1's auc: 0.763899\tvalid_1's binary_logloss: 0.244207\n",
      "[1599]\ttraining's auc: 0.910752\ttraining's binary_logloss: 0.190842\tvalid_1's auc: 0.763898\tvalid_1's binary_logloss: 0.244209\n",
      "[1600]\ttraining's auc: 0.91076\ttraining's binary_logloss: 0.190837\tvalid_1's auc: 0.763903\tvalid_1's binary_logloss: 0.244207\n",
      "[1601]\ttraining's auc: 0.910799\ttraining's binary_logloss: 0.190818\tvalid_1's auc: 0.763893\tvalid_1's binary_logloss: 0.244209\n",
      "[1602]\ttraining's auc: 0.910847\ttraining's binary_logloss: 0.19079\tvalid_1's auc: 0.763897\tvalid_1's binary_logloss: 0.244209\n",
      "[1603]\ttraining's auc: 0.910914\ttraining's binary_logloss: 0.19076\tvalid_1's auc: 0.763883\tvalid_1's binary_logloss: 0.244213\n",
      "[1604]\ttraining's auc: 0.910964\ttraining's binary_logloss: 0.190733\tvalid_1's auc: 0.763876\tvalid_1's binary_logloss: 0.244215\n",
      "[1605]\ttraining's auc: 0.911007\ttraining's binary_logloss: 0.190711\tvalid_1's auc: 0.763878\tvalid_1's binary_logloss: 0.244213\n",
      "[1606]\ttraining's auc: 0.911058\ttraining's binary_logloss: 0.190682\tvalid_1's auc: 0.763855\tvalid_1's binary_logloss: 0.244223\n",
      "[1607]\ttraining's auc: 0.911081\ttraining's binary_logloss: 0.190667\tvalid_1's auc: 0.763849\tvalid_1's binary_logloss: 0.244226\n",
      "[1608]\ttraining's auc: 0.911149\ttraining's binary_logloss: 0.190635\tvalid_1's auc: 0.763843\tvalid_1's binary_logloss: 0.244225\n",
      "[1609]\ttraining's auc: 0.911195\ttraining's binary_logloss: 0.190606\tvalid_1's auc: 0.763835\tvalid_1's binary_logloss: 0.244228\n",
      "[1610]\ttraining's auc: 0.911244\ttraining's binary_logloss: 0.190577\tvalid_1's auc: 0.763831\tvalid_1's binary_logloss: 0.244228\n",
      "[1611]\ttraining's auc: 0.911289\ttraining's binary_logloss: 0.190552\tvalid_1's auc: 0.763828\tvalid_1's binary_logloss: 0.24423\n",
      "[1612]\ttraining's auc: 0.911333\ttraining's binary_logloss: 0.190526\tvalid_1's auc: 0.763827\tvalid_1's binary_logloss: 0.244227\n",
      "[1613]\ttraining's auc: 0.911363\ttraining's binary_logloss: 0.19051\tvalid_1's auc: 0.763807\tvalid_1's binary_logloss: 0.244233\n",
      "[1614]\ttraining's auc: 0.911412\ttraining's binary_logloss: 0.190479\tvalid_1's auc: 0.763784\tvalid_1's binary_logloss: 0.244238\n",
      "[1615]\ttraining's auc: 0.911471\ttraining's binary_logloss: 0.190448\tvalid_1's auc: 0.763781\tvalid_1's binary_logloss: 0.24424\n",
      "[1616]\ttraining's auc: 0.911533\ttraining's binary_logloss: 0.190419\tvalid_1's auc: 0.76375\tvalid_1's binary_logloss: 0.244246\n",
      "[1617]\ttraining's auc: 0.91158\ttraining's binary_logloss: 0.190397\tvalid_1's auc: 0.763754\tvalid_1's binary_logloss: 0.244244\n",
      "[1618]\ttraining's auc: 0.911637\ttraining's binary_logloss: 0.190367\tvalid_1's auc: 0.763752\tvalid_1's binary_logloss: 0.244245\n",
      "[1619]\ttraining's auc: 0.911705\ttraining's binary_logloss: 0.190337\tvalid_1's auc: 0.763742\tvalid_1's binary_logloss: 0.244248\n",
      "[1620]\ttraining's auc: 0.911745\ttraining's binary_logloss: 0.190315\tvalid_1's auc: 0.763736\tvalid_1's binary_logloss: 0.24425\n",
      "[1621]\ttraining's auc: 0.911804\ttraining's binary_logloss: 0.190281\tvalid_1's auc: 0.763726\tvalid_1's binary_logloss: 0.24425\n",
      "[1622]\ttraining's auc: 0.911868\ttraining's binary_logloss: 0.190249\tvalid_1's auc: 0.763725\tvalid_1's binary_logloss: 0.24425\n",
      "[1623]\ttraining's auc: 0.91193\ttraining's binary_logloss: 0.190219\tvalid_1's auc: 0.763756\tvalid_1's binary_logloss: 0.244245\n",
      "[1624]\ttraining's auc: 0.911986\ttraining's binary_logloss: 0.190191\tvalid_1's auc: 0.763748\tvalid_1's binary_logloss: 0.244246\n",
      "[1625]\ttraining's auc: 0.912043\ttraining's binary_logloss: 0.190162\tvalid_1's auc: 0.763734\tvalid_1's binary_logloss: 0.244248\n",
      "[1626]\ttraining's auc: 0.912089\ttraining's binary_logloss: 0.19014\tvalid_1's auc: 0.763732\tvalid_1's binary_logloss: 0.24425\n",
      "[1627]\ttraining's auc: 0.912132\ttraining's binary_logloss: 0.190112\tvalid_1's auc: 0.763727\tvalid_1's binary_logloss: 0.24425\n",
      "[1628]\ttraining's auc: 0.912203\ttraining's binary_logloss: 0.190082\tvalid_1's auc: 0.763736\tvalid_1's binary_logloss: 0.244251\n",
      "[1629]\ttraining's auc: 0.912251\ttraining's binary_logloss: 0.190057\tvalid_1's auc: 0.763745\tvalid_1's binary_logloss: 0.244247\n",
      "[1630]\ttraining's auc: 0.912307\ttraining's binary_logloss: 0.190029\tvalid_1's auc: 0.763732\tvalid_1's binary_logloss: 0.244251\n",
      "[1631]\ttraining's auc: 0.912361\ttraining's binary_logloss: 0.189997\tvalid_1's auc: 0.763729\tvalid_1's binary_logloss: 0.244255\n",
      "[1632]\ttraining's auc: 0.912407\ttraining's binary_logloss: 0.189969\tvalid_1's auc: 0.763733\tvalid_1's binary_logloss: 0.244256\n",
      "[1633]\ttraining's auc: 0.91245\ttraining's binary_logloss: 0.189945\tvalid_1's auc: 0.763731\tvalid_1's binary_logloss: 0.244256\n",
      "[1634]\ttraining's auc: 0.912495\ttraining's binary_logloss: 0.189922\tvalid_1's auc: 0.763737\tvalid_1's binary_logloss: 0.244252\n",
      "[1635]\ttraining's auc: 0.912529\ttraining's binary_logloss: 0.189906\tvalid_1's auc: 0.763733\tvalid_1's binary_logloss: 0.244253\n",
      "[1636]\ttraining's auc: 0.912584\ttraining's binary_logloss: 0.189877\tvalid_1's auc: 0.76372\tvalid_1's binary_logloss: 0.244257\n",
      "[1637]\ttraining's auc: 0.912623\ttraining's binary_logloss: 0.189853\tvalid_1's auc: 0.763714\tvalid_1's binary_logloss: 0.244258\n",
      "[1638]\ttraining's auc: 0.912666\ttraining's binary_logloss: 0.189825\tvalid_1's auc: 0.763728\tvalid_1's binary_logloss: 0.244255\n",
      "[1639]\ttraining's auc: 0.912724\ttraining's binary_logloss: 0.189798\tvalid_1's auc: 0.763724\tvalid_1's binary_logloss: 0.244258\n",
      "[1640]\ttraining's auc: 0.912783\ttraining's binary_logloss: 0.189768\tvalid_1's auc: 0.763711\tvalid_1's binary_logloss: 0.244257\n",
      "[1641]\ttraining's auc: 0.912799\ttraining's binary_logloss: 0.189758\tvalid_1's auc: 0.763708\tvalid_1's binary_logloss: 0.244259\n",
      "[1642]\ttraining's auc: 0.912841\ttraining's binary_logloss: 0.189733\tvalid_1's auc: 0.763701\tvalid_1's binary_logloss: 0.244263\n",
      "[1643]\ttraining's auc: 0.912896\ttraining's binary_logloss: 0.189702\tvalid_1's auc: 0.763693\tvalid_1's binary_logloss: 0.244266\n",
      "[1644]\ttraining's auc: 0.91292\ttraining's binary_logloss: 0.189689\tvalid_1's auc: 0.763695\tvalid_1's binary_logloss: 0.244267\n",
      "[1645]\ttraining's auc: 0.912981\ttraining's binary_logloss: 0.189659\tvalid_1's auc: 0.763677\tvalid_1's binary_logloss: 0.244271\n",
      "[1646]\ttraining's auc: 0.913033\ttraining's binary_logloss: 0.189634\tvalid_1's auc: 0.763662\tvalid_1's binary_logloss: 0.244273\n",
      "[1647]\ttraining's auc: 0.913079\ttraining's binary_logloss: 0.189607\tvalid_1's auc: 0.76367\tvalid_1's binary_logloss: 0.244275\n",
      "[1648]\ttraining's auc: 0.913111\ttraining's binary_logloss: 0.189587\tvalid_1's auc: 0.763672\tvalid_1's binary_logloss: 0.244275\n",
      "[1649]\ttraining's auc: 0.913167\ttraining's binary_logloss: 0.189556\tvalid_1's auc: 0.763669\tvalid_1's binary_logloss: 0.244275\n",
      "[1650]\ttraining's auc: 0.913227\ttraining's binary_logloss: 0.189523\tvalid_1's auc: 0.76367\tvalid_1's binary_logloss: 0.244274\n",
      "[1651]\ttraining's auc: 0.913286\ttraining's binary_logloss: 0.189495\tvalid_1's auc: 0.763672\tvalid_1's binary_logloss: 0.244273\n",
      "[1652]\ttraining's auc: 0.913338\ttraining's binary_logloss: 0.189466\tvalid_1's auc: 0.763668\tvalid_1's binary_logloss: 0.244276\n",
      "[1653]\ttraining's auc: 0.913404\ttraining's binary_logloss: 0.189438\tvalid_1's auc: 0.763666\tvalid_1's binary_logloss: 0.244276\n",
      "[1654]\ttraining's auc: 0.913417\ttraining's binary_logloss: 0.189429\tvalid_1's auc: 0.763661\tvalid_1's binary_logloss: 0.244279\n",
      "[1655]\ttraining's auc: 0.913459\ttraining's binary_logloss: 0.1894\tvalid_1's auc: 0.763662\tvalid_1's binary_logloss: 0.24428\n",
      "[1656]\ttraining's auc: 0.913512\ttraining's binary_logloss: 0.189371\tvalid_1's auc: 0.763651\tvalid_1's binary_logloss: 0.244283\n",
      "[1657]\ttraining's auc: 0.913557\ttraining's binary_logloss: 0.189343\tvalid_1's auc: 0.763653\tvalid_1's binary_logloss: 0.244283\n",
      "[1658]\ttraining's auc: 0.91362\ttraining's binary_logloss: 0.189312\tvalid_1's auc: 0.763643\tvalid_1's binary_logloss: 0.244285\n",
      "[1659]\ttraining's auc: 0.913666\ttraining's binary_logloss: 0.189286\tvalid_1's auc: 0.763646\tvalid_1's binary_logloss: 0.244286\n",
      "[1660]\ttraining's auc: 0.913727\ttraining's binary_logloss: 0.189259\tvalid_1's auc: 0.76366\tvalid_1's binary_logloss: 0.244283\n",
      "[1661]\ttraining's auc: 0.91379\ttraining's binary_logloss: 0.189227\tvalid_1's auc: 0.76367\tvalid_1's binary_logloss: 0.244282\n",
      "[1662]\ttraining's auc: 0.913846\ttraining's binary_logloss: 0.189199\tvalid_1's auc: 0.763662\tvalid_1's binary_logloss: 0.244283\n",
      "[1663]\ttraining's auc: 0.913886\ttraining's binary_logloss: 0.189172\tvalid_1's auc: 0.763634\tvalid_1's binary_logloss: 0.24429\n",
      "[1664]\ttraining's auc: 0.913936\ttraining's binary_logloss: 0.189143\tvalid_1's auc: 0.763613\tvalid_1's binary_logloss: 0.244295\n",
      "[1665]\ttraining's auc: 0.913967\ttraining's binary_logloss: 0.18913\tvalid_1's auc: 0.763618\tvalid_1's binary_logloss: 0.244293\n",
      "[1666]\ttraining's auc: 0.914014\ttraining's binary_logloss: 0.189107\tvalid_1's auc: 0.763605\tvalid_1's binary_logloss: 0.244297\n",
      "[1667]\ttraining's auc: 0.914065\ttraining's binary_logloss: 0.189079\tvalid_1's auc: 0.763606\tvalid_1's binary_logloss: 0.244296\n",
      "[1668]\ttraining's auc: 0.914115\ttraining's binary_logloss: 0.189053\tvalid_1's auc: 0.763619\tvalid_1's binary_logloss: 0.244293\n",
      "[1669]\ttraining's auc: 0.91416\ttraining's binary_logloss: 0.189027\tvalid_1's auc: 0.763628\tvalid_1's binary_logloss: 0.24429\n",
      "[1670]\ttraining's auc: 0.914179\ttraining's binary_logloss: 0.189015\tvalid_1's auc: 0.76362\tvalid_1's binary_logloss: 0.244292\n",
      "[1671]\ttraining's auc: 0.914202\ttraining's binary_logloss: 0.189003\tvalid_1's auc: 0.76363\tvalid_1's binary_logloss: 0.244289\n",
      "[1672]\ttraining's auc: 0.914225\ttraining's binary_logloss: 0.188992\tvalid_1's auc: 0.763629\tvalid_1's binary_logloss: 0.244292\n",
      "[1673]\ttraining's auc: 0.914251\ttraining's binary_logloss: 0.188977\tvalid_1's auc: 0.763632\tvalid_1's binary_logloss: 0.244292\n",
      "[1674]\ttraining's auc: 0.91429\ttraining's binary_logloss: 0.188957\tvalid_1's auc: 0.763625\tvalid_1's binary_logloss: 0.244297\n",
      "[1675]\ttraining's auc: 0.914303\ttraining's binary_logloss: 0.188949\tvalid_1's auc: 0.763629\tvalid_1's binary_logloss: 0.244296\n",
      "[1676]\ttraining's auc: 0.914337\ttraining's binary_logloss: 0.188931\tvalid_1's auc: 0.763631\tvalid_1's binary_logloss: 0.244293\n",
      "[1677]\ttraining's auc: 0.91439\ttraining's binary_logloss: 0.188903\tvalid_1's auc: 0.763642\tvalid_1's binary_logloss: 0.244292\n",
      "[1678]\ttraining's auc: 0.914425\ttraining's binary_logloss: 0.18888\tvalid_1's auc: 0.763657\tvalid_1's binary_logloss: 0.244286\n",
      "[1679]\ttraining's auc: 0.914474\ttraining's binary_logloss: 0.188851\tvalid_1's auc: 0.763663\tvalid_1's binary_logloss: 0.244282\n",
      "[1680]\ttraining's auc: 0.9145\ttraining's binary_logloss: 0.188834\tvalid_1's auc: 0.76367\tvalid_1's binary_logloss: 0.244281\n",
      "[1681]\ttraining's auc: 0.914551\ttraining's binary_logloss: 0.18881\tvalid_1's auc: 0.763669\tvalid_1's binary_logloss: 0.244284\n",
      "[1682]\ttraining's auc: 0.914597\ttraining's binary_logloss: 0.188785\tvalid_1's auc: 0.763654\tvalid_1's binary_logloss: 0.244287\n",
      "[1683]\ttraining's auc: 0.914652\ttraining's binary_logloss: 0.188758\tvalid_1's auc: 0.763645\tvalid_1's binary_logloss: 0.244288\n",
      "[1684]\ttraining's auc: 0.914682\ttraining's binary_logloss: 0.18874\tvalid_1's auc: 0.763645\tvalid_1's binary_logloss: 0.244288\n",
      "[1685]\ttraining's auc: 0.914706\ttraining's binary_logloss: 0.188727\tvalid_1's auc: 0.763649\tvalid_1's binary_logloss: 0.244287\n",
      "[1686]\ttraining's auc: 0.914749\ttraining's binary_logloss: 0.188701\tvalid_1's auc: 0.763642\tvalid_1's binary_logloss: 0.244288\n",
      "[1687]\ttraining's auc: 0.914808\ttraining's binary_logloss: 0.188667\tvalid_1's auc: 0.763643\tvalid_1's binary_logloss: 0.24429\n",
      "[1688]\ttraining's auc: 0.914844\ttraining's binary_logloss: 0.188645\tvalid_1's auc: 0.763636\tvalid_1's binary_logloss: 0.244291\n",
      "[1689]\ttraining's auc: 0.914899\ttraining's binary_logloss: 0.188624\tvalid_1's auc: 0.763626\tvalid_1's binary_logloss: 0.244293\n",
      "[1690]\ttraining's auc: 0.914944\ttraining's binary_logloss: 0.188598\tvalid_1's auc: 0.763631\tvalid_1's binary_logloss: 0.244293\n",
      "[1691]\ttraining's auc: 0.914996\ttraining's binary_logloss: 0.188574\tvalid_1's auc: 0.763608\tvalid_1's binary_logloss: 0.244299\n",
      "[1692]\ttraining's auc: 0.915044\ttraining's binary_logloss: 0.188549\tvalid_1's auc: 0.763612\tvalid_1's binary_logloss: 0.244297\n",
      "[1693]\ttraining's auc: 0.915083\ttraining's binary_logloss: 0.188529\tvalid_1's auc: 0.763591\tvalid_1's binary_logloss: 0.244301\n",
      "[1694]\ttraining's auc: 0.915144\ttraining's binary_logloss: 0.188499\tvalid_1's auc: 0.763602\tvalid_1's binary_logloss: 0.244299\n",
      "[1695]\ttraining's auc: 0.915202\ttraining's binary_logloss: 0.188469\tvalid_1's auc: 0.763599\tvalid_1's binary_logloss: 0.2443\n",
      "[1696]\ttraining's auc: 0.915221\ttraining's binary_logloss: 0.188459\tvalid_1's auc: 0.763604\tvalid_1's binary_logloss: 0.244301\n",
      "[1697]\ttraining's auc: 0.915268\ttraining's binary_logloss: 0.188432\tvalid_1's auc: 0.76362\tvalid_1's binary_logloss: 0.2443\n",
      "[1698]\ttraining's auc: 0.915308\ttraining's binary_logloss: 0.188409\tvalid_1's auc: 0.76361\tvalid_1's binary_logloss: 0.244305\n",
      "[1699]\ttraining's auc: 0.91536\ttraining's binary_logloss: 0.188383\tvalid_1's auc: 0.763618\tvalid_1's binary_logloss: 0.244306\n",
      "[1700]\ttraining's auc: 0.915417\ttraining's binary_logloss: 0.188355\tvalid_1's auc: 0.763612\tvalid_1's binary_logloss: 0.244307\n",
      "[1701]\ttraining's auc: 0.915471\ttraining's binary_logloss: 0.188324\tvalid_1's auc: 0.763605\tvalid_1's binary_logloss: 0.244307\n",
      "[1702]\ttraining's auc: 0.915516\ttraining's binary_logloss: 0.188294\tvalid_1's auc: 0.763601\tvalid_1's binary_logloss: 0.244312\n",
      "[1703]\ttraining's auc: 0.915561\ttraining's binary_logloss: 0.188268\tvalid_1's auc: 0.763597\tvalid_1's binary_logloss: 0.244315\n",
      "[1704]\ttraining's auc: 0.915618\ttraining's binary_logloss: 0.188239\tvalid_1's auc: 0.763582\tvalid_1's binary_logloss: 0.244319\n",
      "[1705]\ttraining's auc: 0.915678\ttraining's binary_logloss: 0.188209\tvalid_1's auc: 0.763576\tvalid_1's binary_logloss: 0.244323\n",
      "[1706]\ttraining's auc: 0.91573\ttraining's binary_logloss: 0.188181\tvalid_1's auc: 0.763573\tvalid_1's binary_logloss: 0.244327\n",
      "[1707]\ttraining's auc: 0.915746\ttraining's binary_logloss: 0.188171\tvalid_1's auc: 0.763579\tvalid_1's binary_logloss: 0.244324\n",
      "[1708]\ttraining's auc: 0.915793\ttraining's binary_logloss: 0.188149\tvalid_1's auc: 0.76357\tvalid_1's binary_logloss: 0.244328\n",
      "[1709]\ttraining's auc: 0.915853\ttraining's binary_logloss: 0.18812\tvalid_1's auc: 0.763569\tvalid_1's binary_logloss: 0.244329\n",
      "[1710]\ttraining's auc: 0.91591\ttraining's binary_logloss: 0.188092\tvalid_1's auc: 0.763552\tvalid_1's binary_logloss: 0.244333\n",
      "[1711]\ttraining's auc: 0.91593\ttraining's binary_logloss: 0.188079\tvalid_1's auc: 0.763544\tvalid_1's binary_logloss: 0.244333\n",
      "[1712]\ttraining's auc: 0.915997\ttraining's binary_logloss: 0.188051\tvalid_1's auc: 0.763539\tvalid_1's binary_logloss: 0.244332\n",
      "[1713]\ttraining's auc: 0.916041\ttraining's binary_logloss: 0.188022\tvalid_1's auc: 0.763554\tvalid_1's binary_logloss: 0.24433\n",
      "[1714]\ttraining's auc: 0.916051\ttraining's binary_logloss: 0.188016\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.244331\n",
      "[1715]\ttraining's auc: 0.916104\ttraining's binary_logloss: 0.187986\tvalid_1's auc: 0.763557\tvalid_1's binary_logloss: 0.24433\n",
      "[1716]\ttraining's auc: 0.916159\ttraining's binary_logloss: 0.187958\tvalid_1's auc: 0.76356\tvalid_1's binary_logloss: 0.244334\n",
      "[1717]\ttraining's auc: 0.916178\ttraining's binary_logloss: 0.187946\tvalid_1's auc: 0.763552\tvalid_1's binary_logloss: 0.244338\n",
      "[1718]\ttraining's auc: 0.916223\ttraining's binary_logloss: 0.187916\tvalid_1's auc: 0.763551\tvalid_1's binary_logloss: 0.244338\n",
      "[1719]\ttraining's auc: 0.91625\ttraining's binary_logloss: 0.187899\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.24434\n",
      "[1720]\ttraining's auc: 0.9163\ttraining's binary_logloss: 0.18787\tvalid_1's auc: 0.763546\tvalid_1's binary_logloss: 0.244339\n",
      "[1721]\ttraining's auc: 0.916351\ttraining's binary_logloss: 0.187841\tvalid_1's auc: 0.763528\tvalid_1's binary_logloss: 0.244342\n",
      "[1722]\ttraining's auc: 0.91639\ttraining's binary_logloss: 0.187815\tvalid_1's auc: 0.763533\tvalid_1's binary_logloss: 0.244342\n",
      "[1723]\ttraining's auc: 0.916426\ttraining's binary_logloss: 0.187796\tvalid_1's auc: 0.76352\tvalid_1's binary_logloss: 0.244346\n",
      "[1724]\ttraining's auc: 0.916468\ttraining's binary_logloss: 0.187768\tvalid_1's auc: 0.763515\tvalid_1's binary_logloss: 0.244347\n",
      "[1725]\ttraining's auc: 0.916519\ttraining's binary_logloss: 0.18774\tvalid_1's auc: 0.763529\tvalid_1's binary_logloss: 0.244341\n",
      "[1726]\ttraining's auc: 0.916565\ttraining's binary_logloss: 0.187713\tvalid_1's auc: 0.763541\tvalid_1's binary_logloss: 0.244337\n",
      "[1727]\ttraining's auc: 0.916585\ttraining's binary_logloss: 0.187699\tvalid_1's auc: 0.763535\tvalid_1's binary_logloss: 0.244337\n",
      "[1728]\ttraining's auc: 0.916645\ttraining's binary_logloss: 0.187665\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.244334\n",
      "[1729]\ttraining's auc: 0.916661\ttraining's binary_logloss: 0.187657\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.244334\n",
      "[1730]\ttraining's auc: 0.916698\ttraining's binary_logloss: 0.187641\tvalid_1's auc: 0.763547\tvalid_1's binary_logloss: 0.244333\n",
      "[1731]\ttraining's auc: 0.916732\ttraining's binary_logloss: 0.18762\tvalid_1's auc: 0.763548\tvalid_1's binary_logloss: 0.244333\n",
      "[1732]\ttraining's auc: 0.916775\ttraining's binary_logloss: 0.187594\tvalid_1's auc: 0.763563\tvalid_1's binary_logloss: 0.244332\n",
      "[1733]\ttraining's auc: 0.916822\ttraining's binary_logloss: 0.187568\tvalid_1's auc: 0.763563\tvalid_1's binary_logloss: 0.244334\n",
      "[1734]\ttraining's auc: 0.916868\ttraining's binary_logloss: 0.187547\tvalid_1's auc: 0.763565\tvalid_1's binary_logloss: 0.244334\n",
      "[1735]\ttraining's auc: 0.916914\ttraining's binary_logloss: 0.187519\tvalid_1's auc: 0.763564\tvalid_1's binary_logloss: 0.244335\n",
      "[1736]\ttraining's auc: 0.916964\ttraining's binary_logloss: 0.18749\tvalid_1's auc: 0.763548\tvalid_1's binary_logloss: 0.24434\n",
      "[1737]\ttraining's auc: 0.917027\ttraining's binary_logloss: 0.187459\tvalid_1's auc: 0.763515\tvalid_1's binary_logloss: 0.244349\n",
      "[1738]\ttraining's auc: 0.917079\ttraining's binary_logloss: 0.18743\tvalid_1's auc: 0.763504\tvalid_1's binary_logloss: 0.24435\n",
      "[1739]\ttraining's auc: 0.917145\ttraining's binary_logloss: 0.187401\tvalid_1's auc: 0.763477\tvalid_1's binary_logloss: 0.244355\n",
      "[1740]\ttraining's auc: 0.917179\ttraining's binary_logloss: 0.187381\tvalid_1's auc: 0.763477\tvalid_1's binary_logloss: 0.244354\n",
      "[1741]\ttraining's auc: 0.917232\ttraining's binary_logloss: 0.187355\tvalid_1's auc: 0.763493\tvalid_1's binary_logloss: 0.24435\n",
      "[1742]\ttraining's auc: 0.917292\ttraining's binary_logloss: 0.187325\tvalid_1's auc: 0.763505\tvalid_1's binary_logloss: 0.244351\n",
      "[1743]\ttraining's auc: 0.917332\ttraining's binary_logloss: 0.187305\tvalid_1's auc: 0.763502\tvalid_1's binary_logloss: 0.244351\n",
      "[1744]\ttraining's auc: 0.917383\ttraining's binary_logloss: 0.187279\tvalid_1's auc: 0.763509\tvalid_1's binary_logloss: 0.244349\n",
      "[1745]\ttraining's auc: 0.917411\ttraining's binary_logloss: 0.187264\tvalid_1's auc: 0.763523\tvalid_1's binary_logloss: 0.244347\n",
      "[1746]\ttraining's auc: 0.917463\ttraining's binary_logloss: 0.187238\tvalid_1's auc: 0.763505\tvalid_1's binary_logloss: 0.244351\n",
      "[1747]\ttraining's auc: 0.917531\ttraining's binary_logloss: 0.187203\tvalid_1's auc: 0.763525\tvalid_1's binary_logloss: 0.244346\n",
      "[1748]\ttraining's auc: 0.917576\ttraining's binary_logloss: 0.187178\tvalid_1's auc: 0.76351\tvalid_1's binary_logloss: 0.244348\n",
      "[1749]\ttraining's auc: 0.917615\ttraining's binary_logloss: 0.18716\tvalid_1's auc: 0.763501\tvalid_1's binary_logloss: 0.24435\n",
      "[1750]\ttraining's auc: 0.91767\ttraining's binary_logloss: 0.187136\tvalid_1's auc: 0.763488\tvalid_1's binary_logloss: 0.244353\n",
      "[1751]\ttraining's auc: 0.91772\ttraining's binary_logloss: 0.187107\tvalid_1's auc: 0.763489\tvalid_1's binary_logloss: 0.244355\n",
      "[1752]\ttraining's auc: 0.917736\ttraining's binary_logloss: 0.187099\tvalid_1's auc: 0.763486\tvalid_1's binary_logloss: 0.244357\n",
      "[1753]\ttraining's auc: 0.917792\ttraining's binary_logloss: 0.187071\tvalid_1's auc: 0.763477\tvalid_1's binary_logloss: 0.244359\n",
      "[1754]\ttraining's auc: 0.917847\ttraining's binary_logloss: 0.187045\tvalid_1's auc: 0.763476\tvalid_1's binary_logloss: 0.244361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=0.613, learning_rate=0.01, max_bin=407,\n",
       "               max_depth=11, min_child_samples=165, min_child_weight=6,\n",
       "               n_estimators=5000, nthread=-1, num_leaves=58, reg_alpha=3.564,\n",
       "               reg_lambda=4.93, silent=-1, subsample=0.708, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.613, learning_rate=0.01, max_bin=407,\n",
       "               max_depth=11, min_child_samples=165, min_child_weight=6,\n",
       "               n_estimators=5000, nthread=-1, num_leaves=58, reg_alpha=3.564,\n",
       "               reg_lambda=4.93, silent=-1, subsample=0.708, verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.613, learning_rate=0.01, max_bin=407,\n",
       "               max_depth=11, min_child_samples=165, min_child_weight=6,\n",
       "               n_estimators=5000, nthread=-1, num_leaves=58, reg_alpha=3.564,\n",
       "               reg_lambda=4.93, silent=-1, subsample=0.708, verbose=-1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(\n",
    "    nthread=-1,\n",
    "    #device_type='gpu',\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=11,\n",
    "    num_leaves=58,\n",
    "    colsample_bytree=0.613,\n",
    "    subsample=0.708,\n",
    "    max_bin=407,\n",
    "    reg_alpha=3.564,\n",
    "    reg_lambda=4.930,\n",
    "    min_child_weight=6,\n",
    "    min_child_samples=165,\n",
    "    #keep_training_booster=True,\n",
    "    silent=-1,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    eval_metric='auc',\n",
    "    # verbose=500,\n",
    "    verbose=-1,\n",
    "    early_stopping_rounds=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBMClassifier(colsample_bytree=0.613, learning_rate=0.01, max_bin=407,\n",
    "               max_depth=11, min_child_samples=165, min_child_weight=6,\n",
    "               n_estimators=5000, nthread=-1, num_leaves=58, reg_alpha=3.564,\n",
    "               reg_lambda=4.93, silent=-1, subsample=0.708, verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>18348</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>1583</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive  Negative\n",
       "Positive     18348        38\n",
       "Negative      1583        31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Positive', 'Negative'],\n",
    "                     columns=['Positive', 'Negative'])\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            \tPositive \tNegative\n",
    "    Positive \t18348 \t38\n",
    "    Negative \t1583 \t31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon bah même avec leurs paramètres le résultat est mauvais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_clf = clf.predict_proba(X_test)\n",
    "y_pred_proba_positive_clf = y_pred_proba_clf[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7645288270222306"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_clf, tpr_clf, thresholds_clf = roc_curve(y_test, y_pred_proba_positive_clf)\n",
    "roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "roc_auc_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Applying the fine-tuned best classifier for the final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Features' importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "# InvalidModelError: Model type not yet supported by TreeExplainer:\n",
    "# <class 'imblearn.pipeline.Pipeline'>\n",
    "explainer = shap.TreeExplainer(gs_S.best_estimator_)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas nécessaire de le rappeler normalement\n",
    "feature_names = imputer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MemoryError: Unable to allocate 148. GiB for an array with shape\n",
    "# (3312, 5984000) and data type float64\n",
    "K = 10\n",
    "X_train_sample = shap.sample(X_train, K)\n",
    "print(X_train_sample.shape)\n",
    "explainer = shap.KernelExplainer(\n",
    "    gs_Sb.best_estimator_.predict_proba,\n",
    "    X_train_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je rajoute \".values\" uniquement pour supprimer le million de warnings \"X does\n",
    "# not have valid feature names, but MinMaxScaler was fitted with feature names\"\n",
    "# que SHAP sort.\n",
    "# Ça ne fonctionne pas donc je supprime tous les warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "shap_values = explainer.shap_values(X_test, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test.values, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.get_feature_names_out(input_features=feature_names)[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.columns[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Precision:\",\n",
    "      precision_score(gs_S.predict(X_test), y_test))\n",
    "      # precision_score(rs.best_estimator_.predict(X_test), y_test))\n",
    "print(\"Recall:\",\n",
    "      recall_score(gs_S.best_estimator_.predict(X_test), y_test))\n",
    "print(\"ROC AUC Score:\",\n",
    "      roc_auc_score(gs_S.best_estimator_.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot la courbe roc du meilleur pour chaque sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap (et lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "#rs_cv.fit(X_train, y_train)\n",
    "#rs_cv.fit(X_train_u, y_train_u)\n",
    "rs_cv.fit(X_train_o, y_train_o)\n",
    "\n",
    "for algorithm in classifiers.keys():\n",
    "    print(f\"Best parameters for {algorithm}: {rs_cv.best_params_[algorithm]}\")\n",
    "    print(f\"Best AUC score for {algorithm}: {rs_cv.best_score_[algorithm]['roc_auc']:.3f}\")\n",
    "    print(f\"Best accuracy score for {algorithm}: {rs_cv.best_score_[algorithm]['accuracy']:.3f}\")\n",
    "\n",
    "#best_algorithm = rs_cv.best_estimator_.keys()[0]\n",
    "best_algorithm = rs_cv.best_estimator_.named_steps.keys()\n",
    "print(f\"Overall best algorithm: {best_algorithm}\")\n",
    "print(f\"Best AUC score: {rs_cv.best_score_[best_algorithm]['roc_auc']:.3f}\")\n",
    "print(f\"Best accuracy score: {rs_cv.best_score_[best_algorithm]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_S.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
